Using cpu device
Training started...
-----------------------------
| time/              |      |
|    fps             | 1360 |
|    iterations      | 1    |
|    time_elapsed    | 1    |
|    total_timesteps | 2048 |
-----------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 919           |
|    iterations           | 2             |
|    time_elapsed         | 4             |
|    total_timesteps      | 4096          |
| train/                  |               |
|    approx_kl            | 0.00071833585 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.26         |
|    explained_variance   | -0.036        |
|    learning_rate        | 0.0001        |
|    loss                 | 14.3          |
|    n_updates            | 10            |
|    policy_gradient_loss | -0.00129      |
|    std                  | 1             |
|    value_loss           | 43.5          |
-------------------------------------------
C:\Users\jimal\anaconda3\envs\Y2B\lib\site-packages\stable_baselines3\common\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-529.13 +/- 91.75
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -529        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.001282417 |
|    clip_fraction        | 0.000391    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | -0.27       |
|    learning_rate        | 0.0001      |
|    loss                 | 26.1        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0015     |
|    std                  | 1           |
|    value_loss           | 88.2        |
-----------------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 613  |
|    iterations      | 3    |
|    time_elapsed    | 10   |
|    total_timesteps | 6144 |
-----------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 633          |
|    iterations           | 4            |
|    time_elapsed         | 12           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0032022279 |
|    clip_fraction        | 0.00205      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | -0.00042     |
|    learning_rate        | 0.0001       |
|    loss                 | 7.4          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00248     |
|    std                  | 1            |
|    value_loss           | 47           |
------------------------------------------
Eval num_timesteps=10000, episode_reward=-498.46 +/- 142.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -498         |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0050474806 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | -0.894       |
|    learning_rate        | 0.0001       |
|    loss                 | 3.12         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00301     |
|    std                  | 1.01         |
|    value_loss           | 23.3         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 532   |
|    iterations      | 5     |
|    time_elapsed    | 19    |
|    total_timesteps | 10240 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 540          |
|    iterations           | 6            |
|    time_elapsed         | 22           |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0009979941 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.294        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.25         |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.000311    |
|    std                  | 1.01         |
|    value_loss           | 11.2         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 539          |
|    iterations           | 7            |
|    time_elapsed         | 26           |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0017463075 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.232        |
|    learning_rate        | 0.0001       |
|    loss                 | 10.2         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00207     |
|    std                  | 1.02         |
|    value_loss           | 29.8         |
------------------------------------------
Eval num_timesteps=15000, episode_reward=-615.29 +/- 159.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -615         |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0020743208 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.252        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.45         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.01         |
|    value_loss           | 13           |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 508   |
|    iterations      | 8     |
|    time_elapsed    | 32    |
|    total_timesteps | 16384 |
------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 511           |
|    iterations           | 9             |
|    time_elapsed         | 36            |
|    total_timesteps      | 18432         |
| train/                  |               |
|    approx_kl            | 0.00056703447 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.31         |
|    explained_variance   | -0.814        |
|    learning_rate        | 0.0001        |
|    loss                 | 7.64          |
|    n_updates            | 80            |
|    policy_gradient_loss | -0.000482     |
|    std                  | 1.02          |
|    value_loss           | 34.7          |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=-463.02 +/- 177.37
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -463         |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0009563337 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.32        |
|    explained_variance   | -0.035       |
|    learning_rate        | 0.0001       |
|    loss                 | 10.1         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00116     |
|    std                  | 1.03         |
|    value_loss           | 30.4         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 489   |
|    iterations      | 10    |
|    time_elapsed    | 41    |
|    total_timesteps | 20480 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 506         |
|    iterations           | 11          |
|    time_elapsed         | 44          |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.004655262 |
|    clip_fraction        | 0.00728     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.33       |
|    explained_variance   | -0.227      |
|    learning_rate        | 0.0001      |
|    loss                 | 17.1        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00272    |
|    std                  | 1.03        |
|    value_loss           | 47.5        |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 521         |
|    iterations           | 12          |
|    time_elapsed         | 47          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.003908585 |
|    clip_fraction        | 0.0111      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.34       |
|    explained_variance   | -0.0341     |
|    learning_rate        | 0.0001      |
|    loss                 | 12.1        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00352    |
|    std                  | 1.03        |
|    value_loss           | 36.6        |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=-405.94 +/- 94.76
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.001911384 |
|    clip_fraction        | 0.00161     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.35       |
|    explained_variance   | -0.0325     |
|    learning_rate        | 0.0001      |
|    loss                 | 5.46        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.000738   |
|    std                  | 1.04        |
|    value_loss           | 15.5        |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 502   |
|    iterations      | 13    |
|    time_elapsed    | 52    |
|    total_timesteps | 26624 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 510          |
|    iterations           | 14           |
|    time_elapsed         | 56           |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0024662153 |
|    clip_fraction        | 0.00176      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.38        |
|    explained_variance   | -0.0355      |
|    learning_rate        | 0.0001       |
|    loss                 | 8.44         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00156     |
|    std                  | 1.04         |
|    value_loss           | 28.9         |
------------------------------------------
Eval num_timesteps=30000, episode_reward=-560.95 +/- 151.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -561         |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0020167804 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.39        |
|    explained_variance   | -0.00699     |
|    learning_rate        | 0.0001       |
|    loss                 | 24           |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00197     |
|    std                  | 1.05         |
|    value_loss           | 65.2         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 484   |
|    iterations      | 15    |
|    time_elapsed    | 63    |
|    total_timesteps | 30720 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 494         |
|    iterations           | 16          |
|    time_elapsed         | 66          |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.002597594 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.41       |
|    explained_variance   | 0.00303     |
|    learning_rate        | 0.0001      |
|    loss                 | 4.63        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00263    |
|    std                  | 1.06        |
|    value_loss           | 13.8        |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 502          |
|    iterations           | 17           |
|    time_elapsed         | 69           |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0007776391 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.44        |
|    explained_variance   | -0.0292      |
|    learning_rate        | 0.0001       |
|    loss                 | 4.73         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000948    |
|    std                  | 1.06         |
|    value_loss           | 16           |
------------------------------------------
Eval num_timesteps=35000, episode_reward=-510.14 +/- 248.36
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -510         |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0002607359 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.44        |
|    explained_variance   | -0.00544     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.5          |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.000555    |
|    std                  | 1.07         |
|    value_loss           | 13.6         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 491   |
|    iterations      | 18    |
|    time_elapsed    | 75    |
|    total_timesteps | 36864 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 500          |
|    iterations           | 19           |
|    time_elapsed         | 77           |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0053840005 |
|    clip_fraction        | 0.00889      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.45        |
|    explained_variance   | -0.0105      |
|    learning_rate        | 0.0001       |
|    loss                 | 2.06         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00215     |
|    std                  | 1.06         |
|    value_loss           | 4.25         |
------------------------------------------
Eval num_timesteps=40000, episode_reward=-457.77 +/- 179.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -458         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0009511941 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.44        |
|    explained_variance   | -0.00195     |
|    learning_rate        | 0.0001       |
|    loss                 | 4.1          |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.07         |
|    value_loss           | 11.4         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 488   |
|    iterations      | 20    |
|    time_elapsed    | 83    |
|    total_timesteps | 40960 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 497         |
|    iterations           | 21          |
|    time_elapsed         | 86          |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.003459623 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.47       |
|    explained_variance   | -0.000137   |
|    learning_rate        | 0.0001      |
|    loss                 | 1.18        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00255    |
|    std                  | 1.08        |
|    value_loss           | 1.97        |
-----------------------------------------
Eval num_timesteps=45000, episode_reward=-408.69 +/- 193.87
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -409          |
| time/                   |               |
|    total_timesteps      | 45000         |
| train/                  |               |
|    approx_kl            | 0.00019323087 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.49         |
|    explained_variance   | -0.00176      |
|    learning_rate        | 0.0001        |
|    loss                 | 1.69          |
|    n_updates            | 210           |
|    policy_gradient_loss | -0.000384     |
|    std                  | 1.08          |
|    value_loss           | 6.27          |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 488   |
|    iterations      | 22    |
|    time_elapsed    | 92    |
|    total_timesteps | 45056 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 495          |
|    iterations           | 23           |
|    time_elapsed         | 95           |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 8.218942e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | -0.00234     |
|    learning_rate        | 0.0001       |
|    loss                 | 6.58         |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000106    |
|    std                  | 1.08         |
|    value_loss           | 21.7         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 502          |
|    iterations           | 24           |
|    time_elapsed         | 97           |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 9.014728e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 0.000199     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.23         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000216    |
|    std                  | 1.08         |
|    value_loss           | 13.4         |
------------------------------------------
Eval num_timesteps=50000, episode_reward=-601.28 +/- 128.73
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -601          |
| time/                   |               |
|    total_timesteps      | 50000         |
| train/                  |               |
|    approx_kl            | 0.00010548727 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.49         |
|    explained_variance   | -0.00258      |
|    learning_rate        | 0.0001        |
|    loss                 | 8.66          |
|    n_updates            | 240           |
|    policy_gradient_loss | -0.000419     |
|    std                  | 1.08          |
|    value_loss           | 29.5          |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 489   |
|    iterations      | 25    |
|    time_elapsed    | 104   |
|    total_timesteps | 51200 |
------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 494           |
|    iterations           | 26            |
|    time_elapsed         | 107           |
|    total_timesteps      | 53248         |
| train/                  |               |
|    approx_kl            | 0.00011767092 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.5          |
|    explained_variance   | -0.00182      |
|    learning_rate        | 0.0001        |
|    loss                 | 4.02          |
|    n_updates            | 250           |
|    policy_gradient_loss | -0.000288     |
|    std                  | 1.08          |
|    value_loss           | 14.1          |
-------------------------------------------
Eval num_timesteps=55000, episode_reward=-524.32 +/- 110.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -524         |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0040771705 |
|    clip_fraction        | 0.00483      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.5         |
|    explained_variance   | -0.000255    |
|    learning_rate        | 0.0001       |
|    loss                 | 8.09         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.09         |
|    value_loss           | 19.2         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 487   |
|    iterations      | 27    |
|    time_elapsed    | 113   |
|    total_timesteps | 55296 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 491          |
|    iterations           | 28           |
|    time_elapsed         | 116          |
|    total_timesteps      | 57344        |
| train/                  |              |
|    approx_kl            | 0.0024099555 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.51        |
|    explained_variance   | -0.000201    |
|    learning_rate        | 0.0001       |
|    loss                 | 1.42         |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000875    |
|    std                  | 1.09         |
|    value_loss           | 20.7         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 494          |
|    iterations           | 29           |
|    time_elapsed         | 120          |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0006110774 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.51        |
|    explained_variance   | 0.00125      |
|    learning_rate        | 0.0001       |
|    loss                 | 2.95         |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00113     |
|    std                  | 1.09         |
|    value_loss           | 11.7         |
------------------------------------------
Eval num_timesteps=60000, episode_reward=-471.40 +/- 236.57
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -471          |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 4.7220412e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.51         |
|    explained_variance   | 0.000171      |
|    learning_rate        | 0.0001        |
|    loss                 | 7.58          |
|    n_updates            | 290           |
|    policy_gradient_loss | -0.00019      |
|    std                  | 1.09          |
|    value_loss           | 21            |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 483   |
|    iterations      | 30    |
|    time_elapsed    | 126   |
|    total_timesteps | 61440 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 489          |
|    iterations           | 31           |
|    time_elapsed         | 129          |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0010561822 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.52        |
|    explained_variance   | 0.000441     |
|    learning_rate        | 0.0001       |
|    loss                 | 2.18         |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.0017      |
|    std                  | 1.09         |
|    value_loss           | 9.99         |
------------------------------------------
Eval num_timesteps=65000, episode_reward=-395.14 +/- 166.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -395         |
| time/                   |              |
|    total_timesteps      | 65000        |
| train/                  |              |
|    approx_kl            | 0.0034313204 |
|    clip_fraction        | 0.00342      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.53        |
|    explained_variance   | -9.26e-05    |
|    learning_rate        | 0.0001       |
|    loss                 | 2.3          |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.0011      |
|    std                  | 1.09         |
|    value_loss           | 5.53         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 480   |
|    iterations      | 32    |
|    time_elapsed    | 136   |
|    total_timesteps | 65536 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 484          |
|    iterations           | 33           |
|    time_elapsed         | 139          |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0012337938 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.53        |
|    explained_variance   | 0.000239     |
|    learning_rate        | 0.0001       |
|    loss                 | 3.74         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00108     |
|    std                  | 1.09         |
|    value_loss           | 10.3         |
------------------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 489           |
|    iterations           | 34            |
|    time_elapsed         | 142           |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 8.6964516e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.53         |
|    explained_variance   | 0.000114      |
|    learning_rate        | 0.0001        |
|    loss                 | 8.17          |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.00013      |
|    std                  | 1.1           |
|    value_loss           | 23.4          |
-------------------------------------------
Eval num_timesteps=70000, episode_reward=-411.67 +/- 207.67
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.004180275 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.54       |
|    explained_variance   | 0.000181    |
|    learning_rate        | 0.0001      |
|    loss                 | 6.94        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00244    |
|    std                  | 1.1         |
|    value_loss           | 15.3        |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 480   |
|    iterations      | 35    |
|    time_elapsed    | 149   |
|    total_timesteps | 71680 |
------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 485           |
|    iterations           | 36            |
|    time_elapsed         | 151           |
|    total_timesteps      | 73728         |
| train/                  |               |
|    approx_kl            | 8.2292914e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.55         |
|    explained_variance   | 0.00112       |
|    learning_rate        | 0.0001        |
|    loss                 | 13.4          |
|    n_updates            | 350           |
|    policy_gradient_loss | -0.000178     |
|    std                  | 1.1           |
|    value_loss           | 33.7          |
-------------------------------------------
Eval num_timesteps=75000, episode_reward=-392.72 +/- 103.84
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -393        |
| time/                   |             |
|    total_timesteps      | 75000       |
| train/                  |             |
|    approx_kl            | 0.008487399 |
|    clip_fraction        | 0.0701      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.58       |
|    explained_variance   | -0.00198    |
|    learning_rate        | 0.0001      |
|    loss                 | 0.326       |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.00589    |
|    std                  | 1.13        |
|    value_loss           | 2           |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 477   |
|    iterations      | 37    |
|    time_elapsed    | 158   |
|    total_timesteps | 75776 |
------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 479        |
|    iterations           | 38         |
|    time_elapsed         | 162        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.00441748 |
|    clip_fraction        | 0.0188     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.61      |
|    explained_variance   | 0.0414     |
|    learning_rate        | 0.0001     |
|    loss                 | 0.186      |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.00357   |
|    std                  | 1.12       |
|    value_loss           | 1.36       |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 483         |
|    iterations           | 39          |
|    time_elapsed         | 165         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.007650164 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.62       |
|    explained_variance   | 0.0267      |
|    learning_rate        | 0.0001      |
|    loss                 | 0.635       |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.0037     |
|    std                  | 1.13        |
|    value_loss           | 1.5         |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=-354.14 +/- 122.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -354         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0041601933 |
|    clip_fraction        | 0.00381      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.63        |
|    explained_variance   | 0.277        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.408        |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00219     |
|    std                  | 1.13         |
|    value_loss           | 2.05         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 479   |
|    iterations      | 40    |
|    time_elapsed    | 171   |
|    total_timesteps | 81920 |
------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 483           |
|    iterations           | 41            |
|    time_elapsed         | 173           |
|    total_timesteps      | 83968         |
| train/                  |               |
|    approx_kl            | 0.00060283625 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.63         |
|    explained_variance   | 0.598         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.684         |
|    n_updates            | 400           |
|    policy_gradient_loss | -0.00118      |
|    std                  | 1.13          |
|    value_loss           | 4.35          |
-------------------------------------------
Eval num_timesteps=85000, episode_reward=-458.69 +/- 136.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -459         |
| time/                   |              |
|    total_timesteps      | 85000        |
| train/                  |              |
|    approx_kl            | 0.0054731467 |
|    clip_fraction        | 0.0351       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.64        |
|    explained_variance   | 0.893        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.437        |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00503     |
|    std                  | 1.14         |
|    value_loss           | 1.1          |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 477   |
|    iterations      | 42    |
|    time_elapsed    | 180   |
|    total_timesteps | 86016 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 43           |
|    time_elapsed         | 183          |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0002113686 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.65        |
|    explained_variance   | 0.795        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.627        |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.000225    |
|    std                  | 1.14         |
|    value_loss           | 3.98         |
------------------------------------------
Eval num_timesteps=90000, episode_reward=-322.87 +/- 128.10
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -323        |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.001831282 |
|    clip_fraction        | 0.00205     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.66       |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.174       |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.000841   |
|    std                  | 1.15        |
|    value_loss           | 1.45        |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 475   |
|    iterations      | 44    |
|    time_elapsed    | 189   |
|    total_timesteps | 90112 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 478         |
|    iterations           | 45          |
|    time_elapsed         | 192         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.004060382 |
|    clip_fraction        | 0.00557     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.67       |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.39        |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00131    |
|    std                  | 1.15        |
|    value_loss           | 1.85        |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 46           |
|    time_elapsed         | 196          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0026133517 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.67        |
|    explained_variance   | 0.119        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.55         |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.15         |
|    value_loss           | 3.51         |
------------------------------------------
Eval num_timesteps=95000, episode_reward=-513.15 +/- 82.04
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -513          |
| time/                   |               |
|    total_timesteps      | 95000         |
| train/                  |               |
|    approx_kl            | 8.0671976e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.67         |
|    explained_variance   | 0.398         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.74          |
|    n_updates            | 460           |
|    policy_gradient_loss | -0.000155     |
|    std                  | 1.15          |
|    value_loss           | 23.4          |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 476   |
|    iterations      | 47    |
|    time_elapsed    | 202   |
|    total_timesteps | 96256 |
------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 479           |
|    iterations           | 48            |
|    time_elapsed         | 204           |
|    total_timesteps      | 98304         |
| train/                  |               |
|    approx_kl            | 0.00066528376 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.67         |
|    explained_variance   | 0.832         |
|    learning_rate        | 0.0001        |
|    loss                 | 2.8           |
|    n_updates            | 470           |
|    policy_gradient_loss | -0.000722     |
|    std                  | 1.15          |
|    value_loss           | 8.12          |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=-407.55 +/- 145.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -408         |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0014022649 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.68        |
|    explained_variance   | 0.763        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.57         |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00166     |
|    std                  | 1.15         |
|    value_loss           | 7.71         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 49     |
|    time_elapsed    | 210    |
|    total_timesteps | 100352 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 50           |
|    time_elapsed         | 213          |
|    total_timesteps      | 102400       |
| train/                  |              |
|    approx_kl            | 0.0014274524 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.68        |
|    explained_variance   | 0.425        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.55         |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00141     |
|    std                  | 1.15         |
|    value_loss           | 11.3         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 51           |
|    time_elapsed         | 216          |
|    total_timesteps      | 104448       |
| train/                  |              |
|    approx_kl            | 0.0061871205 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.68        |
|    explained_variance   | 0.804        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.585        |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00388     |
|    std                  | 1.16         |
|    value_loss           | 1.72         |
------------------------------------------
Eval num_timesteps=105000, episode_reward=-398.73 +/- 85.36
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -399         |
| time/                   |              |
|    total_timesteps      | 105000       |
| train/                  |              |
|    approx_kl            | 0.0013051874 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.24         |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.00173     |
|    std                  | 1.16         |
|    value_loss           | 4.45         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 52     |
|    time_elapsed    | 222    |
|    total_timesteps | 106496 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 53           |
|    time_elapsed         | 225          |
|    total_timesteps      | 108544       |
| train/                  |              |
|    approx_kl            | 0.0038072763 |
|    clip_fraction        | 0.00615      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.14         |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00304     |
|    std                  | 1.16         |
|    value_loss           | 11.2         |
------------------------------------------
Eval num_timesteps=110000, episode_reward=-610.25 +/- 98.22
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -610        |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.002289311 |
|    clip_fraction        | 0.00156     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.69       |
|    explained_variance   | 0.961       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.465       |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.00199    |
|    std                  | 1.16        |
|    value_loss           | 2.57        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 54     |
|    time_elapsed    | 231    |
|    total_timesteps | 110592 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 55            |
|    time_elapsed         | 233           |
|    total_timesteps      | 112640        |
| train/                  |               |
|    approx_kl            | 0.00090808654 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.69         |
|    explained_variance   | 0.763         |
|    learning_rate        | 0.0001        |
|    loss                 | 50.8          |
|    n_updates            | 540           |
|    policy_gradient_loss | -0.000972     |
|    std                  | 1.16          |
|    value_loss           | 52.2          |
-------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 484          |
|    iterations           | 56           |
|    time_elapsed         | 236          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0040642973 |
|    clip_fraction        | 0.00903      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.7         |
|    explained_variance   | 0.699        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.499        |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00135     |
|    std                  | 1.16         |
|    value_loss           | 3.37         |
------------------------------------------
Eval num_timesteps=115000, episode_reward=-480.93 +/- 82.53
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -481        |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.002215236 |
|    clip_fraction        | 0.00244     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.968       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.57        |
|    n_updates            | 560         |
|    policy_gradient_loss | -0.000609   |
|    std                  | 1.16        |
|    value_loss           | 2.15        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 479    |
|    iterations      | 57     |
|    time_elapsed    | 243    |
|    total_timesteps | 116736 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 58          |
|    time_elapsed         | 246         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.002111121 |
|    clip_fraction        | 0.00127     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.7        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.45        |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00149    |
|    std                  | 1.16        |
|    value_loss           | 3.12        |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=-453.44 +/- 51.77
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -453         |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0039945035 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.7         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.444        |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00132     |
|    std                  | 1.16         |
|    value_loss           | 2.23         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 59     |
|    time_elapsed    | 252    |
|    total_timesteps | 120832 |
-------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 479        |
|    iterations           | 60         |
|    time_elapsed         | 256        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.00562553 |
|    clip_fraction        | 0.0202     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.71      |
|    explained_variance   | 0.984      |
|    learning_rate        | 0.0001     |
|    loss                 | 4.47       |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.00411   |
|    std                  | 1.16       |
|    value_loss           | 7.51       |
----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 61           |
|    time_elapsed         | 259          |
|    total_timesteps      | 124928       |
| train/                  |              |
|    approx_kl            | 0.0053107226 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.71        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.452        |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00344     |
|    std                  | 1.16         |
|    value_loss           | 2.49         |
------------------------------------------
Eval num_timesteps=125000, episode_reward=-425.10 +/- 155.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -425         |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0030359768 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.7         |
|    explained_variance   | 0.472        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.261        |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.000836    |
|    std                  | 1.16         |
|    value_loss           | 3.35         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 62     |
|    time_elapsed    | 265    |
|    total_timesteps | 126976 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 63           |
|    time_elapsed         | 268          |
|    total_timesteps      | 129024       |
| train/                  |              |
|    approx_kl            | 0.0053899894 |
|    clip_fraction        | 0.0427       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.7         |
|    explained_variance   | 0.92         |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0413      |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00316     |
|    std                  | 1.16         |
|    value_loss           | 0.474        |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-481.08 +/- 175.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -481         |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0049882913 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.71        |
|    explained_variance   | 0.957        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.299        |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00342     |
|    std                  | 1.16         |
|    value_loss           | 2.9          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 64     |
|    time_elapsed    | 274    |
|    total_timesteps | 131072 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 65            |
|    time_elapsed         | 277           |
|    total_timesteps      | 133120        |
| train/                  |               |
|    approx_kl            | 0.00018667139 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.71         |
|    explained_variance   | 0.974         |
|    learning_rate        | 0.0001        |
|    loss                 | 35.2          |
|    n_updates            | 640           |
|    policy_gradient_loss | -0.00062      |
|    std                  | 1.16          |
|    value_loss           | 39.1          |
-------------------------------------------
Eval num_timesteps=135000, episode_reward=-395.35 +/- 149.68
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -395         |
| time/                   |              |
|    total_timesteps      | 135000       |
| train/                  |              |
|    approx_kl            | 0.0050857253 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.71        |
|    explained_variance   | 0.771        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.214        |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00318     |
|    std                  | 1.17         |
|    value_loss           | 2.03         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 66     |
|    time_elapsed    | 283    |
|    total_timesteps | 135168 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 479         |
|    iterations           | 67          |
|    time_elapsed         | 286         |
|    total_timesteps      | 137216      |
| train/                  |             |
|    approx_kl            | 0.003262275 |
|    clip_fraction        | 0.00249     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.73       |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.108       |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.00159    |
|    std                  | 1.17        |
|    value_loss           | 2.84        |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 68           |
|    time_elapsed         | 289          |
|    total_timesteps      | 139264       |
| train/                  |              |
|    approx_kl            | 0.0037178914 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.74        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00104      |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.00235     |
|    std                  | 1.18         |
|    value_loss           | 0.986        |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-340.79 +/- 64.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -341         |
| time/                   |              |
|    total_timesteps      | 140000       |
| train/                  |              |
|    approx_kl            | 0.0051937047 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.75        |
|    explained_variance   | 0.931        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00821     |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.003       |
|    std                  | 1.18         |
|    value_loss           | 0.453        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 479    |
|    iterations      | 69     |
|    time_elapsed    | 294    |
|    total_timesteps | 141312 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 70           |
|    time_elapsed         | 297          |
|    total_timesteps      | 143360       |
| train/                  |              |
|    approx_kl            | 0.0038642082 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.76        |
|    explained_variance   | 0.844        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.118        |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00185     |
|    std                  | 1.19         |
|    value_loss           | 1.33         |
------------------------------------------
Eval num_timesteps=145000, episode_reward=-377.89 +/- 67.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -378         |
| time/                   |              |
|    total_timesteps      | 145000       |
| train/                  |              |
|    approx_kl            | 0.0029202397 |
|    clip_fraction        | 0.00195      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.77        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.102        |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.002       |
|    std                  | 1.19         |
|    value_loss           | 2.03         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 479    |
|    iterations      | 71     |
|    time_elapsed    | 303    |
|    total_timesteps | 145408 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 72           |
|    time_elapsed         | 306          |
|    total_timesteps      | 147456       |
| train/                  |              |
|    approx_kl            | 0.0028165858 |
|    clip_fraction        | 0.000928     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.77        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.112        |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.19         |
|    value_loss           | 1.85         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 73           |
|    time_elapsed         | 310          |
|    total_timesteps      | 149504       |
| train/                  |              |
|    approx_kl            | 0.0042243823 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.78        |
|    explained_variance   | 0.707        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.046        |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.19         |
|    value_loss           | 1.4          |
------------------------------------------
Eval num_timesteps=150000, episode_reward=-362.95 +/- 168.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -363         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0019757124 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.78        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00211      |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00111     |
|    std                  | 1.19         |
|    value_loss           | 0.766        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 479    |
|    iterations      | 74     |
|    time_elapsed    | 315    |
|    total_timesteps | 151552 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 75           |
|    time_elapsed         | 318          |
|    total_timesteps      | 153600       |
| train/                  |              |
|    approx_kl            | 0.0007681332 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.78        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.77         |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.000796    |
|    std                  | 1.19         |
|    value_loss           | 12.5         |
------------------------------------------
Eval num_timesteps=155000, episode_reward=-364.56 +/- 117.58
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 155000       |
| train/                  |              |
|    approx_kl            | 0.0060833776 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.79        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0667      |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00424     |
|    std                  | 1.2          |
|    value_loss           | 0.381        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 479    |
|    iterations      | 76     |
|    time_elapsed    | 324    |
|    total_timesteps | 155648 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 481           |
|    iterations           | 77            |
|    time_elapsed         | 327           |
|    total_timesteps      | 157696        |
| train/                  |               |
|    approx_kl            | 0.00038260804 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.8          |
|    explained_variance   | 0.978         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.072         |
|    n_updates            | 760           |
|    policy_gradient_loss | -0.000141     |
|    std                  | 1.2           |
|    value_loss           | 3.71          |
-------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 484          |
|    iterations           | 78           |
|    time_elapsed         | 329          |
|    total_timesteps      | 159744       |
| train/                  |              |
|    approx_kl            | 0.0050736116 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0759       |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00228     |
|    std                  | 1.2          |
|    value_loss           | 0.716        |
------------------------------------------
Eval num_timesteps=160000, episode_reward=-393.85 +/- 186.35
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -394        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.006914966 |
|    clip_fraction        | 0.051       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.81       |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0725      |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00426    |
|    std                  | 1.2         |
|    value_loss           | 0.393       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 480    |
|    iterations      | 79     |
|    time_elapsed    | 337    |
|    total_timesteps | 161792 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 80           |
|    time_elapsed         | 340          |
|    total_timesteps      | 163840       |
| train/                  |              |
|    approx_kl            | 0.0030843094 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.83        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0301       |
|    n_updates            | 790          |
|    policy_gradient_loss | -0.00194     |
|    std                  | 1.21         |
|    value_loss           | 0.272        |
------------------------------------------
Eval num_timesteps=165000, episode_reward=-408.84 +/- 123.87
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -409         |
| time/                   |              |
|    total_timesteps      | 165000       |
| train/                  |              |
|    approx_kl            | 0.0030901479 |
|    clip_fraction        | 0.00327      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.84        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0633      |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.22         |
|    value_loss           | 0.153        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 479    |
|    iterations      | 81     |
|    time_elapsed    | 345    |
|    total_timesteps | 165888 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 82           |
|    time_elapsed         | 348          |
|    total_timesteps      | 167936       |
| train/                  |              |
|    approx_kl            | 0.0002809479 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.85        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0001       |
|    loss                 | 12.9         |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.000499    |
|    std                  | 1.22         |
|    value_loss           | 42.7         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 83           |
|    time_elapsed         | 351          |
|    total_timesteps      | 169984       |
| train/                  |              |
|    approx_kl            | 0.0015680059 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.85        |
|    explained_variance   | 0.949        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00409     |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00129     |
|    std                  | 1.22         |
|    value_loss           | 0.915        |
------------------------------------------
Eval num_timesteps=170000, episode_reward=-323.84 +/- 86.66
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -324        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.004056789 |
|    clip_fraction        | 0.00669     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.86       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0699      |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00132    |
|    std                  | 1.23        |
|    value_loss           | 1.04        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 480    |
|    iterations      | 84     |
|    time_elapsed    | 358    |
|    total_timesteps | 172032 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 482          |
|    iterations           | 85           |
|    time_elapsed         | 361          |
|    total_timesteps      | 174080       |
| train/                  |              |
|    approx_kl            | 0.0037506074 |
|    clip_fraction        | 0.00791      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.87        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.037        |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00152     |
|    std                  | 1.23         |
|    value_loss           | 14.1         |
------------------------------------------
Eval num_timesteps=175000, episode_reward=-364.39 +/- 183.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -364         |
| time/                   |              |
|    total_timesteps      | 175000       |
| train/                  |              |
|    approx_kl            | 0.0036945785 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.87        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0485      |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00145     |
|    std                  | 1.23         |
|    value_loss           | 0.697        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 479    |
|    iterations      | 86     |
|    time_elapsed    | 367    |
|    total_timesteps | 176128 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 87            |
|    time_elapsed         | 370           |
|    total_timesteps      | 178176        |
| train/                  |               |
|    approx_kl            | 0.00016980528 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.87         |
|    explained_variance   | 0.98          |
|    learning_rate        | 0.0001        |
|    loss                 | 17.3          |
|    n_updates            | 860           |
|    policy_gradient_loss | -0.000268     |
|    std                  | 1.23          |
|    value_loss           | 31.5          |
-------------------------------------------
Eval num_timesteps=180000, episode_reward=-375.77 +/- 139.76
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -376       |
| time/                   |            |
|    total_timesteps      | 180000     |
| train/                  |            |
|    approx_kl            | 0.00440824 |
|    clip_fraction        | 0.0265     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.87      |
|    explained_variance   | 0.995      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0571    |
|    n_updates            | 870        |
|    policy_gradient_loss | -0.00212   |
|    std                  | 1.22       |
|    value_loss           | 0.322      |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 88     |
|    time_elapsed    | 377    |
|    total_timesteps | 180224 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 478          |
|    iterations           | 89           |
|    time_elapsed         | 381          |
|    total_timesteps      | 182272       |
| train/                  |              |
|    approx_kl            | 0.0020815518 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.87        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.082        |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00199     |
|    std                  | 1.23         |
|    value_loss           | 2.77         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 90           |
|    time_elapsed         | 383          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.0073422086 |
|    clip_fraction        | 0.0723       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.88        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.089       |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.00697     |
|    std                  | 1.24         |
|    value_loss           | 0.156        |
------------------------------------------
Eval num_timesteps=185000, episode_reward=-325.01 +/- 94.09
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -325        |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.005177605 |
|    clip_fraction        | 0.0158      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.9        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0141      |
|    n_updates            | 900         |
|    policy_gradient_loss | -0.00253    |
|    std                  | 1.24        |
|    value_loss           | 0.567       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 91     |
|    time_elapsed    | 389    |
|    total_timesteps | 186368 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 479           |
|    iterations           | 92            |
|    time_elapsed         | 392           |
|    total_timesteps      | 188416        |
| train/                  |               |
|    approx_kl            | 0.00031063872 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.9          |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.582         |
|    n_updates            | 910           |
|    policy_gradient_loss | -0.00031      |
|    std                  | 1.24          |
|    value_loss           | 25.4          |
-------------------------------------------
Eval num_timesteps=190000, episode_reward=-306.78 +/- 172.15
Episode length: 828.60 +/- 344.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 829          |
|    mean_reward          | -307         |
| time/                   |              |
|    total_timesteps      | 190000       |
| train/                  |              |
|    approx_kl            | 0.0031196903 |
|    clip_fraction        | 0.00908      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.9         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0927      |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.000882    |
|    std                  | 1.24         |
|    value_loss           | 0.0896       |
------------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 93     |
|    time_elapsed    | 398    |
|    total_timesteps | 190464 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 479         |
|    iterations           | 94          |
|    time_elapsed         | 401         |
|    total_timesteps      | 192512      |
| train/                  |             |
|    approx_kl            | 0.001060628 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.91       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.583       |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.000924   |
|    std                  | 1.24        |
|    value_loss           | 8.09        |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 95          |
|    time_elapsed         | 404         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.005865487 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.92       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00979    |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.00417    |
|    std                  | 1.25        |
|    value_loss           | 0.476       |
-----------------------------------------
Eval num_timesteps=195000, episode_reward=-356.86 +/- 57.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -357         |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0032658929 |
|    clip_fraction        | 0.00239      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.93        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0509      |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.25         |
|    value_loss           | 0.336        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 479    |
|    iterations      | 96     |
|    time_elapsed    | 410    |
|    total_timesteps | 196608 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 480           |
|    iterations           | 97            |
|    time_elapsed         | 413           |
|    total_timesteps      | 198656        |
| train/                  |               |
|    approx_kl            | 0.00016801237 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.93         |
|    explained_variance   | 0.968         |
|    learning_rate        | 0.0001        |
|    loss                 | 15            |
|    n_updates            | 960           |
|    policy_gradient_loss | 3.96e-06      |
|    std                  | 1.25          |
|    value_loss           | 21.8          |
-------------------------------------------
Eval num_timesteps=200000, episode_reward=-468.53 +/- 191.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -469         |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0056127803 |
|    clip_fraction        | 0.0191       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.93        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0467      |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00324     |
|    std                  | 1.25         |
|    value_loss           | 0.258        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 98     |
|    time_elapsed    | 419    |
|    total_timesteps | 200704 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 99           |
|    time_elapsed         | 422          |
|    total_timesteps      | 202752       |
| train/                  |              |
|    approx_kl            | 0.0012353982 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.94        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0382      |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.000386    |
|    std                  | 1.26         |
|    value_loss           | 0.463        |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 100          |
|    time_elapsed         | 425          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0045346417 |
|    clip_fraction        | 0.0207       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.95        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0312       |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.00245     |
|    std                  | 1.26         |
|    value_loss           | 0.737        |
------------------------------------------
Eval num_timesteps=205000, episode_reward=-466.87 +/- 119.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -467         |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0055145463 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.95        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0578       |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 1.26         |
|    value_loss           | 0.633        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 101    |
|    time_elapsed    | 432    |
|    total_timesteps | 206848 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 102          |
|    time_elapsed         | 434          |
|    total_timesteps      | 208896       |
| train/                  |              |
|    approx_kl            | 0.0064983275 |
|    clip_fraction        | 0.022        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.95        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0437      |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00431     |
|    std                  | 1.26         |
|    value_loss           | 0.347        |
------------------------------------------
Eval num_timesteps=210000, episode_reward=-387.36 +/- 143.06
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -387        |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.002425967 |
|    clip_fraction        | 0.00635     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.95       |
|    explained_variance   | 0.876       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0246     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.000474   |
|    std                  | 1.27        |
|    value_loss           | 0.533       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 103    |
|    time_elapsed    | 440    |
|    total_timesteps | 210944 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 104          |
|    time_elapsed         | 443          |
|    total_timesteps      | 212992       |
| train/                  |              |
|    approx_kl            | 0.0059905807 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.97        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0539      |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00314     |
|    std                  | 1.27         |
|    value_loss           | 0.18         |
------------------------------------------
Eval num_timesteps=215000, episode_reward=-394.88 +/- 126.74
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -395        |
| time/                   |             |
|    total_timesteps      | 215000      |
| train/                  |             |
|    approx_kl            | 0.005956558 |
|    clip_fraction        | 0.0317      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.98       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00305     |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.00347    |
|    std                  | 1.28        |
|    value_loss           | 0.459       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 105    |
|    time_elapsed    | 449    |
|    total_timesteps | 215040 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 106          |
|    time_elapsed         | 452          |
|    total_timesteps      | 217088       |
| train/                  |              |
|    approx_kl            | 0.0031723073 |
|    clip_fraction        | 0.00435      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5           |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.423        |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.00215     |
|    std                  | 1.28         |
|    value_loss           | 0.785        |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 481          |
|    iterations           | 107          |
|    time_elapsed         | 455          |
|    total_timesteps      | 219136       |
| train/                  |              |
|    approx_kl            | 0.0032437209 |
|    clip_fraction        | 0.00391      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.01        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0947       |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.00103     |
|    std                  | 1.29         |
|    value_loss           | 0.656        |
------------------------------------------
Eval num_timesteps=220000, episode_reward=-332.24 +/- 93.17
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -332        |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.004280746 |
|    clip_fraction        | 0.00937     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.02       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0318      |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00243    |
|    std                  | 1.29        |
|    value_loss           | 0.759       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 108    |
|    time_elapsed    | 461    |
|    total_timesteps | 221184 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 479           |
|    iterations           | 109           |
|    time_elapsed         | 465           |
|    total_timesteps      | 223232        |
| train/                  |               |
|    approx_kl            | 0.00027629858 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.02         |
|    explained_variance   | 0.983         |
|    learning_rate        | 0.0001        |
|    loss                 | 11.1          |
|    n_updates            | 1080          |
|    policy_gradient_loss | -0.000143     |
|    std                  | 1.29          |
|    value_loss           | 11.1          |
-------------------------------------------
Eval num_timesteps=225000, episode_reward=-320.09 +/- 235.23
Episode length: 815.40 +/- 371.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -320         |
| time/                   |              |
|    total_timesteps      | 225000       |
| train/                  |              |
|    approx_kl            | 0.0035090828 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.03        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0829      |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 1.29         |
|    value_loss           | 0.203        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 476    |
|    iterations      | 110    |
|    time_elapsed    | 472    |
|    total_timesteps | 225280 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 111          |
|    time_elapsed         | 475          |
|    total_timesteps      | 227328       |
| train/                  |              |
|    approx_kl            | 0.0019538426 |
|    clip_fraction        | 0.00225      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.03        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.173        |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00096     |
|    std                  | 1.3          |
|    value_loss           | 0.562        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 479         |
|    iterations           | 112         |
|    time_elapsed         | 478         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.004510106 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0069      |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00332    |
|    std                  | 1.31        |
|    value_loss           | 0.7         |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=-217.62 +/- 35.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -218         |
| time/                   |              |
|    total_timesteps      | 230000       |
| train/                  |              |
|    approx_kl            | 0.0048713298 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.06        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0915      |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00357     |
|    std                  | 1.31         |
|    value_loss           | 0.334        |
------------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 113    |
|    time_elapsed    | 484    |
|    total_timesteps | 231424 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 478         |
|    iterations           | 114         |
|    time_elapsed         | 487         |
|    total_timesteps      | 233472      |
| train/                  |             |
|    approx_kl            | 0.004493839 |
|    clip_fraction        | 0.00928     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.06       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.059      |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00107    |
|    std                  | 1.31        |
|    value_loss           | 0.303       |
-----------------------------------------
Eval num_timesteps=235000, episode_reward=-361.40 +/- 101.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -361         |
| time/                   |              |
|    total_timesteps      | 235000       |
| train/                  |              |
|    approx_kl            | 0.0030653854 |
|    clip_fraction        | 0.00464      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.07        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00703      |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.31         |
|    value_loss           | 1.05         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 115    |
|    time_elapsed    | 493    |
|    total_timesteps | 235520 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 478          |
|    iterations           | 116          |
|    time_elapsed         | 496          |
|    total_timesteps      | 237568       |
| train/                  |              |
|    approx_kl            | 0.0013449081 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.07        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.442        |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.31         |
|    value_loss           | 5.87         |
------------------------------------------
---------------------------------------
| time/                   |           |
|    fps                  | 480       |
|    iterations           | 117       |
|    time_elapsed         | 498       |
|    total_timesteps      | 239616    |
| train/                  |           |
|    approx_kl            | 0.0065281 |
|    clip_fraction        | 0.045     |
|    clip_range           | 0.2       |
|    entropy_loss         | -5.08     |
|    explained_variance   | 0.951     |
|    learning_rate        | 0.0001    |
|    loss                 | -0.0881   |
|    n_updates            | 1160      |
|    policy_gradient_loss | -0.00575  |
|    std                  | 1.32      |
|    value_loss           | 0.738     |
---------------------------------------
Eval num_timesteps=240000, episode_reward=-359.87 +/- 129.26
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -360       |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.00336486 |
|    clip_fraction        | 0.00859    |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.1       |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.0683    |
|    n_updates            | 1170       |
|    policy_gradient_loss | -0.00104   |
|    std                  | 1.32       |
|    value_loss           | 0.282      |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 118    |
|    time_elapsed    | 505    |
|    total_timesteps | 241664 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 119          |
|    time_elapsed         | 508          |
|    total_timesteps      | 243712       |
| train/                  |              |
|    approx_kl            | 0.0008174204 |
|    clip_fraction        | 0.000684     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.1         |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0001       |
|    loss                 | 16.9         |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.000856    |
|    std                  | 1.33         |
|    value_loss           | 7.48         |
------------------------------------------
Eval num_timesteps=245000, episode_reward=-322.42 +/- 218.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -322        |
| time/                   |             |
|    total_timesteps      | 245000      |
| train/                  |             |
|    approx_kl            | 0.006608413 |
|    clip_fraction        | 0.0435      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.12       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0117     |
|    n_updates            | 1190        |
|    policy_gradient_loss | -0.0044     |
|    std                  | 1.34        |
|    value_loss           | 0.194       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 120    |
|    time_elapsed    | 514    |
|    total_timesteps | 245760 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 121          |
|    time_elapsed         | 517          |
|    total_timesteps      | 247808       |
| train/                  |              |
|    approx_kl            | 0.0013924966 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.13        |
|    explained_variance   | 0.944        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.015        |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 1.34         |
|    value_loss           | 2.01         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 122          |
|    time_elapsed         | 520          |
|    total_timesteps      | 249856       |
| train/                  |              |
|    approx_kl            | 0.0027229334 |
|    clip_fraction        | 0.00498      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.15        |
|    explained_variance   | 0.484        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.122        |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 1.35         |
|    value_loss           | 1.17         |
------------------------------------------
Eval num_timesteps=250000, episode_reward=-288.87 +/- 175.50
Episode length: 817.20 +/- 367.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 817          |
|    mean_reward          | -289         |
| time/                   |              |
|    total_timesteps      | 250000       |
| train/                  |              |
|    approx_kl            | 0.0047010547 |
|    clip_fraction        | 0.00957      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.18        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0872      |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 1.37         |
|    value_loss           | 0.497        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 123    |
|    time_elapsed    | 526    |
|    total_timesteps | 251904 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 479          |
|    iterations           | 124          |
|    time_elapsed         | 529          |
|    total_timesteps      | 253952       |
| train/                  |              |
|    approx_kl            | 0.0040310873 |
|    clip_fraction        | 0.00527      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.2         |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0001       |
|    loss                 | 8.49         |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 1.37         |
|    value_loss           | 13.8         |
------------------------------------------
Eval num_timesteps=255000, episode_reward=-329.02 +/- 162.44
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -329        |
| time/                   |             |
|    total_timesteps      | 255000      |
| train/                  |             |
|    approx_kl            | 0.006051177 |
|    clip_fraction        | 0.0369      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.2        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0814     |
|    n_updates            | 1240        |
|    policy_gradient_loss | -0.00356    |
|    std                  | 1.37        |
|    value_loss           | 0.24        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 125    |
|    time_elapsed    | 536    |
|    total_timesteps | 256000 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 478         |
|    iterations           | 126         |
|    time_elapsed         | 539         |
|    total_timesteps      | 258048      |
| train/                  |             |
|    approx_kl            | 0.005958697 |
|    clip_fraction        | 0.0259      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.2        |
|    explained_variance   | 0.959       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0275      |
|    n_updates            | 1250        |
|    policy_gradient_loss | -0.00394    |
|    std                  | 1.37        |
|    value_loss           | 0.54        |
-----------------------------------------
Eval num_timesteps=260000, episode_reward=-315.10 +/- 142.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -315         |
| time/                   |              |
|    total_timesteps      | 260000       |
| train/                  |              |
|    approx_kl            | 0.0056871036 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.21        |
|    explained_variance   | 0.0423       |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0841      |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.00377     |
|    std                  | 1.38         |
|    value_loss           | 0.323        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 476    |
|    iterations      | 127    |
|    time_elapsed    | 545    |
|    total_timesteps | 260096 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 128          |
|    time_elapsed         | 548          |
|    total_timesteps      | 262144       |
| train/                  |              |
|    approx_kl            | 0.0015681061 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.22        |
|    explained_variance   | 0.676        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.53         |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.000438    |
|    std                  | 1.38         |
|    value_loss           | 4.32         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 478          |
|    iterations           | 129          |
|    time_elapsed         | 552          |
|    total_timesteps      | 264192       |
| train/                  |              |
|    approx_kl            | 0.0024338278 |
|    clip_fraction        | 0.00171      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.23        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0461      |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.000858    |
|    std                  | 1.39         |
|    value_loss           | 0.249        |
------------------------------------------
Eval num_timesteps=265000, episode_reward=-325.42 +/- 71.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -325        |
| time/                   |             |
|    total_timesteps      | 265000      |
| train/                  |             |
|    approx_kl            | 0.004488335 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.25       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0998     |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00342    |
|    std                  | 1.4         |
|    value_loss           | 0.0909      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 476    |
|    iterations      | 130    |
|    time_elapsed    | 559    |
|    total_timesteps | 266240 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 131          |
|    time_elapsed         | 562          |
|    total_timesteps      | 268288       |
| train/                  |              |
|    approx_kl            | 0.0033718245 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.53         |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.4          |
|    value_loss           | 2.91         |
------------------------------------------
Eval num_timesteps=270000, episode_reward=-326.55 +/- 70.74
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -327        |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.004909388 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.27       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00148    |
|    n_updates            | 1310        |
|    policy_gradient_loss | -0.00386    |
|    std                  | 1.4         |
|    value_loss           | 0.182       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 132    |
|    time_elapsed    | 568    |
|    total_timesteps | 270336 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 476           |
|    iterations           | 133           |
|    time_elapsed         | 571           |
|    total_timesteps      | 272384        |
| train/                  |               |
|    approx_kl            | 0.00040165114 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.28         |
|    explained_variance   | 0.594         |
|    learning_rate        | 0.0001        |
|    loss                 | 1.2           |
|    n_updates            | 1320          |
|    policy_gradient_loss | -0.000329     |
|    std                  | 1.41          |
|    value_loss           | 15.8          |
-------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 477          |
|    iterations           | 134          |
|    time_elapsed         | 574          |
|    total_timesteps      | 274432       |
| train/                  |              |
|    approx_kl            | 0.0043509556 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.088       |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.00173     |
|    std                  | 1.41         |
|    value_loss           | 0.185        |
------------------------------------------
Eval num_timesteps=275000, episode_reward=-377.33 +/- 102.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -377         |
| time/                   |              |
|    total_timesteps      | 275000       |
| train/                  |              |
|    approx_kl            | 0.0046125837 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0628       |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.0023      |
|    std                  | 1.4          |
|    value_loss           | 0.624        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 476    |
|    iterations      | 135    |
|    time_elapsed    | 580    |
|    total_timesteps | 276480 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 477           |
|    iterations           | 136           |
|    time_elapsed         | 583           |
|    total_timesteps      | 278528        |
| train/                  |               |
|    approx_kl            | 0.00045936054 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.27         |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.0001        |
|    loss                 | 11.5          |
|    n_updates            | 1350          |
|    policy_gradient_loss | -0.000461     |
|    std                  | 1.4           |
|    value_loss           | 21.5          |
-------------------------------------------
Eval num_timesteps=280000, episode_reward=-326.29 +/- 66.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -326         |
| time/                   |              |
|    total_timesteps      | 280000       |
| train/                  |              |
|    approx_kl            | 0.0013693549 |
|    clip_fraction        | 0.000635     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0939       |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.000312    |
|    std                  | 1.4          |
|    value_loss           | 1.74         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 474    |
|    iterations      | 137    |
|    time_elapsed    | 591    |
|    total_timesteps | 280576 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 138          |
|    time_elapsed         | 594          |
|    total_timesteps      | 282624       |
| train/                  |              |
|    approx_kl            | 0.0036603736 |
|    clip_fraction        | 0.0041       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0629       |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00177     |
|    std                  | 1.4          |
|    value_loss           | 0.908        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 476         |
|    iterations           | 139         |
|    time_elapsed         | 597         |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.003811025 |
|    clip_fraction        | 0.0342      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.28       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0328      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00328    |
|    std                  | 1.41        |
|    value_loss           | 0.673       |
-----------------------------------------
Eval num_timesteps=285000, episode_reward=-323.26 +/- 48.85
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -323        |
| time/                   |             |
|    total_timesteps      | 285000      |
| train/                  |             |
|    approx_kl            | 0.005168975 |
|    clip_fraction        | 0.0191      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.28       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0907     |
|    n_updates            | 1390        |
|    policy_gradient_loss | -0.00324    |
|    std                  | 1.41        |
|    value_loss           | 0.172       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 474    |
|    iterations      | 140    |
|    time_elapsed    | 604    |
|    total_timesteps | 286720 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 141          |
|    time_elapsed         | 607          |
|    total_timesteps      | 288768       |
| train/                  |              |
|    approx_kl            | 0.0012729808 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.29        |
|    explained_variance   | 0.751        |
|    learning_rate        | 0.0001       |
|    loss                 | 13.6         |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.41         |
|    value_loss           | 15.6         |
------------------------------------------
Eval num_timesteps=290000, episode_reward=-375.75 +/- 179.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -376         |
| time/                   |              |
|    total_timesteps      | 290000       |
| train/                  |              |
|    approx_kl            | 0.0048005083 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.3         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.106       |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00363     |
|    std                  | 1.42         |
|    value_loss           | 0.088        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 142    |
|    time_elapsed    | 613    |
|    total_timesteps | 290816 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 143          |
|    time_elapsed         | 616          |
|    total_timesteps      | 292864       |
| train/                  |              |
|    approx_kl            | 0.0031230876 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.3         |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.279        |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 1.42         |
|    value_loss           | 16.7         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 144          |
|    time_elapsed         | 619          |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.0033526495 |
|    clip_fraction        | 0.00469      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.31        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0779      |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.42         |
|    value_loss           | 0.169        |
------------------------------------------
Eval num_timesteps=295000, episode_reward=-351.88 +/- 151.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -352         |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0020652735 |
|    clip_fraction        | 0.00386      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.32        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0716      |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.000911    |
|    std                  | 1.43         |
|    value_loss           | 0.093        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 474    |
|    iterations      | 145    |
|    time_elapsed    | 626    |
|    total_timesteps | 296960 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 474         |
|    iterations           | 146         |
|    time_elapsed         | 629         |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.003693826 |
|    clip_fraction        | 0.00464     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.33       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.355       |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00156    |
|    std                  | 1.43        |
|    value_loss           | 3.25        |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=-223.26 +/- 37.00
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -223        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.005461821 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.34       |
|    explained_variance   | 0.914       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0602     |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00443    |
|    std                  | 1.44        |
|    value_loss           | 0.176       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 147    |
|    time_elapsed    | 635    |
|    total_timesteps | 301056 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 474          |
|    iterations           | 148          |
|    time_elapsed         | 638          |
|    total_timesteps      | 303104       |
| train/                  |              |
|    approx_kl            | 0.0019063086 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.35        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.118        |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.000629    |
|    std                  | 1.44         |
|    value_loss           | 5.67         |
------------------------------------------
Eval num_timesteps=305000, episode_reward=-266.61 +/- 145.92
Episode length: 815.40 +/- 371.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -267         |
| time/                   |              |
|    total_timesteps      | 305000       |
| train/                  |              |
|    approx_kl            | 0.0045634992 |
|    clip_fraction        | 0.0312       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.35        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.106       |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00295     |
|    std                  | 1.44         |
|    value_loss           | 0.193        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 149    |
|    time_elapsed    | 644    |
|    total_timesteps | 305152 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 474          |
|    iterations           | 150          |
|    time_elapsed         | 647          |
|    total_timesteps      | 307200       |
| train/                  |              |
|    approx_kl            | 0.0027942182 |
|    clip_fraction        | 0.00122      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.36        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0001       |
|    loss                 | 10.8         |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00192     |
|    std                  | 1.44         |
|    value_loss           | 15.2         |
------------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 474        |
|    iterations           | 151        |
|    time_elapsed         | 651        |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.00431468 |
|    clip_fraction        | 0.0185     |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.37      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0304     |
|    n_updates            | 1500       |
|    policy_gradient_loss | -0.00302   |
|    std                  | 1.45       |
|    value_loss           | 0.468      |
----------------------------------------
Eval num_timesteps=310000, episode_reward=-431.99 +/- 153.04
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -432        |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.006307105 |
|    clip_fraction        | 0.0352      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.38       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0248     |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00375    |
|    std                  | 1.46        |
|    value_loss           | 0.427       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 152    |
|    time_elapsed    | 658    |
|    total_timesteps | 311296 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 153          |
|    time_elapsed         | 662          |
|    total_timesteps      | 313344       |
| train/                  |              |
|    approx_kl            | 0.0012928515 |
|    clip_fraction        | 0.000342     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.4         |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 33.9         |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.000775    |
|    std                  | 1.47         |
|    value_loss           | 18.4         |
------------------------------------------
Eval num_timesteps=315000, episode_reward=-371.45 +/- 155.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -371         |
| time/                   |              |
|    total_timesteps      | 315000       |
| train/                  |              |
|    approx_kl            | 0.0032774033 |
|    clip_fraction        | 0.00801      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.4         |
|    explained_variance   | 0.902        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0618      |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 1.46         |
|    value_loss           | 0.412        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 154    |
|    time_elapsed    | 670    |
|    total_timesteps | 315392 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 471          |
|    iterations           | 155          |
|    time_elapsed         | 673          |
|    total_timesteps      | 317440       |
| train/                  |              |
|    approx_kl            | 0.0029891082 |
|    clip_fraction        | 0.00757      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.4         |
|    explained_variance   | 0.822        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0171      |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.000673    |
|    std                  | 1.47         |
|    value_loss           | 0.538        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 471         |
|    iterations           | 156         |
|    time_elapsed         | 676         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.003964712 |
|    clip_fraction        | 0.0198      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.41       |
|    explained_variance   | 0.171       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.136       |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.0026     |
|    std                  | 1.47        |
|    value_loss           | 0.665       |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=-254.99 +/- 64.36
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -255         |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0035766065 |
|    clip_fraction        | 0.00747      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.43        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0039      |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 1.48         |
|    value_loss           | 0.341        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 157    |
|    time_elapsed    | 684    |
|    total_timesteps | 321536 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 470           |
|    iterations           | 158           |
|    time_elapsed         | 687           |
|    total_timesteps      | 323584        |
| train/                  |               |
|    approx_kl            | 0.00080413313 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.43         |
|    explained_variance   | 0.965         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.531         |
|    n_updates            | 1570          |
|    policy_gradient_loss | -9.37e-05     |
|    std                  | 1.48          |
|    value_loss           | 1.53          |
-------------------------------------------
Eval num_timesteps=325000, episode_reward=-393.00 +/- 152.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -393         |
| time/                   |              |
|    total_timesteps      | 325000       |
| train/                  |              |
|    approx_kl            | 0.0056682164 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.43        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0131      |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.00614     |
|    std                  | 1.48         |
|    value_loss           | 0.296        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 469    |
|    iterations      | 159    |
|    time_elapsed    | 694    |
|    total_timesteps | 325632 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 469         |
|    iterations           | 160         |
|    time_elapsed         | 698         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.005488608 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.44       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0669     |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.003      |
|    std                  | 1.48        |
|    value_loss           | 0.361       |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 161          |
|    time_elapsed         | 701          |
|    total_timesteps      | 329728       |
| train/                  |              |
|    approx_kl            | 0.0055684336 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.45        |
|    explained_variance   | -1.85        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0388      |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00338     |
|    std                  | 1.49         |
|    value_loss           | 0.302        |
------------------------------------------
Eval num_timesteps=330000, episode_reward=-397.56 +/- 187.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -398         |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0038431175 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.45        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0528      |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.49         |
|    value_loss           | 0.199        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 468    |
|    iterations      | 162    |
|    time_elapsed    | 707    |
|    total_timesteps | 331776 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 469          |
|    iterations           | 163          |
|    time_elapsed         | 711          |
|    total_timesteps      | 333824       |
| train/                  |              |
|    approx_kl            | 0.0011745951 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.45        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 10.8         |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 1.49         |
|    value_loss           | 10.3         |
------------------------------------------
Eval num_timesteps=335000, episode_reward=-393.41 +/- 154.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -393         |
| time/                   |              |
|    total_timesteps      | 335000       |
| train/                  |              |
|    approx_kl            | 0.0051527973 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.47        |
|    explained_variance   | 0.884        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0316      |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.0036      |
|    std                  | 1.5          |
|    value_loss           | 0.161        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 467    |
|    iterations      | 164    |
|    time_elapsed    | 718    |
|    total_timesteps | 335872 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 165          |
|    time_elapsed         | 721          |
|    total_timesteps      | 337920       |
| train/                  |              |
|    approx_kl            | 0.0010235289 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.48        |
|    explained_variance   | 0.902        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11         |
|    n_updates            | 1640         |
|    policy_gradient_loss | -0.001       |
|    std                  | 1.51         |
|    value_loss           | 23.4         |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 469         |
|    iterations           | 166         |
|    time_elapsed         | 724         |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.006354044 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0659     |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00558    |
|    std                  | 1.51        |
|    value_loss           | 0.258       |
-----------------------------------------
Eval num_timesteps=340000, episode_reward=-407.32 +/- 102.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -407         |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0053902427 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.49        |
|    explained_variance   | 0.905        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.025        |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.004       |
|    std                  | 1.51         |
|    value_loss           | 0.28         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 467    |
|    iterations      | 167    |
|    time_elapsed    | 730    |
|    total_timesteps | 342016 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 468          |
|    iterations           | 168          |
|    time_elapsed         | 733          |
|    total_timesteps      | 344064       |
| train/                  |              |
|    approx_kl            | 0.0034634862 |
|    clip_fraction        | 0.0103       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.49        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0535      |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.00172     |
|    std                  | 1.51         |
|    value_loss           | 0.435        |
------------------------------------------
Eval num_timesteps=345000, episode_reward=-488.86 +/- 94.05
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -489        |
| time/                   |             |
|    total_timesteps      | 345000      |
| train/                  |             |
|    approx_kl            | 0.003796324 |
|    clip_fraction        | 0.0118      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.5        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0801     |
|    n_updates            | 1680        |
|    policy_gradient_loss | -0.00124    |
|    std                  | 1.52        |
|    value_loss           | 0.111       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 467    |
|    iterations      | 169    |
|    time_elapsed    | 740    |
|    total_timesteps | 346112 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 170          |
|    time_elapsed         | 744          |
|    total_timesteps      | 348160       |
| train/                  |              |
|    approx_kl            | 0.0017755113 |
|    clip_fraction        | 0.00317      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0442       |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 1.52         |
|    value_loss           | 0.738        |
------------------------------------------
Eval num_timesteps=350000, episode_reward=-491.93 +/- 83.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -492         |
| time/                   |              |
|    total_timesteps      | 350000       |
| train/                  |              |
|    approx_kl            | 0.0044332105 |
|    clip_fraction        | 0.00894      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0792      |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.52         |
|    value_loss           | 0.857        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 466    |
|    iterations      | 171    |
|    time_elapsed    | 751    |
|    total_timesteps | 350208 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 466         |
|    iterations           | 172         |
|    time_elapsed         | 754         |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.003953346 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.52       |
|    explained_variance   | 0.944       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0344     |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00354    |
|    std                  | 1.52        |
|    value_loss           | 2.47        |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 173          |
|    time_elapsed         | 757          |
|    total_timesteps      | 354304       |
| train/                  |              |
|    approx_kl            | 0.0019157045 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.52        |
|    explained_variance   | 0.813        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.4          |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 1.53         |
|    value_loss           | 14.6         |
------------------------------------------
Eval num_timesteps=355000, episode_reward=-311.01 +/- 127.93
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -311         |
| time/                   |              |
|    total_timesteps      | 355000       |
| train/                  |              |
|    approx_kl            | 0.0060905432 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.53        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0559      |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 1.53         |
|    value_loss           | 0.241        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 466    |
|    iterations      | 174    |
|    time_elapsed    | 763    |
|    total_timesteps | 356352 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 467          |
|    iterations           | 175          |
|    time_elapsed         | 767          |
|    total_timesteps      | 358400       |
| train/                  |              |
|    approx_kl            | 0.0035286415 |
|    clip_fraction        | 0.00996      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.54        |
|    explained_variance   | -1.56        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.239        |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.00074     |
|    std                  | 1.54         |
|    value_loss           | 0.588        |
------------------------------------------
Eval num_timesteps=360000, episode_reward=-348.61 +/- 159.58
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -349        |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.005299856 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.067      |
|    n_updates            | 1750        |
|    policy_gradient_loss | -0.00379    |
|    std                  | 1.55        |
|    value_loss           | 0.272       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 466    |
|    iterations      | 176    |
|    time_elapsed    | 773    |
|    total_timesteps | 360448 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 466          |
|    iterations           | 177          |
|    time_elapsed         | 776          |
|    total_timesteps      | 362496       |
| train/                  |              |
|    approx_kl            | 0.0005352071 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.103        |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.000597    |
|    std                  | 1.55         |
|    value_loss           | 6.39         |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 466          |
|    iterations           | 178          |
|    time_elapsed         | 781          |
|    total_timesteps      | 364544       |
| train/                  |              |
|    approx_kl            | 0.0057013547 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0613      |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00301     |
|    std                  | 1.55         |
|    value_loss           | 0.275        |
------------------------------------------
Eval num_timesteps=365000, episode_reward=-373.95 +/- 206.65
Episode length: 813.00 +/- 376.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -374         |
| time/                   |              |
|    total_timesteps      | 365000       |
| train/                  |              |
|    approx_kl            | 0.0036988975 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.59        |
|    explained_variance   | 0.946        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0743      |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.56         |
|    value_loss           | 0.135        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 465    |
|    iterations      | 179    |
|    time_elapsed    | 787    |
|    total_timesteps | 366592 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 465         |
|    iterations           | 180         |
|    time_elapsed         | 791         |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.001528961 |
|    clip_fraction        | 0.00107     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.59       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.715       |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.000998   |
|    std                  | 1.56        |
|    value_loss           | 9.08        |
-----------------------------------------
Eval num_timesteps=370000, episode_reward=-397.19 +/- 121.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -397         |
| time/                   |              |
|    total_timesteps      | 370000       |
| train/                  |              |
|    approx_kl            | 0.0066112257 |
|    clip_fraction        | 0.0237       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.6         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0359      |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00367     |
|    std                  | 1.57         |
|    value_loss           | 0.216        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 463    |
|    iterations      | 181    |
|    time_elapsed    | 798    |
|    total_timesteps | 370688 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 182          |
|    time_elapsed         | 801          |
|    total_timesteps      | 372736       |
| train/                  |              |
|    approx_kl            | 0.0024741911 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.6         |
|    explained_variance   | 0.958        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.073       |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 1.57         |
|    value_loss           | 0.4          |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 465         |
|    iterations           | 183         |
|    time_elapsed         | 805         |
|    total_timesteps      | 374784      |
| train/                  |             |
|    approx_kl            | 0.006159749 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.61       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0228     |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00254    |
|    std                  | 1.57        |
|    value_loss           | 0.496       |
-----------------------------------------
Eval num_timesteps=375000, episode_reward=-289.14 +/- 160.98
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -289         |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0068071047 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.61        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.017       |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.00579     |
|    std                  | 1.57         |
|    value_loss           | 0.227        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 463    |
|    iterations      | 184    |
|    time_elapsed    | 812    |
|    total_timesteps | 376832 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 185          |
|    time_elapsed         | 815          |
|    total_timesteps      | 378880       |
| train/                  |              |
|    approx_kl            | 0.0028398898 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.62        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0505       |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 1.58         |
|    value_loss           | 0.538        |
------------------------------------------
Eval num_timesteps=380000, episode_reward=-346.38 +/- 57.48
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -346        |
| time/                   |             |
|    total_timesteps      | 380000      |
| train/                  |             |
|    approx_kl            | 0.003481404 |
|    clip_fraction        | 0.0182      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.62       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0949     |
|    n_updates            | 1850        |
|    policy_gradient_loss | -0.00246    |
|    std                  | 1.58        |
|    value_loss           | 0.193       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 463    |
|    iterations      | 186    |
|    time_elapsed    | 821    |
|    total_timesteps | 380928 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 187         |
|    time_elapsed         | 825         |
|    total_timesteps      | 382976      |
| train/                  |             |
|    approx_kl            | 0.003501093 |
|    clip_fraction        | 0.00386     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.63        |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.000766   |
|    std                  | 1.58        |
|    value_loss           | 3.71        |
-----------------------------------------
Eval num_timesteps=385000, episode_reward=-387.97 +/- 56.27
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -388        |
| time/                   |             |
|    total_timesteps      | 385000      |
| train/                  |             |
|    approx_kl            | 0.004441115 |
|    clip_fraction        | 0.0212      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0885     |
|    n_updates            | 1870        |
|    policy_gradient_loss | -0.00234    |
|    std                  | 1.58        |
|    value_loss           | 0.381       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 463    |
|    iterations      | 188    |
|    time_elapsed    | 831    |
|    total_timesteps | 385024 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 189          |
|    time_elapsed         | 834          |
|    total_timesteps      | 387072       |
| train/                  |              |
|    approx_kl            | 0.0040793363 |
|    clip_fraction        | 0.00854      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.64        |
|    explained_variance   | 0.782        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.354        |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00171     |
|    std                  | 1.59         |
|    value_loss           | 17.1         |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 464         |
|    iterations           | 190         |
|    time_elapsed         | 838         |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.004177162 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0669     |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00112    |
|    std                  | 1.6         |
|    value_loss           | 0.281       |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=-327.42 +/- 112.57
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -327         |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 0.0056218184 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.67        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0611      |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00354     |
|    std                  | 1.61         |
|    value_loss           | 0.215        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 191    |
|    time_elapsed    | 844    |
|    total_timesteps | 391168 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 192          |
|    time_elapsed         | 848          |
|    total_timesteps      | 393216       |
| train/                  |              |
|    approx_kl            | 0.0073116384 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.12         |
|    n_updates            | 1910         |
|    policy_gradient_loss | -0.0023      |
|    std                  | 1.61         |
|    value_loss           | 14           |
------------------------------------------
Eval num_timesteps=395000, episode_reward=-437.87 +/- 64.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -438         |
| time/                   |              |
|    total_timesteps      | 395000       |
| train/                  |              |
|    approx_kl            | 0.0030490723 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.83         |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0319      |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 1.61         |
|    value_loss           | 0.293        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 193    |
|    time_elapsed    | 854    |
|    total_timesteps | 395264 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 194         |
|    time_elapsed         | 857         |
|    total_timesteps      | 397312      |
| train/                  |             |
|    approx_kl            | 0.003094794 |
|    clip_fraction        | 0.0128      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.69       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.46        |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.00172    |
|    std                  | 1.62        |
|    value_loss           | 1.52        |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 195          |
|    time_elapsed         | 860          |
|    total_timesteps      | 399360       |
| train/                  |              |
|    approx_kl            | 0.0042353813 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.7         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0509      |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 1.62         |
|    value_loss           | 0.178        |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-509.18 +/- 81.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -509         |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0039663566 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.7         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.114       |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00378     |
|    std                  | 1.61         |
|    value_loss           | 0.268        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 196    |
|    time_elapsed    | 867    |
|    total_timesteps | 401408 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 197          |
|    time_elapsed         | 870          |
|    total_timesteps      | 403456       |
| train/                  |              |
|    approx_kl            | 0.0033614608 |
|    clip_fraction        | 0.00298      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.576        |
|    n_updates            | 1960         |
|    policy_gradient_loss | -0.00163     |
|    std                  | 1.62         |
|    value_loss           | 1.3          |
------------------------------------------
Eval num_timesteps=405000, episode_reward=-379.34 +/- 158.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -379         |
| time/                   |              |
|    total_timesteps      | 405000       |
| train/                  |              |
|    approx_kl            | 0.0061458107 |
|    clip_fraction        | 0.0269       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.7         |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00811      |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00332     |
|    std                  | 1.62         |
|    value_loss           | 0.84         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 198    |
|    time_elapsed    | 876    |
|    total_timesteps | 405504 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 199          |
|    time_elapsed         | 879          |
|    total_timesteps      | 407552       |
| train/                  |              |
|    approx_kl            | 0.0019007752 |
|    clip_fraction        | 0.0063       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.71        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.066       |
|    n_updates            | 1980         |
|    policy_gradient_loss | -4.81e-05    |
|    std                  | 1.63         |
|    value_loss           | 0.276        |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 200          |
|    time_elapsed         | 883          |
|    total_timesteps      | 409600       |
| train/                  |              |
|    approx_kl            | 0.0047818455 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.72        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0829      |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 1.63         |
|    value_loss           | 0.149        |
------------------------------------------
Eval num_timesteps=410000, episode_reward=-369.27 +/- 78.42
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -369        |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.003499344 |
|    clip_fraction        | 0.0063      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.73       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0778     |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00138    |
|    std                  | 1.63        |
|    value_loss           | 0.186       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 201    |
|    time_elapsed    | 890    |
|    total_timesteps | 411648 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 463           |
|    iterations           | 202           |
|    time_elapsed         | 893           |
|    total_timesteps      | 413696        |
| train/                  |               |
|    approx_kl            | 0.00053992646 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.73         |
|    explained_variance   | 0.97          |
|    learning_rate        | 0.0001        |
|    loss                 | 10.4          |
|    n_updates            | 2010          |
|    policy_gradient_loss | -0.000659     |
|    std                  | 1.63          |
|    value_loss           | 15.8          |
-------------------------------------------
Eval num_timesteps=415000, episode_reward=-343.69 +/- 122.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -344         |
| time/                   |              |
|    total_timesteps      | 415000       |
| train/                  |              |
|    approx_kl            | 0.0024938018 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.73        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0295      |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.000348    |
|    std                  | 1.64         |
|    value_loss           | 0.41         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 203    |
|    time_elapsed    | 899    |
|    total_timesteps | 415744 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 462         |
|    iterations           | 204         |
|    time_elapsed         | 902         |
|    total_timesteps      | 417792      |
| train/                  |             |
|    approx_kl            | 0.002889697 |
|    clip_fraction        | 0.00859     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.74       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0994     |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00122    |
|    std                  | 1.64        |
|    value_loss           | 0.161       |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 205          |
|    time_elapsed         | 905          |
|    total_timesteps      | 419840       |
| train/                  |              |
|    approx_kl            | 0.0068062423 |
|    clip_fraction        | 0.0551       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.76        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0301      |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.00585     |
|    std                  | 1.65         |
|    value_loss           | 0.311        |
------------------------------------------
Eval num_timesteps=420000, episode_reward=-267.79 +/- 96.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -268        |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.005243604 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.77       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0339     |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00286    |
|    std                  | 1.66        |
|    value_loss           | 0.494       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 463    |
|    iterations      | 206    |
|    time_elapsed    | 910    |
|    total_timesteps | 421888 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 464          |
|    iterations           | 207          |
|    time_elapsed         | 913          |
|    total_timesteps      | 423936       |
| train/                  |              |
|    approx_kl            | 0.0054245368 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.78        |
|    explained_variance   | 0.86         |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0431      |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.00501     |
|    std                  | 1.66         |
|    value_loss           | 1.24         |
------------------------------------------
Eval num_timesteps=425000, episode_reward=-184.87 +/- 110.31
Episode length: 810.60 +/- 380.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -185        |
| time/                   |             |
|    total_timesteps      | 425000      |
| train/                  |             |
|    approx_kl            | 0.006375163 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.79       |
|    explained_variance   | 0.953       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0306     |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00678    |
|    std                  | 1.67        |
|    value_loss           | 0.488       |
-----------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 463    |
|    iterations      | 208    |
|    time_elapsed    | 918    |
|    total_timesteps | 425984 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 209          |
|    time_elapsed         | 923          |
|    total_timesteps      | 428032       |
| train/                  |              |
|    approx_kl            | 0.0058687255 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.8         |
|    explained_variance   | 0.377        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.69         |
|    n_updates            | 2080         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 1.67         |
|    value_loss           | 1.5          |
------------------------------------------
Eval num_timesteps=430000, episode_reward=-360.47 +/- 47.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -360        |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.005069274 |
|    clip_fraction        | 0.0261      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.8        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0885     |
|    n_updates            | 2090        |
|    policy_gradient_loss | -0.00336    |
|    std                  | 1.67        |
|    value_loss           | 0.136       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 210    |
|    time_elapsed    | 929    |
|    total_timesteps | 430080 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 211          |
|    time_elapsed         | 931          |
|    total_timesteps      | 432128       |
| train/                  |              |
|    approx_kl            | 0.0062715374 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.81        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.077       |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00587     |
|    std                  | 1.68         |
|    value_loss           | 0.545        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 464         |
|    iterations           | 212         |
|    time_elapsed         | 934         |
|    total_timesteps      | 434176      |
| train/                  |             |
|    approx_kl            | 0.007152728 |
|    clip_fraction        | 0.0502      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.83       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0752     |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00701    |
|    std                  | 1.69        |
|    value_loss           | 0.139       |
-----------------------------------------
Eval num_timesteps=435000, episode_reward=-365.25 +/- 174.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 435000       |
| train/                  |              |
|    approx_kl            | 0.0012721884 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.84        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0861      |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.000947    |
|    std                  | 1.7          |
|    value_loss           | 0.226        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 463    |
|    iterations      | 213    |
|    time_elapsed    | 940    |
|    total_timesteps | 436224 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 214          |
|    time_elapsed         | 945          |
|    total_timesteps      | 438272       |
| train/                  |              |
|    approx_kl            | 0.0009750755 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.84        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.125        |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.000629    |
|    std                  | 1.7          |
|    value_loss           | 13.5         |
------------------------------------------
Eval num_timesteps=440000, episode_reward=-346.49 +/- 247.39
Episode length: 816.40 +/- 369.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 816         |
|    mean_reward          | -346        |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.007614021 |
|    clip_fraction        | 0.0751      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.84       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0718     |
|    n_updates            | 2140        |
|    policy_gradient_loss | -0.0088     |
|    std                  | 1.69        |
|    value_loss           | 0.091       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 215    |
|    time_elapsed    | 952    |
|    total_timesteps | 440320 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 462          |
|    iterations           | 216          |
|    time_elapsed         | 956          |
|    total_timesteps      | 442368       |
| train/                  |              |
|    approx_kl            | 0.0056053037 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.85        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0661      |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.00556     |
|    std                  | 1.71         |
|    value_loss           | 0.207        |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 462         |
|    iterations           | 217         |
|    time_elapsed         | 960         |
|    total_timesteps      | 444416      |
| train/                  |             |
|    approx_kl            | 0.007253504 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.86       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0958     |
|    n_updates            | 2160        |
|    policy_gradient_loss | -0.00667    |
|    std                  | 1.71        |
|    value_loss           | 0.127       |
-----------------------------------------
Eval num_timesteps=445000, episode_reward=-335.22 +/- 137.77
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -335        |
| time/                   |             |
|    total_timesteps      | 445000      |
| train/                  |             |
|    approx_kl            | 0.005271451 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.88       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.08       |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00371    |
|    std                  | 1.72        |
|    value_loss           | 0.236       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 218    |
|    time_elapsed    | 966    |
|    total_timesteps | 446464 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 462          |
|    iterations           | 219          |
|    time_elapsed         | 969          |
|    total_timesteps      | 448512       |
| train/                  |              |
|    approx_kl            | 0.0011431212 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.89        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.166        |
|    n_updates            | 2180         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 1.72         |
|    value_loss           | 4.19         |
------------------------------------------
Eval num_timesteps=450000, episode_reward=-434.74 +/- 143.12
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -435        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.004477556 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0806     |
|    n_updates            | 2190        |
|    policy_gradient_loss | -0.00149    |
|    std                  | 1.73        |
|    value_loss           | 0.614       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 461    |
|    iterations      | 220    |
|    time_elapsed    | 975    |
|    total_timesteps | 450560 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 462           |
|    iterations           | 221           |
|    time_elapsed         | 978           |
|    total_timesteps      | 452608        |
| train/                  |               |
|    approx_kl            | 0.00044995244 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.9          |
|    explained_variance   | 0.981         |
|    learning_rate        | 0.0001        |
|    loss                 | 16.4          |
|    n_updates            | 2200          |
|    policy_gradient_loss | -0.000615     |
|    std                  | 1.73          |
|    value_loss           | 16.7          |
-------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 463          |
|    iterations           | 222          |
|    time_elapsed         | 981          |
|    total_timesteps      | 454656       |
| train/                  |              |
|    approx_kl            | 0.0046629403 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.91        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00314     |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.74         |
|    value_loss           | 0.566        |
------------------------------------------
Eval num_timesteps=455000, episode_reward=-273.48 +/- 130.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -273         |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0059226425 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0481      |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.00354     |
|    std                  | 1.74         |
|    value_loss           | 0.127        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 223    |
|    time_elapsed    | 987    |
|    total_timesteps | 456704 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 224         |
|    time_elapsed         | 989         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.005029495 |
|    clip_fraction        | 0.0306      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.92       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.121       |
|    n_updates            | 2230        |
|    policy_gradient_loss | -0.00423    |
|    std                  | 1.74        |
|    value_loss           | 2.48        |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=-391.10 +/- 147.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -391         |
| time/                   |              |
|    total_timesteps      | 460000       |
| train/                  |              |
|    approx_kl            | 0.0037333714 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.418        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.000517    |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.00173     |
|    std                  | 1.75         |
|    value_loss           | 0.326        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 225    |
|    time_elapsed    | 996    |
|    total_timesteps | 460800 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 226         |
|    time_elapsed         | 998         |
|    total_timesteps      | 462848      |
| train/                  |             |
|    approx_kl            | 0.006427804 |
|    clip_fraction        | 0.0383      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.93       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0659      |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.00432    |
|    std                  | 1.75        |
|    value_loss           | 0.258       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 463         |
|    iterations           | 227         |
|    time_elapsed         | 1003        |
|    total_timesteps      | 464896      |
| train/                  |             |
|    approx_kl            | 0.004395241 |
|    clip_fraction        | 0.0214      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.036      |
|    n_updates            | 2260        |
|    policy_gradient_loss | -0.00348    |
|    std                  | 1.76        |
|    value_loss           | 0.265       |
-----------------------------------------
Eval num_timesteps=465000, episode_reward=-457.49 +/- 184.00
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -457         |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0067826062 |
|    clip_fraction        | 0.0597       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.95        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0633      |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00635     |
|    std                  | 1.75         |
|    value_loss           | 0.174        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 462    |
|    iterations      | 228    |
|    time_elapsed    | 1009   |
|    total_timesteps | 466944 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 462           |
|    iterations           | 229           |
|    time_elapsed         | 1013          |
|    total_timesteps      | 468992        |
| train/                  |               |
|    approx_kl            | 0.00087025505 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.94         |
|    explained_variance   | 0.804         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.596         |
|    n_updates            | 2280          |
|    policy_gradient_loss | -0.00131      |
|    std                  | 1.76          |
|    value_loss           | 9.98          |
-------------------------------------------
Eval num_timesteps=470000, episode_reward=-349.71 +/- 152.08
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -350        |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.004714177 |
|    clip_fraction        | 0.0293      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.95       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0878     |
|    n_updates            | 2290        |
|    policy_gradient_loss | -0.00331    |
|    std                  | 1.76        |
|    value_loss           | 0.25        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 461    |
|    iterations      | 230    |
|    time_elapsed    | 1020   |
|    total_timesteps | 471040 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 462          |
|    iterations           | 231          |
|    time_elapsed         | 1023         |
|    total_timesteps      | 473088       |
| train/                  |              |
|    approx_kl            | 0.0015765808 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.95        |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 1.76         |
|    value_loss           | 12.9         |
------------------------------------------
Eval num_timesteps=475000, episode_reward=-264.96 +/- 179.43
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -265        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.005031679 |
|    clip_fraction        | 0.0239      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.109      |
|    n_updates            | 2310        |
|    policy_gradient_loss | -0.00146    |
|    std                  | 1.78        |
|    value_loss           | 0.614       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 461    |
|    iterations      | 232    |
|    time_elapsed    | 1030   |
|    total_timesteps | 475136 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 461          |
|    iterations           | 233          |
|    time_elapsed         | 1033         |
|    total_timesteps      | 477184       |
| train/                  |              |
|    approx_kl            | 0.0027428998 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.98        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.42         |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 1.78         |
|    value_loss           | 18.8         |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 462         |
|    iterations           | 234         |
|    time_elapsed         | 1035        |
|    total_timesteps      | 479232      |
| train/                  |             |
|    approx_kl            | 0.003824879 |
|    clip_fraction        | 0.00596     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.99       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0987     |
|    n_updates            | 2330        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 1.79        |
|    value_loss           | 0.287       |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=-324.92 +/- 104.34
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -325        |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.008476814 |
|    clip_fraction        | 0.0779      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.01       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0916     |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.00841    |
|    std                  | 1.8         |
|    value_loss           | 0.164       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 461    |
|    iterations      | 235    |
|    time_elapsed    | 1042   |
|    total_timesteps | 481280 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 461           |
|    iterations           | 236           |
|    time_elapsed         | 1047          |
|    total_timesteps      | 483328        |
| train/                  |               |
|    approx_kl            | 0.00055582833 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.02         |
|    explained_variance   | 0.988         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.396         |
|    n_updates            | 2350          |
|    policy_gradient_loss | -0.000267     |
|    std                  | 1.8           |
|    value_loss           | 5.73          |
-------------------------------------------
Eval num_timesteps=485000, episode_reward=-464.26 +/- 120.64
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -464        |
| time/                   |             |
|    total_timesteps      | 485000      |
| train/                  |             |
|    approx_kl            | 0.005071911 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.03       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.123      |
|    n_updates            | 2360        |
|    policy_gradient_loss | -0.00292    |
|    std                  | 1.81        |
|    value_loss           | 0.0862      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 460    |
|    iterations      | 237    |
|    time_elapsed    | 1055   |
|    total_timesteps | 485376 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 460         |
|    iterations           | 238         |
|    time_elapsed         | 1058        |
|    total_timesteps      | 487424      |
| train/                  |             |
|    approx_kl            | 0.004889209 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.057      |
|    n_updates            | 2370        |
|    policy_gradient_loss | -0.00214    |
|    std                  | 1.83        |
|    value_loss           | 0.412       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 461         |
|    iterations           | 239         |
|    time_elapsed         | 1060        |
|    total_timesteps      | 489472      |
| train/                  |             |
|    approx_kl            | 0.004609639 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0968     |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00309    |
|    std                  | 1.81        |
|    value_loss           | 0.557       |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=-396.17 +/- 104.06
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -396         |
| time/                   |              |
|    total_timesteps      | 490000       |
| train/                  |              |
|    approx_kl            | 0.0025469274 |
|    clip_fraction        | 0.0083       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.04        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0903      |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 1.81         |
|    value_loss           | 0.245        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 459    |
|    iterations      | 240    |
|    time_elapsed    | 1069   |
|    total_timesteps | 491520 |
-------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 460           |
|    iterations           | 241           |
|    time_elapsed         | 1072          |
|    total_timesteps      | 493568        |
| train/                  |               |
|    approx_kl            | 0.00092808326 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.04         |
|    explained_variance   | 0.975         |
|    learning_rate        | 0.0001        |
|    loss                 | 25.5          |
|    n_updates            | 2400          |
|    policy_gradient_loss | -0.000771     |
|    std                  | 1.82          |
|    value_loss           | 19.2          |
-------------------------------------------
Eval num_timesteps=495000, episode_reward=-391.49 +/- 117.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -391         |
| time/                   |              |
|    total_timesteps      | 495000       |
| train/                  |              |
|    approx_kl            | 0.0057850704 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.05        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0972      |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.00273     |
|    std                  | 1.82         |
|    value_loss           | 0.28         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 459    |
|    iterations      | 242    |
|    time_elapsed    | 1078   |
|    total_timesteps | 495616 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 459         |
|    iterations           | 243         |
|    time_elapsed         | 1082        |
|    total_timesteps      | 497664      |
| train/                  |             |
|    approx_kl            | 0.005236363 |
|    clip_fraction        | 0.0338      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.07       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0552     |
|    n_updates            | 2420        |
|    policy_gradient_loss | -0.00359    |
|    std                  | 1.84        |
|    value_loss           | 0.183       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 460         |
|    iterations           | 244         |
|    time_elapsed         | 1086        |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.005876812 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.126      |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00348    |
|    std                  | 1.84        |
|    value_loss           | 0.105       |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=-351.99 +/- 176.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -352         |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0028538061 |
|    clip_fraction        | 0.00601      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.09        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0728      |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 1.84         |
|    value_loss           | 0.24         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 458    |
|    iterations      | 245    |
|    time_elapsed    | 1093   |
|    total_timesteps | 501760 |
-------------------------------
Training completed in 1095.48 seconds (~0.30 hours).
Model saved successfully!
