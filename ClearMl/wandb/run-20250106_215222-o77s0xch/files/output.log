Using cpu device
Training started...
C:\Users\jimal\anaconda3\envs\Y2B\lib\site-packages\stable_baselines3\common\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-393.08 +/- 172.64
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 5000     |
---------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 883  |
|    iterations      | 1    |
|    time_elapsed    | 9    |
|    total_timesteps | 8192 |
-----------------------------
Eval num_timesteps=10000, episode_reward=-343.28 +/- 112.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -343         |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0005019542 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | -0.00522     |
|    learning_rate        | 5e-05        |
|    loss                 | 10.4         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.000501    |
|    std                  | 1.01         |
|    value_loss           | 65.7         |
------------------------------------------
New best mean reward!
Eval num_timesteps=15000, episode_reward=-581.90 +/- 93.87
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -582     |
| time/              |          |
|    total_timesteps | 15000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 564   |
|    iterations      | 2     |
|    time_elapsed    | 29    |
|    total_timesteps | 16384 |
------------------------------
Eval num_timesteps=20000, episode_reward=-497.45 +/- 101.83
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -497          |
| time/                   |               |
|    total_timesteps      | 20000         |
| train/                  |               |
|    approx_kl            | 0.00055876933 |
|    clip_fraction        | 8.54e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.27         |
|    explained_variance   | 0.0806        |
|    learning_rate        | 5e-05         |
|    loss                 | 8.11          |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.000395     |
|    std                  | 1.01          |
|    value_loss           | 60.7          |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 557   |
|    iterations      | 3     |
|    time_elapsed    | 44    |
|    total_timesteps | 24576 |
------------------------------
Eval num_timesteps=25000, episode_reward=-526.06 +/- 191.74
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -526          |
| time/                   |               |
|    total_timesteps      | 25000         |
| train/                  |               |
|    approx_kl            | 0.00035017752 |
|    clip_fraction        | 1.22e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.28         |
|    explained_variance   | 0.229         |
|    learning_rate        | 5e-05         |
|    loss                 | 8.01          |
|    n_updates            | 30            |
|    policy_gradient_loss | -0.000338     |
|    std                  | 1.01          |
|    value_loss           | 47            |
-------------------------------------------
Eval num_timesteps=30000, episode_reward=-547.85 +/- 175.21
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -548     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 499   |
|    iterations      | 4     |
|    time_elapsed    | 65    |
|    total_timesteps | 32768 |
------------------------------
Eval num_timesteps=35000, episode_reward=-536.10 +/- 143.01
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -536          |
| time/                   |               |
|    total_timesteps      | 35000         |
| train/                  |               |
|    approx_kl            | 0.00092823786 |
|    clip_fraction        | 7.32e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.29         |
|    explained_variance   | -0.013        |
|    learning_rate        | 5e-05         |
|    loss                 | 11.8          |
|    n_updates            | 40            |
|    policy_gradient_loss | -0.000689     |
|    std                  | 1.01          |
|    value_loss           | 44.3          |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-395.14 +/- 122.83
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 481   |
|    iterations      | 5     |
|    time_elapsed    | 84    |
|    total_timesteps | 40960 |
------------------------------
Eval num_timesteps=45000, episode_reward=-495.45 +/- 105.11
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -495          |
| time/                   |               |
|    total_timesteps      | 45000         |
| train/                  |               |
|    approx_kl            | 0.00055232656 |
|    clip_fraction        | 2.44e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.29         |
|    explained_variance   | 0.0261        |
|    learning_rate        | 5e-05         |
|    loss                 | 11.8          |
|    n_updates            | 50            |
|    policy_gradient_loss | -0.000431     |
|    std                  | 1.01          |
|    value_loss           | 50.7          |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 493   |
|    iterations      | 6     |
|    time_elapsed    | 99    |
|    total_timesteps | 49152 |
------------------------------
Eval num_timesteps=50000, episode_reward=-434.23 +/- 133.16
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -434         |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0006798099 |
|    clip_fraction        | 1.22e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.00454      |
|    learning_rate        | 5e-05        |
|    loss                 | 30.6         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00044     |
|    std                  | 1.01         |
|    value_loss           | 96           |
------------------------------------------
Eval num_timesteps=55000, episode_reward=-466.36 +/- 80.26
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -466     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 488   |
|    iterations      | 7     |
|    time_elapsed    | 117   |
|    total_timesteps | 57344 |
------------------------------
Eval num_timesteps=60000, episode_reward=-405.77 +/- 109.45
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -406          |
| time/                   |               |
|    total_timesteps      | 60000         |
| train/                  |               |
|    approx_kl            | 0.00031861418 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.3          |
|    explained_variance   | 0.0172        |
|    learning_rate        | 5e-05         |
|    loss                 | 23.7          |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.000191     |
|    std                  | 1.02          |
|    value_loss           | 78.7          |
-------------------------------------------
Eval num_timesteps=65000, episode_reward=-374.37 +/- 122.71
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 479   |
|    iterations      | 8     |
|    time_elapsed    | 136   |
|    total_timesteps | 65536 |
------------------------------
Eval num_timesteps=70000, episode_reward=-397.92 +/- 190.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -398         |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0019292638 |
|    clip_fraction        | 0.00135      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.31        |
|    explained_variance   | 0.00943      |
|    learning_rate        | 5e-05        |
|    loss                 | 15.4         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.000821    |
|    std                  | 1.02         |
|    value_loss           | 54.8         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 482   |
|    iterations      | 9     |
|    time_elapsed    | 152   |
|    total_timesteps | 73728 |
------------------------------
Eval num_timesteps=75000, episode_reward=-424.60 +/- 221.72
Episode length: 801.00 +/- 400.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 801          |
|    mean_reward          | -425         |
| time/                   |              |
|    total_timesteps      | 75000        |
| train/                  |              |
|    approx_kl            | 0.0015949286 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.33        |
|    explained_variance   | 0.0016       |
|    learning_rate        | 5e-05        |
|    loss                 | 14.3         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000957    |
|    std                  | 1.03         |
|    value_loss           | 60.1         |
------------------------------------------
Eval num_timesteps=80000, episode_reward=-430.47 +/- 149.91
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 80000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 470   |
|    iterations      | 10    |
|    time_elapsed    | 174   |
|    total_timesteps | 81920 |
------------------------------
Eval num_timesteps=85000, episode_reward=-430.11 +/- 128.26
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -430         |
| time/                   |              |
|    total_timesteps      | 85000        |
| train/                  |              |
|    approx_kl            | 0.0014648926 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.35        |
|    explained_variance   | 0.00113      |
|    learning_rate        | 5e-05        |
|    loss                 | 18.6         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00105     |
|    std                  | 1.04         |
|    value_loss           | 65           |
------------------------------------------
Eval num_timesteps=90000, episode_reward=-385.97 +/- 119.32
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 470   |
|    iterations      | 11    |
|    time_elapsed    | 191   |
|    total_timesteps | 90112 |
------------------------------
Eval num_timesteps=95000, episode_reward=-530.50 +/- 132.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -531         |
| time/                   |              |
|    total_timesteps      | 95000        |
| train/                  |              |
|    approx_kl            | 0.0018977751 |
|    clip_fraction        | 0.00172      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.37        |
|    explained_variance   | 0.000298     |
|    learning_rate        | 5e-05        |
|    loss                 | 11.3         |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00127     |
|    std                  | 1.04         |
|    value_loss           | 41.2         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 478   |
|    iterations      | 12    |
|    time_elapsed    | 205   |
|    total_timesteps | 98304 |
------------------------------
Eval num_timesteps=100000, episode_reward=-258.35 +/- 167.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -258         |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0017534504 |
|    clip_fraction        | 0.00162      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.38        |
|    explained_variance   | 5.13e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 9.48         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000775    |
|    std                  | 1.05         |
|    value_loss           | 39.5         |
------------------------------------------
New best mean reward!
Eval num_timesteps=105000, episode_reward=-277.24 +/- 64.90
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 105000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 477    |
|    iterations      | 13     |
|    time_elapsed    | 222    |
|    total_timesteps | 106496 |
-------------------------------
Eval num_timesteps=110000, episode_reward=-352.90 +/- 87.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -353        |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.002351488 |
|    clip_fraction        | 0.00155     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.4        |
|    explained_variance   | 7.65e-05    |
|    learning_rate        | 5e-05       |
|    loss                 | 13.2        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.00136    |
|    std                  | 1.05        |
|    value_loss           | 52.6        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 14     |
|    time_elapsed    | 239    |
|    total_timesteps | 114688 |
-------------------------------
Eval num_timesteps=115000, episode_reward=-438.09 +/- 85.70
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -438         |
| time/                   |              |
|    total_timesteps      | 115000       |
| train/                  |              |
|    approx_kl            | 0.0016224313 |
|    clip_fraction        | 0.000916     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.41        |
|    explained_variance   | 0.00016      |
|    learning_rate        | 5e-05        |
|    loss                 | 10.8         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00082     |
|    std                  | 1.05         |
|    value_loss           | 44.2         |
------------------------------------------
Eval num_timesteps=120000, episode_reward=-303.29 +/- 159.86
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 15     |
|    time_elapsed    | 258    |
|    total_timesteps | 122880 |
-------------------------------
Eval num_timesteps=125000, episode_reward=-450.02 +/- 86.00
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -450         |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0030200067 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.42        |
|    explained_variance   | 0.00013      |
|    learning_rate        | 5e-05        |
|    loss                 | 6.76         |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00231     |
|    std                  | 1.06         |
|    value_loss           | 35.4         |
------------------------------------------
Eval num_timesteps=130000, episode_reward=-305.53 +/- 106.06
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 130000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 16     |
|    time_elapsed    | 275    |
|    total_timesteps | 131072 |
-------------------------------
Eval num_timesteps=135000, episode_reward=-429.23 +/- 154.27
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -429        |
| time/                   |             |
|    total_timesteps      | 135000      |
| train/                  |             |
|    approx_kl            | 0.002789523 |
|    clip_fraction        | 0.00266     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.44       |
|    explained_variance   | 0.00015     |
|    learning_rate        | 5e-05       |
|    loss                 | 11.6        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00148    |
|    std                  | 1.06        |
|    value_loss           | 42.6        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 480    |
|    iterations      | 17     |
|    time_elapsed    | 289    |
|    total_timesteps | 139264 |
-------------------------------
Eval num_timesteps=140000, episode_reward=-432.94 +/- 107.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -433         |
| time/                   |              |
|    total_timesteps      | 140000       |
| train/                  |              |
|    approx_kl            | 0.0022251979 |
|    clip_fraction        | 0.00349      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.45        |
|    explained_variance   | 3.69e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 29.9         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.07         |
|    value_loss           | 77.5         |
------------------------------------------
Eval num_timesteps=145000, episode_reward=-536.45 +/- 96.64
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 480    |
|    iterations      | 18     |
|    time_elapsed    | 306    |
|    total_timesteps | 147456 |
-------------------------------
Eval num_timesteps=150000, episode_reward=-460.51 +/- 208.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -461         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0025329706 |
|    clip_fraction        | 0.00504      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.46        |
|    explained_variance   | -6.35e-05    |
|    learning_rate        | 5e-05        |
|    loss                 | 4.71         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00113     |
|    std                  | 1.08         |
|    value_loss           | 25.8         |
------------------------------------------
Eval num_timesteps=155000, episode_reward=-529.99 +/- 162.29
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 480    |
|    iterations      | 19     |
|    time_elapsed    | 323    |
|    total_timesteps | 155648 |
-------------------------------
Eval num_timesteps=160000, episode_reward=-531.59 +/- 70.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -532         |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0012054055 |
|    clip_fraction        | 0.00011      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.47        |
|    explained_variance   | -2.98e-06    |
|    learning_rate        | 5e-05        |
|    loss                 | 7.98         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.000668    |
|    std                  | 1.07         |
|    value_loss           | 38.3         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 484    |
|    iterations      | 20     |
|    time_elapsed    | 338    |
|    total_timesteps | 163840 |
-------------------------------
Eval num_timesteps=165000, episode_reward=-372.70 +/- 165.83
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -373         |
| time/                   |              |
|    total_timesteps      | 165000       |
| train/                  |              |
|    approx_kl            | 0.0014604398 |
|    clip_fraction        | 0.000891     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.47        |
|    explained_variance   | 3.4e-05      |
|    learning_rate        | 5e-05        |
|    loss                 | 7.44         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00042     |
|    std                  | 1.07         |
|    value_loss           | 34.8         |
------------------------------------------
Eval num_timesteps=170000, episode_reward=-497.15 +/- 245.86
Episode length: 836.80 +/- 328.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 837      |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 170000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 484    |
|    iterations      | 21     |
|    time_elapsed    | 355    |
|    total_timesteps | 172032 |
-------------------------------
Eval num_timesteps=175000, episode_reward=-484.50 +/- 129.58
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -485         |
| time/                   |              |
|    total_timesteps      | 175000       |
| train/                  |              |
|    approx_kl            | 0.0024957422 |
|    clip_fraction        | 0.0032       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.47        |
|    explained_variance   | 3.69e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 5.56         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000734    |
|    std                  | 1.08         |
|    value_loss           | 28.8         |
------------------------------------------
Eval num_timesteps=180000, episode_reward=-409.74 +/- 129.00
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 484    |
|    iterations      | 22     |
|    time_elapsed    | 372    |
|    total_timesteps | 180224 |
-------------------------------
Eval num_timesteps=185000, episode_reward=-425.18 +/- 108.86
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -425         |
| time/                   |              |
|    total_timesteps      | 185000       |
| train/                  |              |
|    approx_kl            | 0.0008974989 |
|    clip_fraction        | 0.000232     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.48        |
|    explained_variance   | 8.52e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 13           |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.000834    |
|    std                  | 1.08         |
|    value_loss           | 49.5         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 485    |
|    iterations      | 23     |
|    time_elapsed    | 388    |
|    total_timesteps | 188416 |
-------------------------------
Eval num_timesteps=190000, episode_reward=-379.83 +/- 190.83
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -380         |
| time/                   |              |
|    total_timesteps      | 190000       |
| train/                  |              |
|    approx_kl            | 0.0027803713 |
|    clip_fraction        | 0.00542      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.49        |
|    explained_variance   | 3.52e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 11.1         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00162     |
|    std                  | 1.08         |
|    value_loss           | 48.9         |
------------------------------------------
Eval num_timesteps=195000, episode_reward=-346.44 +/- 140.87
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -346     |
| time/              |          |
|    total_timesteps | 195000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 24     |
|    time_elapsed    | 410    |
|    total_timesteps | 196608 |
-------------------------------
Eval num_timesteps=200000, episode_reward=-367.03 +/- 146.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -367         |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0018038508 |
|    clip_fraction        | 0.000659     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.51        |
|    explained_variance   | 1.29e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 17           |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.000717    |
|    std                  | 1.09         |
|    value_loss           | 48.6         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 482    |
|    iterations      | 25     |
|    time_elapsed    | 424    |
|    total_timesteps | 204800 |
-------------------------------
Eval num_timesteps=205000, episode_reward=-326.69 +/- 133.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -327         |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0028234485 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.52        |
|    explained_variance   | 3.1e-06      |
|    learning_rate        | 5e-05        |
|    loss                 | 10.3         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00196     |
|    std                  | 1.1          |
|    value_loss           | 25.6         |
------------------------------------------
Eval num_timesteps=210000, episode_reward=-263.65 +/- 109.06
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 481    |
|    iterations      | 26     |
|    time_elapsed    | 442    |
|    total_timesteps | 212992 |
-------------------------------
Eval num_timesteps=215000, episode_reward=-549.41 +/- 149.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -549         |
| time/                   |              |
|    total_timesteps      | 215000       |
| train/                  |              |
|    approx_kl            | 0.0015399483 |
|    clip_fraction        | 0.00153      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.54        |
|    explained_variance   | 1.06e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 14.5         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.001       |
|    std                  | 1.1          |
|    value_loss           | 40.4         |
------------------------------------------
Eval num_timesteps=220000, episode_reward=-399.10 +/- 133.63
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 220000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 481    |
|    iterations      | 27     |
|    time_elapsed    | 459    |
|    total_timesteps | 221184 |
-------------------------------
Eval num_timesteps=225000, episode_reward=-380.02 +/- 95.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -380         |
| time/                   |              |
|    total_timesteps      | 225000       |
| train/                  |              |
|    approx_kl            | 0.0022378075 |
|    clip_fraction        | 0.00172      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.55        |
|    explained_variance   | 9.36e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 16           |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000955    |
|    std                  | 1.11         |
|    value_loss           | 53.1         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 482    |
|    iterations      | 28     |
|    time_elapsed    | 475    |
|    total_timesteps | 229376 |
-------------------------------
Eval num_timesteps=230000, episode_reward=-493.81 +/- 169.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -494         |
| time/                   |              |
|    total_timesteps      | 230000       |
| train/                  |              |
|    approx_kl            | 0.0025678761 |
|    clip_fraction        | 0.00477      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.55        |
|    explained_variance   | 1.43e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 5.66         |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.00101     |
|    std                  | 1.1          |
|    value_loss           | 23.5         |
------------------------------------------
Eval num_timesteps=235000, episode_reward=-497.49 +/- 111.02
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 461    |
|    iterations      | 29     |
|    time_elapsed    | 514    |
|    total_timesteps | 237568 |
-------------------------------
Eval num_timesteps=240000, episode_reward=-518.72 +/- 231.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -519         |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0021947771 |
|    clip_fraction        | 0.00271      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.55        |
|    explained_variance   | 7.33e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 36.6         |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000987    |
|    std                  | 1.1          |
|    value_loss           | 42.7         |
------------------------------------------
Eval num_timesteps=245000, episode_reward=-365.73 +/- 213.65
Episode length: 861.20 +/- 279.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 861      |
|    mean_reward     | -366     |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 441    |
|    iterations      | 30     |
|    time_elapsed    | 556    |
|    total_timesteps | 245760 |
-------------------------------
Eval num_timesteps=250000, episode_reward=-428.16 +/- 177.63
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -428          |
| time/                   |               |
|    total_timesteps      | 250000        |
| train/                  |               |
|    approx_kl            | 0.00074844505 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.56         |
|    explained_variance   | 8.11e-06      |
|    learning_rate        | 5e-05         |
|    loss                 | 8.82          |
|    n_updates            | 300           |
|    policy_gradient_loss | -0.000621     |
|    std                  | 1.11          |
|    value_loss           | 37.3          |
-------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 429    |
|    iterations      | 31     |
|    time_elapsed    | 591    |
|    total_timesteps | 253952 |
-------------------------------
Eval num_timesteps=255000, episode_reward=-446.31 +/- 220.57
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -446        |
| time/                   |             |
|    total_timesteps      | 255000      |
| train/                  |             |
|    approx_kl            | 0.002484176 |
|    clip_fraction        | 0.00425     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.57       |
|    explained_variance   | 8.52e-06    |
|    learning_rate        | 5e-05       |
|    loss                 | 3.84        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00143    |
|    std                  | 1.11        |
|    value_loss           | 18.7        |
-----------------------------------------
Eval num_timesteps=260000, episode_reward=-589.31 +/- 177.99
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -589     |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 414    |
|    iterations      | 32     |
|    time_elapsed    | 632    |
|    total_timesteps | 262144 |
-------------------------------
Eval num_timesteps=265000, episode_reward=-347.29 +/- 53.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -347         |
| time/                   |              |
|    total_timesteps      | 265000       |
| train/                  |              |
|    approx_kl            | 0.0038275113 |
|    clip_fraction        | 0.00442      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.57        |
|    explained_variance   | 1.45e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 9.67         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00121     |
|    std                  | 1.11         |
|    value_loss           | 33.9         |
------------------------------------------
Eval num_timesteps=270000, episode_reward=-395.80 +/- 125.18
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 400    |
|    iterations      | 33     |
|    time_elapsed    | 675    |
|    total_timesteps | 270336 |
-------------------------------
Eval num_timesteps=275000, episode_reward=-421.32 +/- 120.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -421         |
| time/                   |              |
|    total_timesteps      | 275000       |
| train/                  |              |
|    approx_kl            | 0.0034612678 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.57        |
|    explained_variance   | 4.59e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 7.85         |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00253     |
|    std                  | 1.11         |
|    value_loss           | 34.2         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 391    |
|    iterations      | 34     |
|    time_elapsed    | 711    |
|    total_timesteps | 278528 |
-------------------------------
Eval num_timesteps=280000, episode_reward=-498.43 +/- 83.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -498         |
| time/                   |              |
|    total_timesteps      | 280000       |
| train/                  |              |
|    approx_kl            | 0.0028457162 |
|    clip_fraction        | 0.00835      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.58        |
|    explained_variance   | 5.25e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 16           |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.12         |
|    value_loss           | 45.4         |
------------------------------------------
Eval num_timesteps=285000, episode_reward=-285.95 +/- 200.95
Episode length: 849.00 +/- 304.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 285000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 381    |
|    iterations      | 35     |
|    time_elapsed    | 750    |
|    total_timesteps | 286720 |
-------------------------------
Eval num_timesteps=290000, episode_reward=-356.82 +/- 118.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -357         |
| time/                   |              |
|    total_timesteps      | 290000       |
| train/                  |              |
|    approx_kl            | 0.0027196687 |
|    clip_fraction        | 0.00648      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.59        |
|    explained_variance   | 8.34e-07     |
|    learning_rate        | 5e-05        |
|    loss                 | 6.26         |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00133     |
|    std                  | 1.12         |
|    value_loss           | 28.6         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 374    |
|    iterations      | 36     |
|    time_elapsed    | 786    |
|    total_timesteps | 294912 |
-------------------------------
Eval num_timesteps=295000, episode_reward=-485.90 +/- 209.33
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -486         |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0035304695 |
|    clip_fraction        | 0.00673      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.6         |
|    explained_variance   | 3.87e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 8.12         |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00143     |
|    std                  | 1.12         |
|    value_loss           | 33.5         |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-473.63 +/- 112.77
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -474     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 365    |
|    iterations      | 37     |
|    time_elapsed    | 828    |
|    total_timesteps | 303104 |
-------------------------------
Eval num_timesteps=305000, episode_reward=-318.96 +/- 109.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 305000       |
| train/                  |              |
|    approx_kl            | 0.0034405445 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.6         |
|    explained_variance   | 1.79e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 6.05         |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00181     |
|    std                  | 1.13         |
|    value_loss           | 23.5         |
------------------------------------------
Eval num_timesteps=310000, episode_reward=-463.96 +/- 262.64
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -464     |
| time/              |          |
|    total_timesteps | 310000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 357    |
|    iterations      | 38     |
|    time_elapsed    | 870    |
|    total_timesteps | 311296 |
-------------------------------
Eval num_timesteps=315000, episode_reward=-337.52 +/- 146.84
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -338        |
| time/                   |             |
|    total_timesteps      | 315000      |
| train/                  |             |
|    approx_kl            | 0.002566039 |
|    clip_fraction        | 0.00406     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 6.97e-06    |
|    learning_rate        | 5e-05       |
|    loss                 | 7.07        |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.000999   |
|    std                  | 1.13        |
|    value_loss           | 18.9        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 352    |
|    iterations      | 39     |
|    time_elapsed    | 907    |
|    total_timesteps | 319488 |
-------------------------------
Eval num_timesteps=320000, episode_reward=-532.91 +/- 181.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -533         |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 6.961957e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.63        |
|    explained_variance   | 6.91e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 10.2         |
|    n_updates            | 390          |
|    policy_gradient_loss | -6.8e-05     |
|    std                  | 1.13         |
|    value_loss           | 30.8         |
------------------------------------------
Eval num_timesteps=325000, episode_reward=-467.77 +/- 222.68
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -468     |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 345    |
|    iterations      | 40     |
|    time_elapsed    | 949    |
|    total_timesteps | 327680 |
-------------------------------
Eval num_timesteps=330000, episode_reward=-493.25 +/- 173.62
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -493         |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0035684211 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.63        |
|    explained_variance   | 5.96e-07     |
|    learning_rate        | 5e-05        |
|    loss                 | 148          |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.000792    |
|    std                  | 1.13         |
|    value_loss           | 90.4         |
------------------------------------------
Eval num_timesteps=335000, episode_reward=-352.35 +/- 91.93
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -352     |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 338    |
|    iterations      | 41     |
|    time_elapsed    | 992    |
|    total_timesteps | 335872 |
-------------------------------
Eval num_timesteps=340000, episode_reward=-383.18 +/- 124.98
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -383          |
| time/                   |               |
|    total_timesteps      | 340000        |
| train/                  |               |
|    approx_kl            | 0.00023931736 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.63         |
|    explained_variance   | 3.7e-06       |
|    learning_rate        | 5e-05         |
|    loss                 | 10.3          |
|    n_updates            | 410           |
|    policy_gradient_loss | -0.000281     |
|    std                  | 1.13          |
|    value_loss           | 39.5          |
-------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 334    |
|    iterations      | 42     |
|    time_elapsed    | 1028   |
|    total_timesteps | 344064 |
-------------------------------
Eval num_timesteps=345000, episode_reward=-439.84 +/- 287.94
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -440        |
| time/                   |             |
|    total_timesteps      | 345000      |
| train/                  |             |
|    approx_kl            | 0.002093505 |
|    clip_fraction        | 0.00189     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 1.97e-06    |
|    learning_rate        | 5e-05       |
|    loss                 | 6.33        |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.000529   |
|    std                  | 1.13        |
|    value_loss           | 17.3        |
-----------------------------------------
Eval num_timesteps=350000, episode_reward=-481.31 +/- 225.40
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -481     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 329    |
|    iterations      | 43     |
|    time_elapsed    | 1070   |
|    total_timesteps | 352256 |
-------------------------------
Eval num_timesteps=355000, episode_reward=-435.00 +/- 125.14
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -435        |
| time/                   |             |
|    total_timesteps      | 355000      |
| train/                  |             |
|    approx_kl            | 0.001244842 |
|    clip_fraction        | 0.000244    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 7.63e-06    |
|    learning_rate        | 5e-05       |
|    loss                 | 5.52        |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.000514   |
|    std                  | 1.13        |
|    value_loss           | 23.6        |
-----------------------------------------
Eval num_timesteps=360000, episode_reward=-496.71 +/- 181.39
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 323    |
|    iterations      | 44     |
|    time_elapsed    | 1113   |
|    total_timesteps | 360448 |
-------------------------------
Eval num_timesteps=365000, episode_reward=-449.17 +/- 134.59
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -449         |
| time/                   |              |
|    total_timesteps      | 365000       |
| train/                  |              |
|    approx_kl            | 0.0008613751 |
|    clip_fraction        | 2.44e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.63        |
|    explained_variance   | 3.52e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 10.3         |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.000426    |
|    std                  | 1.13         |
|    value_loss           | 40           |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 320    |
|    iterations      | 45     |
|    time_elapsed    | 1149   |
|    total_timesteps | 368640 |
-------------------------------
Eval num_timesteps=370000, episode_reward=-743.80 +/- 177.40
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -744        |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.002126764 |
|    clip_fraction        | 0.0033      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.63       |
|    explained_variance   | 3.81e-06    |
|    learning_rate        | 5e-05       |
|    loss                 | 6.66        |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.000917   |
|    std                  | 1.13        |
|    value_loss           | 23          |
-----------------------------------------
Eval num_timesteps=375000, episode_reward=-450.21 +/- 117.09
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -450     |
| time/              |          |
|    total_timesteps | 375000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 316    |
|    iterations      | 46     |
|    time_elapsed    | 1191   |
|    total_timesteps | 376832 |
-------------------------------
Eval num_timesteps=380000, episode_reward=-554.07 +/- 140.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -554         |
| time/                   |              |
|    total_timesteps      | 380000       |
| train/                  |              |
|    approx_kl            | 0.0021291464 |
|    clip_fraction        | 0.00148      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.63        |
|    explained_variance   | 4.11e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 2.68         |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.000516    |
|    std                  | 1.13         |
|    value_loss           | 11.8         |
------------------------------------------
Eval num_timesteps=385000, episode_reward=-353.62 +/- 134.38
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 312    |
|    iterations      | 47     |
|    time_elapsed    | 1233   |
|    total_timesteps | 385024 |
-------------------------------
Eval num_timesteps=390000, episode_reward=-380.23 +/- 169.04
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -380          |
| time/                   |               |
|    total_timesteps      | 390000        |
| train/                  |               |
|    approx_kl            | 0.00025978353 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.63         |
|    explained_variance   | 1.13e-06      |
|    learning_rate        | 5e-05         |
|    loss                 | 11            |
|    n_updates            | 470           |
|    policy_gradient_loss | -0.000254     |
|    std                  | 1.13          |
|    value_loss           | 27.1          |
-------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 309    |
|    iterations      | 48     |
|    time_elapsed    | 1269   |
|    total_timesteps | 393216 |
-------------------------------
Eval num_timesteps=395000, episode_reward=-414.86 +/- 132.59
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -415         |
| time/                   |              |
|    total_timesteps      | 395000       |
| train/                  |              |
|    approx_kl            | 0.0009489007 |
|    clip_fraction        | 0.000232     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.63        |
|    explained_variance   | 1.37e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 8.39         |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.000617    |
|    std                  | 1.13         |
|    value_loss           | 24.8         |
------------------------------------------
Eval num_timesteps=400000, episode_reward=-536.49 +/- 96.46
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 400000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 306    |
|    iterations      | 49     |
|    time_elapsed    | 1311   |
|    total_timesteps | 401408 |
-------------------------------
Eval num_timesteps=405000, episode_reward=-315.93 +/- 110.37
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -316          |
| time/                   |               |
|    total_timesteps      | 405000        |
| train/                  |               |
|    approx_kl            | 0.00013734499 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.63         |
|    explained_variance   | 3.87e-06      |
|    learning_rate        | 5e-05         |
|    loss                 | 9.52          |
|    n_updates            | 490           |
|    policy_gradient_loss | -0.000184     |
|    std                  | 1.13          |
|    value_loss           | 40.3          |
-------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 304    |
|    iterations      | 50     |
|    time_elapsed    | 1346   |
|    total_timesteps | 409600 |
-------------------------------
Eval num_timesteps=410000, episode_reward=-446.86 +/- 71.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -447         |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0036305953 |
|    clip_fraction        | 0.00847      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.64        |
|    explained_variance   | -5.72e-06    |
|    learning_rate        | 5e-05        |
|    loss                 | 2.38         |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00143     |
|    std                  | 1.14         |
|    value_loss           | 7.69         |
------------------------------------------
Eval num_timesteps=415000, episode_reward=-316.45 +/- 79.21
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 300    |
|    iterations      | 51     |
|    time_elapsed    | 1389   |
|    total_timesteps | 417792 |
-------------------------------
Eval num_timesteps=420000, episode_reward=-409.53 +/- 155.39
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -410         |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0013370689 |
|    clip_fraction        | 0.000623     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.65        |
|    explained_variance   | 7.45e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 4.31         |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.000311    |
|    std                  | 1.14         |
|    value_loss           | 10.3         |
------------------------------------------
Eval num_timesteps=425000, episode_reward=-514.44 +/- 127.91
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 297    |
|    iterations      | 52     |
|    time_elapsed    | 1431   |
|    total_timesteps | 425984 |
-------------------------------
Eval num_timesteps=430000, episode_reward=-355.97 +/- 82.00
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -356         |
| time/                   |              |
|    total_timesteps      | 430000       |
| train/                  |              |
|    approx_kl            | 0.0013488837 |
|    clip_fraction        | 0.000757     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.66        |
|    explained_variance   | 2.38e-07     |
|    learning_rate        | 5e-05        |
|    loss                 | 7.59         |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.000591    |
|    std                  | 1.14         |
|    value_loss           | 71.6         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 296    |
|    iterations      | 53     |
|    time_elapsed    | 1466   |
|    total_timesteps | 434176 |
-------------------------------
Eval num_timesteps=435000, episode_reward=-434.69 +/- 185.83
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -435         |
| time/                   |              |
|    total_timesteps      | 435000       |
| train/                  |              |
|    approx_kl            | 0.0017492638 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.66        |
|    explained_variance   | -2.62e-06    |
|    learning_rate        | 5e-05        |
|    loss                 | 4.66         |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.000369    |
|    std                  | 1.15         |
|    value_loss           | 19.3         |
------------------------------------------
Eval num_timesteps=440000, episode_reward=-410.86 +/- 189.62
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 293    |
|    iterations      | 54     |
|    time_elapsed    | 1507   |
|    total_timesteps | 442368 |
-------------------------------
Eval num_timesteps=445000, episode_reward=-555.34 +/- 175.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -555         |
| time/                   |              |
|    total_timesteps      | 445000       |
| train/                  |              |
|    approx_kl            | 0.0011820429 |
|    clip_fraction        | 0.000232     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.67        |
|    explained_variance   | -7.87e-06    |
|    learning_rate        | 5e-05        |
|    loss                 | 2.75         |
|    n_updates            | 540          |
|    policy_gradient_loss | -0.000557    |
|    std                  | 1.15         |
|    value_loss           | 13.7         |
------------------------------------------
Eval num_timesteps=450000, episode_reward=-403.54 +/- 114.80
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 290    |
|    iterations      | 55     |
|    time_elapsed    | 1549   |
|    total_timesteps | 450560 |
-------------------------------
Eval num_timesteps=455000, episode_reward=-547.66 +/- 277.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -548         |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0033129752 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.67        |
|    explained_variance   | 6.56e-06     |
|    learning_rate        | 5e-05        |
|    loss                 | 3.49         |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00194     |
|    std                  | 1.15         |
|    value_loss           | 6.76         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 289    |
|    iterations      | 56     |
|    time_elapsed    | 1584   |
|    total_timesteps | 458752 |
-------------------------------
Eval num_timesteps=460000, episode_reward=-380.96 +/- 189.73
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -381          |
| time/                   |               |
|    total_timesteps      | 460000        |
| train/                  |               |
|    approx_kl            | 0.00080981734 |
|    clip_fraction        | 0.000146      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.67         |
|    explained_variance   | 1.22e-05      |
|    learning_rate        | 5e-05         |
|    loss                 | 12.1          |
|    n_updates            | 560           |
|    policy_gradient_loss | -0.00035      |
|    std                  | 1.15          |
|    value_loss           | 28.8          |
-------------------------------------------
Eval num_timesteps=465000, episode_reward=-333.33 +/- 91.78
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 465000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 286    |
|    iterations      | 57     |
|    time_elapsed    | 1627   |
|    total_timesteps | 466944 |
-------------------------------
Eval num_timesteps=470000, episode_reward=-413.60 +/- 195.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 470000       |
| train/                  |              |
|    approx_kl            | 0.0031178505 |
|    clip_fraction        | 0.00831      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.67        |
|    explained_variance   | 2.43e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 3.52         |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.0013      |
|    std                  | 1.15         |
|    value_loss           | 9.63         |
------------------------------------------
Eval num_timesteps=475000, episode_reward=-299.49 +/- 91.80
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -299     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 284    |
|    iterations      | 58     |
|    time_elapsed    | 1667   |
|    total_timesteps | 475136 |
-------------------------------
Eval num_timesteps=480000, episode_reward=-437.85 +/- 121.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -438         |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0022513287 |
|    clip_fraction        | 0.0015       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.67        |
|    explained_variance   | 9.3e-06      |
|    learning_rate        | 5e-05        |
|    loss                 | 12.2         |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.00047     |
|    std                  | 1.15         |
|    value_loss           | 28.8         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 283    |
|    iterations      | 59     |
|    time_elapsed    | 1702   |
|    total_timesteps | 483328 |
-------------------------------
Eval num_timesteps=485000, episode_reward=-407.29 +/- 150.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -407         |
| time/                   |              |
|    total_timesteps      | 485000       |
| train/                  |              |
|    approx_kl            | 0.0022744052 |
|    clip_fraction        | 0.00121      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.68        |
|    explained_variance   | 2.72e-05     |
|    learning_rate        | 5e-05        |
|    loss                 | 11.5         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.000316    |
|    std                  | 1.15         |
|    value_loss           | 24.7         |
------------------------------------------
Eval num_timesteps=490000, episode_reward=-355.91 +/- 184.76
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -356     |
| time/              |          |
|    total_timesteps | 490000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 281    |
|    iterations      | 60     |
|    time_elapsed    | 1745   |
|    total_timesteps | 491520 |
-------------------------------
Eval num_timesteps=495000, episode_reward=-207.91 +/- 42.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -208         |
| time/                   |              |
|    total_timesteps      | 495000       |
| train/                  |              |
|    approx_kl            | 0.0042359503 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.68        |
|    explained_variance   | 0.000108     |
|    learning_rate        | 5e-05        |
|    loss                 | 5.19         |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.0017      |
|    std                  | 1.16         |
|    value_loss           | 14.1         |
------------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 280    |
|    iterations      | 61     |
|    time_elapsed    | 1781   |
|    total_timesteps | 499712 |
-------------------------------
Eval num_timesteps=500000, episode_reward=-299.93 +/- 122.01
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0026168055 |
|    clip_fraction        | 0.0028       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.000103     |
|    learning_rate        | 5e-05        |
|    loss                 | 7.07         |
|    n_updates            | 610          |
|    policy_gradient_loss | -0.00071     |
|    std                  | 1.16         |
|    value_loss           | 23.6         |
------------------------------------------
Eval num_timesteps=505000, episode_reward=-339.79 +/- 157.15
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 278    |
|    iterations      | 62     |
|    time_elapsed    | 1822   |
|    total_timesteps | 507904 |
-------------------------------
Eval num_timesteps=510000, episode_reward=-276.19 +/- 105.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -276         |
| time/                   |              |
|    total_timesteps      | 510000       |
| train/                  |              |
|    approx_kl            | 0.0007658118 |
|    clip_fraction        | 1.22e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.69        |
|    explained_variance   | 0.144        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.27         |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.000615    |
|    std                  | 1.16         |
|    value_loss           | 18           |
------------------------------------------
Eval num_timesteps=515000, episode_reward=-417.69 +/- 101.26
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 515000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 276    |
|    iterations      | 63     |
|    time_elapsed    | 1866   |
|    total_timesteps | 516096 |
-------------------------------
Eval num_timesteps=520000, episode_reward=-381.73 +/- 154.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -382         |
| time/                   |              |
|    total_timesteps      | 520000       |
| train/                  |              |
|    approx_kl            | 0.0017068228 |
|    clip_fraction        | 0.00153      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.7         |
|    explained_variance   | 0.284        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.46         |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.000444    |
|    std                  | 1.16         |
|    value_loss           | 10.4         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 275    |
|    iterations      | 64     |
|    time_elapsed    | 1901   |
|    total_timesteps | 524288 |
-------------------------------
Eval num_timesteps=525000, episode_reward=-362.76 +/- 132.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -363         |
| time/                   |              |
|    total_timesteps      | 525000       |
| train/                  |              |
|    approx_kl            | 0.0035755157 |
|    clip_fraction        | 0.00884      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.71        |
|    explained_variance   | 0.55         |
|    learning_rate        | 5e-05        |
|    loss                 | 4.39         |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00228     |
|    std                  | 1.17         |
|    value_loss           | 13.4         |
------------------------------------------
Eval num_timesteps=530000, episode_reward=-409.00 +/- 174.66
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -409     |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 274    |
|    iterations      | 65     |
|    time_elapsed    | 1943   |
|    total_timesteps | 532480 |
-------------------------------
Eval num_timesteps=535000, episode_reward=-325.75 +/- 202.02
Episode length: 816.00 +/- 370.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 816         |
|    mean_reward          | -326        |
| time/                   |             |
|    total_timesteps      | 535000      |
| train/                  |             |
|    approx_kl            | 0.002006606 |
|    clip_fraction        | 0.00228     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.72       |
|    explained_variance   | 0.856       |
|    learning_rate        | 5e-05       |
|    loss                 | 3.27        |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.000897   |
|    std                  | 1.17        |
|    value_loss           | 12          |
-----------------------------------------
Eval num_timesteps=540000, episode_reward=-345.13 +/- 150.39
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -345     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 272    |
|    iterations      | 66     |
|    time_elapsed    | 1984   |
|    total_timesteps | 540672 |
-------------------------------
Eval num_timesteps=545000, episode_reward=-327.50 +/- 103.90
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -328         |
| time/                   |              |
|    total_timesteps      | 545000       |
| train/                  |              |
|    approx_kl            | 0.0028470736 |
|    clip_fraction        | 0.00917      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.73        |
|    explained_variance   | 0.602        |
|    learning_rate        | 5e-05        |
|    loss                 | 5.43         |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.18         |
|    value_loss           | 24           |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 271    |
|    iterations      | 67     |
|    time_elapsed    | 2019   |
|    total_timesteps | 548864 |
-------------------------------
Eval num_timesteps=550000, episode_reward=-498.36 +/- 115.21
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -498        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.003203156 |
|    clip_fraction        | 0.00718     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.74       |
|    explained_variance   | 0.912       |
|    learning_rate        | 5e-05       |
|    loss                 | 10.4        |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00187    |
|    std                  | 1.18        |
|    value_loss           | 20.6        |
-----------------------------------------
Eval num_timesteps=555000, episode_reward=-249.40 +/- 140.32
Episode length: 834.60 +/- 332.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 835      |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 270    |
|    iterations      | 68     |
|    time_elapsed    | 2060   |
|    total_timesteps | 557056 |
-------------------------------
Eval num_timesteps=560000, episode_reward=-461.91 +/- 96.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -462         |
| time/                   |              |
|    total_timesteps      | 560000       |
| train/                  |              |
|    approx_kl            | 0.0021085953 |
|    clip_fraction        | 0.00238      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.75        |
|    explained_variance   | 0.927        |
|    learning_rate        | 5e-05        |
|    loss                 | 11.9         |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00104     |
|    std                  | 1.18         |
|    value_loss           | 20.1         |
------------------------------------------
Eval num_timesteps=565000, episode_reward=-411.62 +/- 170.65
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 268    |
|    iterations      | 69     |
|    time_elapsed    | 2104   |
|    total_timesteps | 565248 |
-------------------------------
Eval num_timesteps=570000, episode_reward=-500.81 +/- 168.04
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -501       |
| time/                   |            |
|    total_timesteps      | 570000     |
| train/                  |            |
|    approx_kl            | 0.00439898 |
|    clip_fraction        | 0.027      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.76      |
|    explained_variance   | 0.986      |
|    learning_rate        | 5e-05      |
|    loss                 | 3.8        |
|    n_updates            | 690        |
|    policy_gradient_loss | -0.00383   |
|    std                  | 1.19       |
|    value_loss           | 9.31       |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 267    |
|    iterations      | 70     |
|    time_elapsed    | 2140   |
|    total_timesteps | 573440 |
-------------------------------
Eval num_timesteps=575000, episode_reward=-436.73 +/- 87.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -437         |
| time/                   |              |
|    total_timesteps      | 575000       |
| train/                  |              |
|    approx_kl            | 0.0039648945 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.76        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.75         |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.0034      |
|    std                  | 1.19         |
|    value_loss           | 12.2         |
------------------------------------------
Eval num_timesteps=580000, episode_reward=-383.10 +/- 110.31
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 580000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 266    |
|    iterations      | 71     |
|    time_elapsed    | 2183   |
|    total_timesteps | 581632 |
-------------------------------
Eval num_timesteps=585000, episode_reward=-390.86 +/- 225.46
Episode length: 816.60 +/- 368.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 817         |
|    mean_reward          | -391        |
| time/                   |             |
|    total_timesteps      | 585000      |
| train/                  |             |
|    approx_kl            | 0.003951591 |
|    clip_fraction        | 0.0196      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.77       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 4.57        |
|    n_updates            | 710         |
|    policy_gradient_loss | -0.00426    |
|    std                  | 1.19        |
|    value_loss           | 15.9        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 265    |
|    iterations      | 72     |
|    time_elapsed    | 2217   |
|    total_timesteps | 589824 |
-------------------------------
Eval num_timesteps=590000, episode_reward=-519.26 +/- 81.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -519         |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0040670773 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.77        |
|    explained_variance   | 0.986        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.96         |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.00372     |
|    std                  | 1.19         |
|    value_loss           | 15.4         |
------------------------------------------
Eval num_timesteps=595000, episode_reward=-314.65 +/- 182.84
Episode length: 814.40 +/- 373.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 264    |
|    iterations      | 73     |
|    time_elapsed    | 2258   |
|    total_timesteps | 598016 |
-------------------------------
Eval num_timesteps=600000, episode_reward=-460.78 +/- 53.96
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -461       |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.00547147 |
|    clip_fraction        | 0.0392     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.78      |
|    explained_variance   | 0.991      |
|    learning_rate        | 5e-05      |
|    loss                 | 3.25       |
|    n_updates            | 730        |
|    policy_gradient_loss | -0.00526   |
|    std                  | 1.19       |
|    value_loss           | 11.8       |
----------------------------------------
Eval num_timesteps=605000, episode_reward=-411.94 +/- 150.47
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 605000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 263    |
|    iterations      | 74     |
|    time_elapsed    | 2301   |
|    total_timesteps | 606208 |
-------------------------------
Eval num_timesteps=610000, episode_reward=-428.72 +/- 69.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -429         |
| time/                   |              |
|    total_timesteps      | 610000       |
| train/                  |              |
|    approx_kl            | 0.0017300565 |
|    clip_fraction        | 0.00161      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.79        |
|    explained_variance   | 0.886        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.99         |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.000207    |
|    std                  | 1.2          |
|    value_loss           | 9.57         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 262    |
|    iterations      | 75     |
|    time_elapsed    | 2338   |
|    total_timesteps | 614400 |
-------------------------------
Eval num_timesteps=615000, episode_reward=-366.67 +/- 54.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -367         |
| time/                   |              |
|    total_timesteps      | 615000       |
| train/                  |              |
|    approx_kl            | 0.0029180343 |
|    clip_fraction        | 0.00419      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.43         |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00202     |
|    std                  | 1.2          |
|    value_loss           | 8.1          |
------------------------------------------
Eval num_timesteps=620000, episode_reward=-459.54 +/- 164.95
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -460     |
| time/              |          |
|    total_timesteps | 620000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 261    |
|    iterations      | 76     |
|    time_elapsed    | 2381   |
|    total_timesteps | 622592 |
-------------------------------
Eval num_timesteps=625000, episode_reward=-474.84 +/- 101.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -475         |
| time/                   |              |
|    total_timesteps      | 625000       |
| train/                  |              |
|    approx_kl            | 0.0011638894 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.8         |
|    explained_variance   | 0.974        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.96         |
|    n_updates            | 760          |
|    policy_gradient_loss | -0.000704    |
|    std                  | 1.2          |
|    value_loss           | 37.2         |
------------------------------------------
Eval num_timesteps=630000, episode_reward=-437.55 +/- 158.07
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 630000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 260    |
|    iterations      | 77     |
|    time_elapsed    | 2423   |
|    total_timesteps | 630784 |
-------------------------------
Eval num_timesteps=635000, episode_reward=-500.50 +/- 104.10
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -500         |
| time/                   |              |
|    total_timesteps      | 635000       |
| train/                  |              |
|    approx_kl            | 0.0034749038 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.81        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.771        |
|    n_updates            | 770          |
|    policy_gradient_loss | -0.00351     |
|    std                  | 1.21         |
|    value_loss           | 6.45         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 259    |
|    iterations      | 78     |
|    time_elapsed    | 2459   |
|    total_timesteps | 638976 |
-------------------------------
Eval num_timesteps=640000, episode_reward=-510.51 +/- 166.85
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -511        |
| time/                   |             |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.002786961 |
|    clip_fraction        | 0.00337     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.988       |
|    learning_rate        | 5e-05       |
|    loss                 | 2.85        |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.000957   |
|    std                  | 1.21        |
|    value_loss           | 46.6        |
-----------------------------------------
Eval num_timesteps=645000, episode_reward=-564.44 +/- 119.07
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 258    |
|    iterations      | 79     |
|    time_elapsed    | 2501   |
|    total_timesteps | 647168 |
-------------------------------
Eval num_timesteps=650000, episode_reward=-587.06 +/- 116.76
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -587        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.003411231 |
|    clip_fraction        | 0.00449     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.82       |
|    explained_variance   | 0.981       |
|    learning_rate        | 5e-05       |
|    loss                 | 32.1        |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.000985   |
|    std                  | 1.21        |
|    value_loss           | 64.1        |
-----------------------------------------
Eval num_timesteps=655000, episode_reward=-499.66 +/- 82.75
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 257    |
|    iterations      | 80     |
|    time_elapsed    | 2545   |
|    total_timesteps | 655360 |
-------------------------------
Eval num_timesteps=660000, episode_reward=-427.35 +/- 74.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -427         |
| time/                   |              |
|    total_timesteps      | 660000       |
| train/                  |              |
|    approx_kl            | 0.0036726084 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.82        |
|    explained_variance   | 0.985        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.65         |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00224     |
|    std                  | 1.21         |
|    value_loss           | 61.7         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 257    |
|    iterations      | 81     |
|    time_elapsed    | 2580   |
|    total_timesteps | 663552 |
-------------------------------
Eval num_timesteps=665000, episode_reward=-460.25 +/- 84.98
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -460         |
| time/                   |              |
|    total_timesteps      | 665000       |
| train/                  |              |
|    approx_kl            | 0.0023309016 |
|    clip_fraction        | 0.00249      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.83        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.63         |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.000329    |
|    std                  | 1.21         |
|    value_loss           | 31.2         |
------------------------------------------
Eval num_timesteps=670000, episode_reward=-411.97 +/- 216.22
Episode length: 801.00 +/- 400.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 801      |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 670000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 256    |
|    iterations      | 82     |
|    time_elapsed    | 2622   |
|    total_timesteps | 671744 |
-------------------------------
Eval num_timesteps=675000, episode_reward=-290.12 +/- 121.52
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -290        |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.005287299 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.84       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.914       |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00276    |
|    std                  | 1.21        |
|    value_loss           | 8.13        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 255    |
|    iterations      | 83     |
|    time_elapsed    | 2657   |
|    total_timesteps | 679936 |
-------------------------------
Eval num_timesteps=680000, episode_reward=-632.25 +/- 77.59
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -632         |
| time/                   |              |
|    total_timesteps      | 680000       |
| train/                  |              |
|    approx_kl            | 0.0031063557 |
|    clip_fraction        | 0.0024       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.84        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.59         |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.000629    |
|    std                  | 1.22         |
|    value_loss           | 25.1         |
------------------------------------------
Eval num_timesteps=685000, episode_reward=-536.33 +/- 193.72
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 685000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 254    |
|    iterations      | 84     |
|    time_elapsed    | 2702   |
|    total_timesteps | 688128 |
-------------------------------
Eval num_timesteps=690000, episode_reward=-333.03 +/- 111.66
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -333        |
| time/                   |             |
|    total_timesteps      | 690000      |
| train/                  |             |
|    approx_kl            | 0.004689758 |
|    clip_fraction        | 0.0132      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.84       |
|    explained_variance   | 0.996       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.42        |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.000969   |
|    std                  | 1.22        |
|    value_loss           | 10.3        |
-----------------------------------------
Eval num_timesteps=695000, episode_reward=-404.69 +/- 152.77
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 695000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 253    |
|    iterations      | 85     |
|    time_elapsed    | 2744   |
|    total_timesteps | 696320 |
-------------------------------
Eval num_timesteps=700000, episode_reward=-495.53 +/- 143.07
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -496         |
| time/                   |              |
|    total_timesteps      | 700000       |
| train/                  |              |
|    approx_kl            | 0.0039950223 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.85        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.75         |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00242     |
|    std                  | 1.22         |
|    value_loss           | 9.32         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 253    |
|    iterations      | 86     |
|    time_elapsed    | 2780   |
|    total_timesteps | 704512 |
-------------------------------
Eval num_timesteps=705000, episode_reward=-446.31 +/- 135.84
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -446         |
| time/                   |              |
|    total_timesteps      | 705000       |
| train/                  |              |
|    approx_kl            | 0.0025450322 |
|    clip_fraction        | 0.00165      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.85        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.15         |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.0011      |
|    std                  | 1.22         |
|    value_loss           | 5.18         |
------------------------------------------
Eval num_timesteps=710000, episode_reward=-429.72 +/- 116.70
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 710000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 252    |
|    iterations      | 87     |
|    time_elapsed    | 2824   |
|    total_timesteps | 712704 |
-------------------------------
Eval num_timesteps=715000, episode_reward=-552.64 +/- 159.86
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -553         |
| time/                   |              |
|    total_timesteps      | 715000       |
| train/                  |              |
|    approx_kl            | 0.0035081469 |
|    clip_fraction        | 0.00491      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.86        |
|    explained_variance   | 0.983        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.677        |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00131     |
|    std                  | 1.23         |
|    value_loss           | 18.9         |
------------------------------------------
Eval num_timesteps=720000, episode_reward=-467.67 +/- 111.23
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -468     |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 251    |
|    iterations      | 88     |
|    time_elapsed    | 2867   |
|    total_timesteps | 720896 |
-------------------------------
Eval num_timesteps=725000, episode_reward=-345.34 +/- 38.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -345         |
| time/                   |              |
|    total_timesteps      | 725000       |
| train/                  |              |
|    approx_kl            | 0.0047099283 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.86        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.51         |
|    n_updates            | 880          |
|    policy_gradient_loss | -0.00238     |
|    std                  | 1.23         |
|    value_loss           | 16           |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 251    |
|    iterations      | 89     |
|    time_elapsed    | 2904   |
|    total_timesteps | 729088 |
-------------------------------
Eval num_timesteps=730000, episode_reward=-456.21 +/- 115.15
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -456         |
| time/                   |              |
|    total_timesteps      | 730000       |
| train/                  |              |
|    approx_kl            | 0.0022331742 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.87        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 8.85         |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.000987    |
|    std                  | 1.23         |
|    value_loss           | 8.48         |
------------------------------------------
Eval num_timesteps=735000, episode_reward=-359.77 +/- 186.54
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 250    |
|    iterations      | 90     |
|    time_elapsed    | 2947   |
|    total_timesteps | 737280 |
-------------------------------
Eval num_timesteps=740000, episode_reward=-426.02 +/- 157.14
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -426         |
| time/                   |              |
|    total_timesteps      | 740000       |
| train/                  |              |
|    approx_kl            | 0.0046368497 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.88        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.725        |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00153     |
|    std                  | 1.23         |
|    value_loss           | 4.71         |
------------------------------------------
Eval num_timesteps=745000, episode_reward=-435.04 +/- 139.45
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -435     |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 249    |
|    iterations      | 91     |
|    time_elapsed    | 2990   |
|    total_timesteps | 745472 |
-------------------------------
Eval num_timesteps=750000, episode_reward=-262.78 +/- 90.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -263         |
| time/                   |              |
|    total_timesteps      | 750000       |
| train/                  |              |
|    approx_kl            | 0.0019543397 |
|    clip_fraction        | 0.00134      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.88        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.08         |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.000316    |
|    std                  | 1.23         |
|    value_loss           | 3.23         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 248    |
|    iterations      | 92     |
|    time_elapsed    | 3027   |
|    total_timesteps | 753664 |
-------------------------------
Eval num_timesteps=755000, episode_reward=-410.84 +/- 84.00
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -411       |
| time/                   |            |
|    total_timesteps      | 755000     |
| train/                  |            |
|    approx_kl            | 0.00415297 |
|    clip_fraction        | 0.0199     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.88      |
|    explained_variance   | 0.993      |
|    learning_rate        | 5e-05      |
|    loss                 | 2.08       |
|    n_updates            | 920        |
|    policy_gradient_loss | -0.00274   |
|    std                  | 1.24       |
|    value_loss           | 12         |
----------------------------------------
Eval num_timesteps=760000, episode_reward=-364.04 +/- 79.94
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 760000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 248    |
|    iterations      | 93     |
|    time_elapsed    | 3070   |
|    total_timesteps | 761856 |
-------------------------------
Eval num_timesteps=765000, episode_reward=-358.96 +/- 157.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -359         |
| time/                   |              |
|    total_timesteps      | 765000       |
| train/                  |              |
|    approx_kl            | 0.0037226027 |
|    clip_fraction        | 0.00835      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.89        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.01         |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00096     |
|    std                  | 1.24         |
|    value_loss           | 6.97         |
------------------------------------------
Eval num_timesteps=770000, episode_reward=-416.88 +/- 96.88
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 247    |
|    iterations      | 94     |
|    time_elapsed    | 3113   |
|    total_timesteps | 770048 |
-------------------------------
Eval num_timesteps=775000, episode_reward=-409.96 +/- 116.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -410         |
| time/                   |              |
|    total_timesteps      | 775000       |
| train/                  |              |
|    approx_kl            | 0.0033516963 |
|    clip_fraction        | 0.00952      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.89        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.563        |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00115     |
|    std                  | 1.24         |
|    value_loss           | 7.99         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 247    |
|    iterations      | 95     |
|    time_elapsed    | 3149   |
|    total_timesteps | 778240 |
-------------------------------
Eval num_timesteps=780000, episode_reward=-432.32 +/- 198.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -432         |
| time/                   |              |
|    total_timesteps      | 780000       |
| train/                  |              |
|    approx_kl            | 0.0031561381 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.9         |
|    explained_variance   | 0.886        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.574        |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.000904    |
|    std                  | 1.24         |
|    value_loss           | 116          |
------------------------------------------
Eval num_timesteps=785000, episode_reward=-313.59 +/- 92.25
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -314     |
| time/              |          |
|    total_timesteps | 785000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 246    |
|    iterations      | 96     |
|    time_elapsed    | 3192   |
|    total_timesteps | 786432 |
-------------------------------
Eval num_timesteps=790000, episode_reward=-432.66 +/- 164.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -433         |
| time/                   |              |
|    total_timesteps      | 790000       |
| train/                  |              |
|    approx_kl            | 0.0028138354 |
|    clip_fraction        | 0.00743      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.91        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.634        |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.000935    |
|    std                  | 1.25         |
|    value_loss           | 5.35         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 246    |
|    iterations      | 97     |
|    time_elapsed    | 3227   |
|    total_timesteps | 794624 |
-------------------------------
Eval num_timesteps=795000, episode_reward=-294.52 +/- 181.47
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -295         |
| time/                   |              |
|    total_timesteps      | 795000       |
| train/                  |              |
|    approx_kl            | 0.0034342231 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.92        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.966        |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00177     |
|    std                  | 1.25         |
|    value_loss           | 9.44         |
------------------------------------------
Eval num_timesteps=800000, episode_reward=-281.90 +/- 106.03
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 245    |
|    iterations      | 98     |
|    time_elapsed    | 3271   |
|    total_timesteps | 802816 |
-------------------------------
Eval num_timesteps=805000, episode_reward=-499.00 +/- 137.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -499         |
| time/                   |              |
|    total_timesteps      | 805000       |
| train/                  |              |
|    approx_kl            | 0.0028915838 |
|    clip_fraction        | 0.00667      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.94        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 34.4         |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.26         |
|    value_loss           | 10.2         |
------------------------------------------
Eval num_timesteps=810000, episode_reward=-291.99 +/- 45.13
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 810000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 244    |
|    iterations      | 99     |
|    time_elapsed    | 3314   |
|    total_timesteps | 811008 |
-------------------------------
Eval num_timesteps=815000, episode_reward=-356.61 +/- 41.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -357         |
| time/                   |              |
|    total_timesteps      | 815000       |
| train/                  |              |
|    approx_kl            | 0.0038344315 |
|    clip_fraction        | 0.00991      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.96        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.575        |
|    n_updates            | 990          |
|    policy_gradient_loss | -0.000922    |
|    std                  | 1.27         |
|    value_loss           | 15.8         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 244    |
|    iterations      | 100    |
|    time_elapsed    | 3350   |
|    total_timesteps | 819200 |
-------------------------------
Eval num_timesteps=820000, episode_reward=-402.96 +/- 129.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -403         |
| time/                   |              |
|    total_timesteps      | 820000       |
| train/                  |              |
|    approx_kl            | 0.0036302048 |
|    clip_fraction        | 0.0184       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.97        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 108          |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 1.27         |
|    value_loss           | 20.5         |
------------------------------------------
Eval num_timesteps=825000, episode_reward=-380.24 +/- 151.69
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -380     |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 243    |
|    iterations      | 101    |
|    time_elapsed    | 3394   |
|    total_timesteps | 827392 |
-------------------------------
Eval num_timesteps=830000, episode_reward=-370.26 +/- 179.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -370         |
| time/                   |              |
|    total_timesteps      | 830000       |
| train/                  |              |
|    approx_kl            | 0.0031106882 |
|    clip_fraction        | 0.00736      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.97        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.01         |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 1.27         |
|    value_loss           | 13.3         |
------------------------------------------
Eval num_timesteps=835000, episode_reward=-333.50 +/- 82.35
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 835000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 243    |
|    iterations      | 102    |
|    time_elapsed    | 3437   |
|    total_timesteps | 835584 |
-------------------------------
Eval num_timesteps=840000, episode_reward=-344.42 +/- 183.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -344         |
| time/                   |              |
|    total_timesteps      | 840000       |
| train/                  |              |
|    approx_kl            | 0.0032529957 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.98        |
|    explained_variance   | 0.982        |
|    learning_rate        | 5e-05        |
|    loss                 | 6.03         |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 1.27         |
|    value_loss           | 6.38         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 243    |
|    iterations      | 103    |
|    time_elapsed    | 3471   |
|    total_timesteps | 843776 |
-------------------------------
Eval num_timesteps=845000, episode_reward=-285.66 +/- 143.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 845000       |
| train/                  |              |
|    approx_kl            | 0.0033501708 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.98        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.502        |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00186     |
|    std                  | 1.28         |
|    value_loss           | 32.6         |
------------------------------------------
Eval num_timesteps=850000, episode_reward=-378.05 +/- 138.18
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 850000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 242    |
|    iterations      | 104    |
|    time_elapsed    | 3515   |
|    total_timesteps | 851968 |
-------------------------------
Eval num_timesteps=855000, episode_reward=-493.58 +/- 183.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -494         |
| time/                   |              |
|    total_timesteps      | 855000       |
| train/                  |              |
|    approx_kl            | 0.0006792686 |
|    clip_fraction        | 2.44e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.99        |
|    explained_variance   | 0.946        |
|    learning_rate        | 5e-05        |
|    loss                 | 178          |
|    n_updates            | 1040         |
|    policy_gradient_loss | 0.000158     |
|    std                  | 1.28         |
|    value_loss           | 122          |
------------------------------------------
Eval num_timesteps=860000, episode_reward=-378.39 +/- 68.44
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 241    |
|    iterations      | 105    |
|    time_elapsed    | 3560   |
|    total_timesteps | 860160 |
-------------------------------
Eval num_timesteps=865000, episode_reward=-375.43 +/- 199.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -375         |
| time/                   |              |
|    total_timesteps      | 865000       |
| train/                  |              |
|    approx_kl            | 0.0032654486 |
|    clip_fraction        | 0.00913      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.99        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.348        |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.000837    |
|    std                  | 1.28         |
|    value_loss           | 3.84         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 241    |
|    iterations      | 106    |
|    time_elapsed    | 3595   |
|    total_timesteps | 868352 |
-------------------------------
Eval num_timesteps=870000, episode_reward=-406.45 +/- 102.46
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 870000      |
| train/                  |             |
|    approx_kl            | 0.004477242 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.99       |
|    explained_variance   | 0.992       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.839       |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00209    |
|    std                  | 1.28        |
|    value_loss           | 8.41        |
-----------------------------------------
Eval num_timesteps=875000, episode_reward=-284.17 +/- 98.10
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 875000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 240    |
|    iterations      | 107    |
|    time_elapsed    | 3638   |
|    total_timesteps | 876544 |
-------------------------------
Eval num_timesteps=880000, episode_reward=-392.04 +/- 160.69
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -392         |
| time/                   |              |
|    total_timesteps      | 880000       |
| train/                  |              |
|    approx_kl            | 0.0042106276 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.99        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.46         |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00258     |
|    std                  | 1.28         |
|    value_loss           | 10.2         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 240    |
|    iterations      | 108    |
|    time_elapsed    | 3674   |
|    total_timesteps | 884736 |
-------------------------------
Eval num_timesteps=885000, episode_reward=-455.49 +/- 157.53
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -455        |
| time/                   |             |
|    total_timesteps      | 885000      |
| train/                  |             |
|    approx_kl            | 0.004429291 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.99       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.164       |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00109    |
|    std                  | 1.28        |
|    value_loss           | 3.19        |
-----------------------------------------
Eval num_timesteps=890000, episode_reward=-414.73 +/- 171.00
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 890000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 240    |
|    iterations      | 109    |
|    time_elapsed    | 3717   |
|    total_timesteps | 892928 |
-------------------------------
Eval num_timesteps=895000, episode_reward=-266.44 +/- 137.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -266         |
| time/                   |              |
|    total_timesteps      | 895000       |
| train/                  |              |
|    approx_kl            | 0.0021473574 |
|    clip_fraction        | 0.0024       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5           |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 5.13         |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.000862    |
|    std                  | 1.29         |
|    value_loss           | 6.5          |
------------------------------------------
Eval num_timesteps=900000, episode_reward=-482.83 +/- 206.93
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -483     |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 239    |
|    iterations      | 110    |
|    time_elapsed    | 3761   |
|    total_timesteps | 901120 |
-------------------------------
Eval num_timesteps=905000, episode_reward=-343.28 +/- 82.01
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -343         |
| time/                   |              |
|    total_timesteps      | 905000       |
| train/                  |              |
|    approx_kl            | 0.0029922286 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.01        |
|    explained_variance   | 0.979        |
|    learning_rate        | 5e-05        |
|    loss                 | 10.1         |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 1.29         |
|    value_loss           | 49.8         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 239    |
|    iterations      | 111    |
|    time_elapsed    | 3799   |
|    total_timesteps | 909312 |
-------------------------------
Eval num_timesteps=910000, episode_reward=-408.33 +/- 151.60
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -408         |
| time/                   |              |
|    total_timesteps      | 910000       |
| train/                  |              |
|    approx_kl            | 0.0047786855 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.01        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.211        |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 1.29         |
|    value_loss           | 5.09         |
------------------------------------------
Eval num_timesteps=915000, episode_reward=-355.37 +/- 150.93
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -355     |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 238    |
|    iterations      | 112    |
|    time_elapsed    | 3842   |
|    total_timesteps | 917504 |
-------------------------------
Eval num_timesteps=920000, episode_reward=-374.75 +/- 75.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -375         |
| time/                   |              |
|    total_timesteps      | 920000       |
| train/                  |              |
|    approx_kl            | 0.0045661535 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.01        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.331        |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 1.29         |
|    value_loss           | 2.78         |
------------------------------------------
Eval num_timesteps=925000, episode_reward=-384.92 +/- 130.55
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -385     |
| time/              |          |
|    total_timesteps | 925000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 238    |
|    iterations      | 113    |
|    time_elapsed    | 3886   |
|    total_timesteps | 925696 |
-------------------------------
Eval num_timesteps=930000, episode_reward=-369.68 +/- 193.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -370         |
| time/                   |              |
|    total_timesteps      | 930000       |
| train/                  |              |
|    approx_kl            | 0.0025357227 |
|    clip_fraction        | 0.00206      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.02        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.08         |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.000724    |
|    std                  | 1.29         |
|    value_loss           | 2.5          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 238    |
|    iterations      | 114    |
|    time_elapsed    | 3923   |
|    total_timesteps | 933888 |
-------------------------------
Eval num_timesteps=935000, episode_reward=-464.54 +/- 116.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -465        |
| time/                   |             |
|    total_timesteps      | 935000      |
| train/                  |             |
|    approx_kl            | 0.004301454 |
|    clip_fraction        | 0.0164      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.03       |
|    explained_variance   | 0.972       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.09        |
|    n_updates            | 1140        |
|    policy_gradient_loss | -0.00135    |
|    std                  | 1.3         |
|    value_loss           | 98.8        |
-----------------------------------------
Eval num_timesteps=940000, episode_reward=-495.85 +/- 164.49
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -496     |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 237    |
|    iterations      | 115    |
|    time_elapsed    | 3965   |
|    total_timesteps | 942080 |
-------------------------------
Eval num_timesteps=945000, episode_reward=-386.68 +/- 132.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -387         |
| time/                   |              |
|    total_timesteps      | 945000       |
| train/                  |              |
|    approx_kl            | 0.0043478934 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.05        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.202        |
|    n_updates            | 1150         |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.31         |
|    value_loss           | 2.59         |
------------------------------------------
Eval num_timesteps=950000, episode_reward=-392.58 +/- 199.64
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 237    |
|    iterations      | 116    |
|    time_elapsed    | 4008   |
|    total_timesteps | 950272 |
-------------------------------
Eval num_timesteps=955000, episode_reward=-349.07 +/- 204.90
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -349        |
| time/                   |             |
|    total_timesteps      | 955000      |
| train/                  |             |
|    approx_kl            | 0.002307981 |
|    clip_fraction        | 0.00551     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.05       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.151       |
|    n_updates            | 1160        |
|    policy_gradient_loss | -0.00063    |
|    std                  | 1.31        |
|    value_loss           | 5.16        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 236    |
|    iterations      | 117    |
|    time_elapsed    | 4044   |
|    total_timesteps | 958464 |
-------------------------------
Eval num_timesteps=960000, episode_reward=-281.00 +/- 86.07
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -281         |
| time/                   |              |
|    total_timesteps      | 960000       |
| train/                  |              |
|    approx_kl            | 0.0034183855 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.07        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.098        |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 1.31         |
|    value_loss           | 26.8         |
------------------------------------------
Eval num_timesteps=965000, episode_reward=-295.72 +/- 64.52
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 965000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 236    |
|    iterations      | 118    |
|    time_elapsed    | 4086   |
|    total_timesteps | 966656 |
-------------------------------
Eval num_timesteps=970000, episode_reward=-340.30 +/- 203.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -340         |
| time/                   |              |
|    total_timesteps      | 970000       |
| train/                  |              |
|    approx_kl            | 0.0010152684 |
|    clip_fraction        | 0.000806     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.07        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.553        |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.000265    |
|    std                  | 1.32         |
|    value_loss           | 11.7         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 236    |
|    iterations      | 119    |
|    time_elapsed    | 4121   |
|    total_timesteps | 974848 |
-------------------------------
Eval num_timesteps=975000, episode_reward=-239.25 +/- 41.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -239         |
| time/                   |              |
|    total_timesteps      | 975000       |
| train/                  |              |
|    approx_kl            | 0.0034689275 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.07        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.405        |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 1.31         |
|    value_loss           | 26.3         |
------------------------------------------
Eval num_timesteps=980000, episode_reward=-392.06 +/- 184.06
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 980000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 236    |
|    iterations      | 120    |
|    time_elapsed    | 4165   |
|    total_timesteps | 983040 |
-------------------------------
Eval num_timesteps=985000, episode_reward=-469.32 +/- 138.60
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -469         |
| time/                   |              |
|    total_timesteps      | 985000       |
| train/                  |              |
|    approx_kl            | 0.0043232692 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.07        |
|    explained_variance   | 0.966        |
|    learning_rate        | 5e-05        |
|    loss                 | 15.3         |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 1.32         |
|    value_loss           | 98.1         |
------------------------------------------
Eval num_timesteps=990000, episode_reward=-357.61 +/- 181.69
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -358     |
| time/              |          |
|    total_timesteps | 990000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 235    |
|    iterations      | 121    |
|    time_elapsed    | 4207   |
|    total_timesteps | 991232 |
-------------------------------
Eval num_timesteps=995000, episode_reward=-391.62 +/- 146.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -392         |
| time/                   |              |
|    total_timesteps      | 995000       |
| train/                  |              |
|    approx_kl            | 0.0014521729 |
|    clip_fraction        | 0.000671     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.08        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.609        |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.000425    |
|    std                  | 1.32         |
|    value_loss           | 50           |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 235    |
|    iterations      | 122    |
|    time_elapsed    | 4244   |
|    total_timesteps | 999424 |
-------------------------------
Eval num_timesteps=1000000, episode_reward=-367.56 +/- 130.71
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -368        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.003928037 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.09       |
|    explained_variance   | 0.991       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.69        |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00122    |
|    std                  | 1.33        |
|    value_loss           | 32.3        |
-----------------------------------------
Eval num_timesteps=1005000, episode_reward=-387.65 +/- 125.25
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 1005000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 235     |
|    iterations      | 123     |
|    time_elapsed    | 4287    |
|    total_timesteps | 1007616 |
--------------------------------
Eval num_timesteps=1010000, episode_reward=-259.34 +/- 156.82
Episode length: 815.40 +/- 371.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -259         |
| time/                   |              |
|    total_timesteps      | 1010000      |
| train/                  |              |
|    approx_kl            | 0.0027122844 |
|    clip_fraction        | 0.00844      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.11        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 50.3         |
|    n_updates            | 1230         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 1.33         |
|    value_loss           | 55.2         |
------------------------------------------
Eval num_timesteps=1015000, episode_reward=-403.61 +/- 131.41
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 1015000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 124     |
|    time_elapsed    | 4329    |
|    total_timesteps | 1015808 |
--------------------------------
Eval num_timesteps=1020000, episode_reward=-374.71 +/- 43.22
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -375         |
| time/                   |              |
|    total_timesteps      | 1020000      |
| train/                  |              |
|    approx_kl            | 0.0023332825 |
|    clip_fraction        | 0.00227      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.12        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.768        |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.000505    |
|    std                  | 1.34         |
|    value_loss           | 50.4         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 125     |
|    time_elapsed    | 4364    |
|    total_timesteps | 1024000 |
--------------------------------
Eval num_timesteps=1025000, episode_reward=-329.56 +/- 97.60
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -330         |
| time/                   |              |
|    total_timesteps      | 1025000      |
| train/                  |              |
|    approx_kl            | 0.0033099274 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.12        |
|    explained_variance   | 0.979        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.3          |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 1.34         |
|    value_loss           | 98.6         |
------------------------------------------
Eval num_timesteps=1030000, episode_reward=-300.49 +/- 78.72
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1030000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 126     |
|    time_elapsed    | 4407    |
|    total_timesteps | 1032192 |
--------------------------------
Eval num_timesteps=1035000, episode_reward=-366.60 +/- 199.52
Episode length: 813.40 +/- 375.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -367         |
| time/                   |              |
|    total_timesteps      | 1035000      |
| train/                  |              |
|    approx_kl            | 0.0033642477 |
|    clip_fraction        | 0.00881      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.12        |
|    explained_variance   | 0.987        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.313        |
|    n_updates            | 1260         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 1.34         |
|    value_loss           | 56.4         |
------------------------------------------
Eval num_timesteps=1040000, episode_reward=-389.85 +/- 151.52
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 1040000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 127     |
|    time_elapsed    | 4448    |
|    total_timesteps | 1040384 |
--------------------------------
Eval num_timesteps=1045000, episode_reward=-438.87 +/- 293.69
Episode length: 816.80 +/- 368.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 817          |
|    mean_reward          | -439         |
| time/                   |              |
|    total_timesteps      | 1045000      |
| train/                  |              |
|    approx_kl            | 0.0021464606 |
|    clip_fraction        | 0.000745     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.13        |
|    explained_variance   | 0.967        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.461        |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.000293    |
|    std                  | 1.34         |
|    value_loss           | 150          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 128     |
|    time_elapsed    | 4484    |
|    total_timesteps | 1048576 |
--------------------------------
Eval num_timesteps=1050000, episode_reward=-441.90 +/- 169.59
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -442        |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.002762678 |
|    clip_fraction        | 0.0111      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.14       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.167       |
|    n_updates            | 1280        |
|    policy_gradient_loss | -0.00141    |
|    std                  | 1.35        |
|    value_loss           | 6.51        |
-----------------------------------------
Eval num_timesteps=1055000, episode_reward=-349.55 +/- 65.23
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 1055000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 129     |
|    time_elapsed    | 4530    |
|    total_timesteps | 1056768 |
--------------------------------
Eval num_timesteps=1060000, episode_reward=-489.87 +/- 86.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -490         |
| time/                   |              |
|    total_timesteps      | 1060000      |
| train/                  |              |
|    approx_kl            | 0.0026131906 |
|    clip_fraction        | 0.00546      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.16        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.294        |
|    n_updates            | 1290         |
|    policy_gradient_loss | -8.82e-05    |
|    std                  | 1.36         |
|    value_loss           | 41.5         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 130     |
|    time_elapsed    | 4566    |
|    total_timesteps | 1064960 |
--------------------------------
Eval num_timesteps=1065000, episode_reward=-412.21 +/- 107.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -412         |
| time/                   |              |
|    total_timesteps      | 1065000      |
| train/                  |              |
|    approx_kl            | 0.0028441148 |
|    clip_fraction        | 0.00886      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.17        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.476        |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 1.36         |
|    value_loss           | 36.8         |
------------------------------------------
Eval num_timesteps=1070000, episode_reward=-380.60 +/- 127.03
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -381     |
| time/              |          |
|    total_timesteps | 1070000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 131     |
|    time_elapsed    | 4609    |
|    total_timesteps | 1073152 |
--------------------------------
Eval num_timesteps=1075000, episode_reward=-335.61 +/- 95.16
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -336         |
| time/                   |              |
|    total_timesteps      | 1075000      |
| train/                  |              |
|    approx_kl            | 0.0035038204 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.19        |
|    explained_variance   | 0.978        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.626        |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 1.37         |
|    value_loss           | 114          |
------------------------------------------
Eval num_timesteps=1080000, episode_reward=-216.10 +/- 47.28
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 1080000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 132     |
|    time_elapsed    | 4653    |
|    total_timesteps | 1081344 |
--------------------------------
Eval num_timesteps=1085000, episode_reward=-413.95 +/- 166.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 1085000      |
| train/                  |              |
|    approx_kl            | 0.0044695926 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.2         |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.286        |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 1.38         |
|    value_loss           | 34.7         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 133     |
|    time_elapsed    | 4689    |
|    total_timesteps | 1089536 |
--------------------------------
Eval num_timesteps=1090000, episode_reward=-468.55 +/- 149.37
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -469         |
| time/                   |              |
|    total_timesteps      | 1090000      |
| train/                  |              |
|    approx_kl            | 0.0031752284 |
|    clip_fraction        | 0.00667      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.22        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.37         |
|    n_updates            | 1330         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 1.39         |
|    value_loss           | 3.04         |
------------------------------------------
Eval num_timesteps=1095000, episode_reward=-363.77 +/- 150.45
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 1095000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 134     |
|    time_elapsed    | 4733    |
|    total_timesteps | 1097728 |
--------------------------------
Eval num_timesteps=1100000, episode_reward=-365.16 +/- 100.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 1100000      |
| train/                  |              |
|    approx_kl            | 0.0018123534 |
|    clip_fraction        | 0.0015       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.23        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.424        |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.000518    |
|    std                  | 1.39         |
|    value_loss           | 23           |
------------------------------------------
Eval num_timesteps=1105000, episode_reward=-394.45 +/- 119.98
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 1105000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 135     |
|    time_elapsed    | 4776    |
|    total_timesteps | 1105920 |
--------------------------------
Eval num_timesteps=1110000, episode_reward=-358.10 +/- 119.16
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -358        |
| time/                   |             |
|    total_timesteps      | 1110000     |
| train/                  |             |
|    approx_kl            | 0.003705835 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.24       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 17.3        |
|    n_updates            | 1350        |
|    policy_gradient_loss | -0.00171    |
|    std                  | 1.39        |
|    value_loss           | 34.9        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 136     |
|    time_elapsed    | 4812    |
|    total_timesteps | 1114112 |
--------------------------------
Eval num_timesteps=1115000, episode_reward=-381.47 +/- 168.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -381         |
| time/                   |              |
|    total_timesteps      | 1115000      |
| train/                  |              |
|    approx_kl            | 0.0022550938 |
|    clip_fraction        | 0.00396      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.24        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.417        |
|    n_updates            | 1360         |
|    policy_gradient_loss | -0.00106     |
|    std                  | 1.4          |
|    value_loss           | 2.55         |
------------------------------------------
Eval num_timesteps=1120000, episode_reward=-533.79 +/- 69.19
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 1120000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 137     |
|    time_elapsed    | 4856    |
|    total_timesteps | 1122304 |
--------------------------------
Eval num_timesteps=1125000, episode_reward=-310.63 +/- 252.65
Episode length: 823.80 +/- 354.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 824          |
|    mean_reward          | -311         |
| time/                   |              |
|    total_timesteps      | 1125000      |
| train/                  |              |
|    approx_kl            | 0.0028548802 |
|    clip_fraction        | 0.00424      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.25        |
|    explained_variance   | 0.987        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.271        |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.000827    |
|    std                  | 1.4          |
|    value_loss           | 42.8         |
------------------------------------------
Eval num_timesteps=1130000, episode_reward=-384.04 +/- 155.04
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -384     |
| time/              |          |
|    total_timesteps | 1130000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 138     |
|    time_elapsed    | 4898    |
|    total_timesteps | 1130496 |
--------------------------------
Eval num_timesteps=1135000, episode_reward=-427.17 +/- 186.42
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -427        |
| time/                   |             |
|    total_timesteps      | 1135000     |
| train/                  |             |
|    approx_kl            | 0.004196086 |
|    clip_fraction        | 0.0213      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.25       |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 3.88        |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00162    |
|    std                  | 1.4         |
|    value_loss           | 2.17        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 139     |
|    time_elapsed    | 4933    |
|    total_timesteps | 1138688 |
--------------------------------
Eval num_timesteps=1140000, episode_reward=-538.91 +/- 245.49
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -539         |
| time/                   |              |
|    total_timesteps      | 1140000      |
| train/                  |              |
|    approx_kl            | 0.0042996127 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.26        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.405        |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00258     |
|    std                  | 1.41         |
|    value_loss           | 43.3         |
------------------------------------------
Eval num_timesteps=1145000, episode_reward=-286.88 +/- 143.80
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 1145000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 140     |
|    time_elapsed    | 4977    |
|    total_timesteps | 1146880 |
--------------------------------
Eval num_timesteps=1150000, episode_reward=-390.65 +/- 119.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -391         |
| time/                   |              |
|    total_timesteps      | 1150000      |
| train/                  |              |
|    approx_kl            | 0.0048392736 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.28        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.786        |
|    n_updates            | 1400         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 1.41         |
|    value_loss           | 1.27         |
------------------------------------------
Eval num_timesteps=1155000, episode_reward=-293.38 +/- 104.11
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 1155000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 141     |
|    time_elapsed    | 5021    |
|    total_timesteps | 1155072 |
--------------------------------
Eval num_timesteps=1160000, episode_reward=-403.82 +/- 165.05
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -404        |
| time/                   |             |
|    total_timesteps      | 1160000     |
| train/                  |             |
|    approx_kl            | 0.003676969 |
|    clip_fraction        | 0.0198      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.29       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 269         |
|    n_updates            | 1410        |
|    policy_gradient_loss | -0.00194    |
|    std                  | 1.42        |
|    value_loss           | 42.7        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 142     |
|    time_elapsed    | 5056    |
|    total_timesteps | 1163264 |
--------------------------------
Eval num_timesteps=1165000, episode_reward=-419.88 +/- 141.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -420         |
| time/                   |              |
|    total_timesteps      | 1165000      |
| train/                  |              |
|    approx_kl            | 0.0023344907 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.3         |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 10.6         |
|    n_updates            | 1420         |
|    policy_gradient_loss | -0.000812    |
|    std                  | 1.43         |
|    value_loss           | 8.33         |
------------------------------------------
Eval num_timesteps=1170000, episode_reward=-324.69 +/- 211.10
Episode length: 827.40 +/- 347.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 827      |
|    mean_reward     | -325     |
| time/              |          |
|    total_timesteps | 1170000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 229     |
|    iterations      | 143     |
|    time_elapsed    | 5098    |
|    total_timesteps | 1171456 |
--------------------------------
Eval num_timesteps=1175000, episode_reward=-456.78 +/- 158.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -457         |
| time/                   |              |
|    total_timesteps      | 1175000      |
| train/                  |              |
|    approx_kl            | 0.0049038054 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.32        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.091        |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.0028      |
|    std                  | 1.43         |
|    value_loss           | 3.46         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 229     |
|    iterations      | 144     |
|    time_elapsed    | 5135    |
|    total_timesteps | 1179648 |
--------------------------------
Eval num_timesteps=1180000, episode_reward=-355.22 +/- 138.89
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -355        |
| time/                   |             |
|    total_timesteps      | 1180000     |
| train/                  |             |
|    approx_kl            | 0.004325552 |
|    clip_fraction        | 0.0128      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.33       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 2.01        |
|    n_updates            | 1440        |
|    policy_gradient_loss | -0.00223    |
|    std                  | 1.43        |
|    value_loss           | 11.5        |
-----------------------------------------
Eval num_timesteps=1185000, episode_reward=-350.29 +/- 135.66
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 1185000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 229     |
|    iterations      | 145     |
|    time_elapsed    | 5179    |
|    total_timesteps | 1187840 |
--------------------------------
Eval num_timesteps=1190000, episode_reward=-351.36 +/- 104.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -351         |
| time/                   |              |
|    total_timesteps      | 1190000      |
| train/                  |              |
|    approx_kl            | 0.0029154124 |
|    clip_fraction        | 0.00945      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.32        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 43.4         |
|    n_updates            | 1450         |
|    policy_gradient_loss | -0.000974    |
|    std                  | 1.43         |
|    value_loss           | 47.2         |
------------------------------------------
Eval num_timesteps=1195000, episode_reward=-424.85 +/- 187.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 1195000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 146     |
|    time_elapsed    | 5223    |
|    total_timesteps | 1196032 |
--------------------------------
Eval num_timesteps=1200000, episode_reward=-456.92 +/- 180.00
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -457          |
| time/                   |               |
|    total_timesteps      | 1200000       |
| train/                  |               |
|    approx_kl            | 0.00077142066 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.33         |
|    explained_variance   | 0.985         |
|    learning_rate        | 5e-05         |
|    loss                 | 34.2          |
|    n_updates            | 1460          |
|    policy_gradient_loss | -0.000488     |
|    std                  | 1.44          |
|    value_loss           | 62            |
-------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 147     |
|    time_elapsed    | 5260    |
|    total_timesteps | 1204224 |
--------------------------------
Eval num_timesteps=1205000, episode_reward=-406.98 +/- 146.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -407         |
| time/                   |              |
|    total_timesteps      | 1205000      |
| train/                  |              |
|    approx_kl            | 0.0046280706 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.34        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 16.1         |
|    n_updates            | 1470         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 1.44         |
|    value_loss           | 43           |
------------------------------------------
Eval num_timesteps=1210000, episode_reward=-390.91 +/- 117.68
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 1210000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 148     |
|    time_elapsed    | 5302    |
|    total_timesteps | 1212416 |
--------------------------------
Eval num_timesteps=1215000, episode_reward=-238.89 +/- 118.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -239         |
| time/                   |              |
|    total_timesteps      | 1215000      |
| train/                  |              |
|    approx_kl            | 0.0057936744 |
|    clip_fraction        | 0.0266       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.35        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.02         |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 1.44         |
|    value_loss           | 27.4         |
------------------------------------------
Eval num_timesteps=1220000, episode_reward=-300.92 +/- 168.82
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 1220000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 149     |
|    time_elapsed    | 5345    |
|    total_timesteps | 1220608 |
--------------------------------
Eval num_timesteps=1225000, episode_reward=-456.51 +/- 223.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -457         |
| time/                   |              |
|    total_timesteps      | 1225000      |
| train/                  |              |
|    approx_kl            | 0.0031266897 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.35        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 10.5         |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 1.45         |
|    value_loss           | 61.7         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 150     |
|    time_elapsed    | 5381    |
|    total_timesteps | 1228800 |
--------------------------------
Eval num_timesteps=1230000, episode_reward=-279.86 +/- 88.33
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -280         |
| time/                   |              |
|    total_timesteps      | 1230000      |
| train/                  |              |
|    approx_kl            | 0.0027140172 |
|    clip_fraction        | 0.00814      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.36        |
|    explained_variance   | 0.984        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.18         |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.000847    |
|    std                  | 1.45         |
|    value_loss           | 95.8         |
------------------------------------------
Eval num_timesteps=1235000, episode_reward=-331.85 +/- 73.20
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -332     |
| time/              |          |
|    total_timesteps | 1235000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 151     |
|    time_elapsed    | 5424    |
|    total_timesteps | 1236992 |
--------------------------------
Eval num_timesteps=1240000, episode_reward=-316.68 +/- 157.34
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -317         |
| time/                   |              |
|    total_timesteps      | 1240000      |
| train/                  |              |
|    approx_kl            | 0.0012753766 |
|    clip_fraction        | 0.00159      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.37        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.157        |
|    n_updates            | 1510         |
|    policy_gradient_loss | -0.000611    |
|    std                  | 1.45         |
|    value_loss           | 17.5         |
------------------------------------------
Eval num_timesteps=1245000, episode_reward=-377.81 +/- 83.05
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 1245000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 152     |
|    time_elapsed    | 5469    |
|    total_timesteps | 1245184 |
--------------------------------
Eval num_timesteps=1250000, episode_reward=-410.41 +/- 145.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -410         |
| time/                   |              |
|    total_timesteps      | 1250000      |
| train/                  |              |
|    approx_kl            | 0.0023236745 |
|    clip_fraction        | 0.00686      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.37        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 29.5         |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.000416    |
|    std                  | 1.46         |
|    value_loss           | 57.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 153     |
|    time_elapsed    | 5506    |
|    total_timesteps | 1253376 |
--------------------------------
Eval num_timesteps=1255000, episode_reward=-519.11 +/- 204.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -519         |
| time/                   |              |
|    total_timesteps      | 1255000      |
| train/                  |              |
|    approx_kl            | 0.0054349923 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.38        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.074        |
|    n_updates            | 1530         |
|    policy_gradient_loss | -0.0022      |
|    std                  | 1.46         |
|    value_loss           | 47.2         |
------------------------------------------
Eval num_timesteps=1260000, episode_reward=-317.72 +/- 224.78
Episode length: 817.20 +/- 367.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | -318     |
| time/              |          |
|    total_timesteps | 1260000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 154     |
|    time_elapsed    | 5548    |
|    total_timesteps | 1261568 |
--------------------------------
Eval num_timesteps=1265000, episode_reward=-444.20 +/- 104.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -444        |
| time/                   |             |
|    total_timesteps      | 1265000     |
| train/                  |             |
|    approx_kl            | 0.004130994 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.38       |
|    explained_variance   | 0.989       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.4         |
|    n_updates            | 1540        |
|    policy_gradient_loss | -0.0012     |
|    std                  | 1.46        |
|    value_loss           | 90.9        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 155     |
|    time_elapsed    | 5584    |
|    total_timesteps | 1269760 |
--------------------------------
Eval num_timesteps=1270000, episode_reward=-257.75 +/- 103.37
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -258         |
| time/                   |              |
|    total_timesteps      | 1270000      |
| train/                  |              |
|    approx_kl            | 0.0047196206 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.39        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 16.8         |
|    n_updates            | 1550         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.47         |
|    value_loss           | 25.1         |
------------------------------------------
Eval num_timesteps=1275000, episode_reward=-451.15 +/- 171.18
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -451     |
| time/              |          |
|    total_timesteps | 1275000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 156     |
|    time_elapsed    | 5627    |
|    total_timesteps | 1277952 |
--------------------------------
Eval num_timesteps=1280000, episode_reward=-331.82 +/- 104.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 1280000      |
| train/                  |              |
|    approx_kl            | 0.0008120631 |
|    clip_fraction        | 0.00011      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.39        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 19.6         |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.000236    |
|    std                  | 1.47         |
|    value_loss           | 64.1         |
------------------------------------------
Eval num_timesteps=1285000, episode_reward=-437.05 +/- 165.15
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 1285000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 157     |
|    time_elapsed    | 5671    |
|    total_timesteps | 1286144 |
--------------------------------
Eval num_timesteps=1290000, episode_reward=-295.47 +/- 160.52
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -295        |
| time/                   |             |
|    total_timesteps      | 1290000     |
| train/                  |             |
|    approx_kl            | 0.002488128 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.4        |
|    explained_variance   | 0.983       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.483       |
|    n_updates            | 1570        |
|    policy_gradient_loss | -0.000375   |
|    std                  | 1.47        |
|    value_loss           | 139         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 158     |
|    time_elapsed    | 5708    |
|    total_timesteps | 1294336 |
--------------------------------
Eval num_timesteps=1295000, episode_reward=-323.28 +/- 158.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -323        |
| time/                   |             |
|    total_timesteps      | 1295000     |
| train/                  |             |
|    approx_kl            | 0.004653701 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.4        |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.363       |
|    n_updates            | 1580        |
|    policy_gradient_loss | -0.00262    |
|    std                  | 1.47        |
|    value_loss           | 3.45        |
-----------------------------------------
Eval num_timesteps=1300000, episode_reward=-311.83 +/- 219.24
Episode length: 817.40 +/- 367.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 817      |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 1300000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 159     |
|    time_elapsed    | 5751    |
|    total_timesteps | 1302528 |
--------------------------------
Eval num_timesteps=1305000, episode_reward=-383.42 +/- 124.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -383         |
| time/                   |              |
|    total_timesteps      | 1305000      |
| train/                  |              |
|    approx_kl            | 0.0037217294 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.4         |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.182        |
|    n_updates            | 1590         |
|    policy_gradient_loss | -0.00121     |
|    std                  | 1.47         |
|    value_loss           | 54.8         |
------------------------------------------
Eval num_timesteps=1310000, episode_reward=-307.34 +/- 120.46
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 1310000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 160     |
|    time_elapsed    | 5795    |
|    total_timesteps | 1310720 |
--------------------------------
Eval num_timesteps=1315000, episode_reward=-333.38 +/- 137.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -333         |
| time/                   |              |
|    total_timesteps      | 1315000      |
| train/                  |              |
|    approx_kl            | 0.0023464067 |
|    clip_fraction        | 0.00242      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.41        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 87.5         |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.000136    |
|    std                  | 1.48         |
|    value_loss           | 61.6         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 161     |
|    time_elapsed    | 5832    |
|    total_timesteps | 1318912 |
--------------------------------
Eval num_timesteps=1320000, episode_reward=-437.07 +/- 165.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -437         |
| time/                   |              |
|    total_timesteps      | 1320000      |
| train/                  |              |
|    approx_kl            | 0.0022848106 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.41        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.183        |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.000491    |
|    std                  | 1.48         |
|    value_loss           | 62.3         |
------------------------------------------
Eval num_timesteps=1325000, episode_reward=-376.04 +/- 216.53
Episode length: 810.40 +/- 381.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1325000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 162     |
|    time_elapsed    | 5873    |
|    total_timesteps | 1327104 |
--------------------------------
Eval num_timesteps=1330000, episode_reward=-441.44 +/- 95.10
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -441        |
| time/                   |             |
|    total_timesteps      | 1330000     |
| train/                  |             |
|    approx_kl            | 0.001909612 |
|    clip_fraction        | 0.00311     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.42       |
|    explained_variance   | 0.975       |
|    learning_rate        | 5e-05       |
|    loss                 | 952         |
|    n_updates            | 1620        |
|    policy_gradient_loss | -0.000856   |
|    std                  | 1.48        |
|    value_loss           | 218         |
-----------------------------------------
Eval num_timesteps=1335000, episode_reward=-334.94 +/- 84.88
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 1335000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 163     |
|    time_elapsed    | 5916    |
|    total_timesteps | 1335296 |
--------------------------------
Eval num_timesteps=1340000, episode_reward=-433.64 +/- 150.85
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -434        |
| time/                   |             |
|    total_timesteps      | 1340000     |
| train/                  |             |
|    approx_kl            | 0.003918482 |
|    clip_fraction        | 0.02        |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.43       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0924      |
|    n_updates            | 1630        |
|    policy_gradient_loss | -0.00133    |
|    std                  | 1.49        |
|    value_loss           | 20.8        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 164     |
|    time_elapsed    | 5953    |
|    total_timesteps | 1343488 |
--------------------------------
Eval num_timesteps=1345000, episode_reward=-415.73 +/- 156.59
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 1345000     |
| train/                  |             |
|    approx_kl            | 0.004175719 |
|    clip_fraction        | 0.0139      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.44       |
|    explained_variance   | 0.992       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.104       |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00178    |
|    std                  | 1.49        |
|    value_loss           | 86.6        |
-----------------------------------------
Eval num_timesteps=1350000, episode_reward=-321.31 +/- 152.40
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -321     |
| time/              |          |
|    total_timesteps | 1350000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 165     |
|    time_elapsed    | 5997    |
|    total_timesteps | 1351680 |
--------------------------------
Eval num_timesteps=1355000, episode_reward=-392.25 +/- 139.00
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -392        |
| time/                   |             |
|    total_timesteps      | 1355000     |
| train/                  |             |
|    approx_kl            | 0.003393887 |
|    clip_fraction        | 0.0137      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.44       |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.56        |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.00194    |
|    std                  | 1.49        |
|    value_loss           | 7.18        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 166     |
|    time_elapsed    | 6033    |
|    total_timesteps | 1359872 |
--------------------------------
Eval num_timesteps=1360000, episode_reward=-399.85 +/- 126.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -400         |
| time/                   |              |
|    total_timesteps      | 1360000      |
| train/                  |              |
|    approx_kl            | 0.0035025626 |
|    clip_fraction        | 0.0098       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.44        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.287        |
|    n_updates            | 1660         |
|    policy_gradient_loss | -0.000982    |
|    std                  | 1.49         |
|    value_loss           | 39.4         |
------------------------------------------
Eval num_timesteps=1365000, episode_reward=-344.47 +/- 66.70
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 1365000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 167     |
|    time_elapsed    | 6078    |
|    total_timesteps | 1368064 |
--------------------------------
Eval num_timesteps=1370000, episode_reward=-385.73 +/- 128.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -386         |
| time/                   |              |
|    total_timesteps      | 1370000      |
| train/                  |              |
|    approx_kl            | 0.0028711313 |
|    clip_fraction        | 0.00725      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.45        |
|    explained_variance   | 0.986        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.8          |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.000838    |
|    std                  | 1.5          |
|    value_loss           | 143          |
------------------------------------------
Eval num_timesteps=1375000, episode_reward=-287.70 +/- 115.82
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 1375000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 168     |
|    time_elapsed    | 6121    |
|    total_timesteps | 1376256 |
--------------------------------
Eval num_timesteps=1380000, episode_reward=-418.27 +/- 125.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -418         |
| time/                   |              |
|    total_timesteps      | 1380000      |
| train/                  |              |
|    approx_kl            | 0.0034085852 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.46        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.164        |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 1.5          |
|    value_loss           | 81.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 169     |
|    time_elapsed    | 6157    |
|    total_timesteps | 1384448 |
--------------------------------
Eval num_timesteps=1385000, episode_reward=-416.80 +/- 159.36
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -417         |
| time/                   |              |
|    total_timesteps      | 1385000      |
| train/                  |              |
|    approx_kl            | 0.0035497725 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.47        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.272        |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 1.51         |
|    value_loss           | 1.62         |
------------------------------------------
Eval num_timesteps=1390000, episode_reward=-400.83 +/- 156.49
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 1390000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 170     |
|    time_elapsed    | 6201    |
|    total_timesteps | 1392640 |
--------------------------------
Eval num_timesteps=1395000, episode_reward=-251.46 +/- 125.52
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -251        |
| time/                   |             |
|    total_timesteps      | 1395000     |
| train/                  |             |
|    approx_kl            | 0.004272959 |
|    clip_fraction        | 0.0139      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.48       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.205       |
|    n_updates            | 1700        |
|    policy_gradient_loss | -0.000933   |
|    std                  | 1.51        |
|    value_loss           | 2.96        |
-----------------------------------------
Eval num_timesteps=1400000, episode_reward=-353.67 +/- 177.83
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 1400000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 171     |
|    time_elapsed    | 6245    |
|    total_timesteps | 1400832 |
--------------------------------
Eval num_timesteps=1405000, episode_reward=-317.02 +/- 76.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -317         |
| time/                   |              |
|    total_timesteps      | 1405000      |
| train/                  |              |
|    approx_kl            | 0.0027767178 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.49        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0199       |
|    n_updates            | 1710         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 1.51         |
|    value_loss           | 50.1         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 172     |
|    time_elapsed    | 6283    |
|    total_timesteps | 1409024 |
--------------------------------
Eval num_timesteps=1410000, episode_reward=-403.04 +/- 141.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -403         |
| time/                   |              |
|    total_timesteps      | 1410000      |
| train/                  |              |
|    approx_kl            | 0.0013108721 |
|    clip_fraction        | 0.00131      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.49        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.88         |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.000307    |
|    std                  | 1.52         |
|    value_loss           | 76.6         |
------------------------------------------
Eval num_timesteps=1415000, episode_reward=-368.80 +/- 126.67
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 1415000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 173     |
|    time_elapsed    | 6327    |
|    total_timesteps | 1417216 |
--------------------------------
Eval num_timesteps=1420000, episode_reward=-287.02 +/- 77.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -287         |
| time/                   |              |
|    total_timesteps      | 1420000      |
| train/                  |              |
|    approx_kl            | 0.0026827126 |
|    clip_fraction        | 0.00598      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.358        |
|    n_updates            | 1730         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 1.53         |
|    value_loss           | 133          |
------------------------------------------
Eval num_timesteps=1425000, episode_reward=-362.14 +/- 186.57
Episode length: 812.60 +/- 376.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | -362     |
| time/              |          |
|    total_timesteps | 1425000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 174     |
|    time_elapsed    | 6368    |
|    total_timesteps | 1425408 |
--------------------------------
Eval num_timesteps=1430000, episode_reward=-446.03 +/- 152.16
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -446        |
| time/                   |             |
|    total_timesteps      | 1430000     |
| train/                  |             |
|    approx_kl            | 0.004999984 |
|    clip_fraction        | 0.0288      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.51       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0341      |
|    n_updates            | 1740        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 1.53        |
|    value_loss           | 1.16        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 175     |
|    time_elapsed    | 6405    |
|    total_timesteps | 1433600 |
--------------------------------
Eval num_timesteps=1435000, episode_reward=-348.96 +/- 148.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -349         |
| time/                   |              |
|    total_timesteps      | 1435000      |
| train/                  |              |
|    approx_kl            | 0.0034936974 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0588       |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.52         |
|    value_loss           | 1.05         |
------------------------------------------
Eval num_timesteps=1440000, episode_reward=-353.06 +/- 55.48
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -353     |
| time/              |          |
|    total_timesteps | 1440000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 176     |
|    time_elapsed    | 6449    |
|    total_timesteps | 1441792 |
--------------------------------
Eval num_timesteps=1445000, episode_reward=-314.98 +/- 157.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -315         |
| time/                   |              |
|    total_timesteps      | 1445000      |
| train/                  |              |
|    approx_kl            | 0.0040878598 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.52        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.124        |
|    n_updates            | 1760         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 1.53         |
|    value_loss           | 4.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 177     |
|    time_elapsed    | 6485    |
|    total_timesteps | 1449984 |
--------------------------------
Eval num_timesteps=1450000, episode_reward=-424.04 +/- 50.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -424         |
| time/                   |              |
|    total_timesteps      | 1450000      |
| train/                  |              |
|    approx_kl            | 0.0045030406 |
|    clip_fraction        | 0.0182       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.53        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.474        |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 1.54         |
|    value_loss           | 34           |
------------------------------------------
Eval num_timesteps=1455000, episode_reward=-293.46 +/- 70.44
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 1455000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 178     |
|    time_elapsed    | 6529    |
|    total_timesteps | 1458176 |
--------------------------------
Eval num_timesteps=1460000, episode_reward=-307.37 +/- 139.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -307         |
| time/                   |              |
|    total_timesteps      | 1460000      |
| train/                  |              |
|    approx_kl            | 0.0043035257 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.55        |
|    explained_variance   | 0.98         |
|    learning_rate        | 5e-05        |
|    loss                 | 0.271        |
|    n_updates            | 1780         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 1.54         |
|    value_loss           | 146          |
------------------------------------------
Eval num_timesteps=1465000, episode_reward=-357.96 +/- 160.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -358     |
| time/              |          |
|    total_timesteps | 1465000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 179     |
|    time_elapsed    | 6574    |
|    total_timesteps | 1466368 |
--------------------------------
Eval num_timesteps=1470000, episode_reward=-442.32 +/- 146.50
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -442        |
| time/                   |             |
|    total_timesteps      | 1470000     |
| train/                  |             |
|    approx_kl            | 0.002957854 |
|    clip_fraction        | 0.0101      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 17.6        |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.00173    |
|    std                  | 1.55        |
|    value_loss           | 7.31        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 180     |
|    time_elapsed    | 6610    |
|    total_timesteps | 1474560 |
--------------------------------
Eval num_timesteps=1475000, episode_reward=-320.44 +/- 58.77
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -320         |
| time/                   |              |
|    total_timesteps      | 1475000      |
| train/                  |              |
|    approx_kl            | 0.0048819715 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.57        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0912       |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00296     |
|    std                  | 1.56         |
|    value_loss           | 78.3         |
------------------------------------------
Eval num_timesteps=1480000, episode_reward=-264.13 +/- 114.55
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 1480000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 181     |
|    time_elapsed    | 6654    |
|    total_timesteps | 1482752 |
--------------------------------
Eval num_timesteps=1485000, episode_reward=-292.51 +/- 103.54
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -293         |
| time/                   |              |
|    total_timesteps      | 1485000      |
| train/                  |              |
|    approx_kl            | 0.0015403084 |
|    clip_fraction        | 0.000232     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 0.986        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.49         |
|    n_updates            | 1810         |
|    policy_gradient_loss | -0.000391    |
|    std                  | 1.56         |
|    value_loss           | 109          |
------------------------------------------
Eval num_timesteps=1490000, episode_reward=-410.75 +/- 139.08
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 1490000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 182     |
|    time_elapsed    | 6697    |
|    total_timesteps | 1490944 |
--------------------------------
Eval num_timesteps=1495000, episode_reward=-288.47 +/- 186.61
Episode length: 820.20 +/- 361.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 820         |
|    mean_reward          | -288        |
| time/                   |             |
|    total_timesteps      | 1495000     |
| train/                  |             |
|    approx_kl            | 0.004665903 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.58       |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 2.69        |
|    n_updates            | 1820        |
|    policy_gradient_loss | -0.00203    |
|    std                  | 1.56        |
|    value_loss           | 1.01        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 183     |
|    time_elapsed    | 6734    |
|    total_timesteps | 1499136 |
--------------------------------
Eval num_timesteps=1500000, episode_reward=-225.24 +/- 54.07
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -225         |
| time/                   |              |
|    total_timesteps      | 1500000      |
| train/                  |              |
|    approx_kl            | 0.0034745391 |
|    clip_fraction        | 0.00636      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.58        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0778       |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.000517    |
|    std                  | 1.56         |
|    value_loss           | 0.378        |
------------------------------------------
Eval num_timesteps=1505000, episode_reward=-445.99 +/- 125.16
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -446     |
| time/              |          |
|    total_timesteps | 1505000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 184     |
|    time_elapsed    | 6778    |
|    total_timesteps | 1507328 |
--------------------------------
Eval num_timesteps=1510000, episode_reward=-367.92 +/- 169.89
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -368          |
| time/                   |               |
|    total_timesteps      | 1510000       |
| train/                  |               |
|    approx_kl            | 0.00020536662 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.59         |
|    explained_variance   | 0.993         |
|    learning_rate        | 5e-05         |
|    loss                 | 7.99          |
|    n_updates            | 1840          |
|    policy_gradient_loss | -0.000161     |
|    std                  | 1.56          |
|    value_loss           | 84.1          |
-------------------------------------------
Eval num_timesteps=1515000, episode_reward=-265.33 +/- 79.90
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 1515000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 185     |
|    time_elapsed    | 6822    |
|    total_timesteps | 1515520 |
--------------------------------
Eval num_timesteps=1520000, episode_reward=-470.16 +/- 170.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -470         |
| time/                   |              |
|    total_timesteps      | 1520000      |
| train/                  |              |
|    approx_kl            | 0.0049399636 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.59        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.383        |
|    n_updates            | 1850         |
|    policy_gradient_loss | -0.0023      |
|    std                  | 1.57         |
|    value_loss           | 79.8         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 186     |
|    time_elapsed    | 6858    |
|    total_timesteps | 1523712 |
--------------------------------
Eval num_timesteps=1525000, episode_reward=-294.36 +/- 172.56
Episode length: 806.40 +/- 389.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 806         |
|    mean_reward          | -294        |
| time/                   |             |
|    total_timesteps      | 1525000     |
| train/                  |             |
|    approx_kl            | 0.004302979 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.6        |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0923      |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00194    |
|    std                  | 1.57        |
|    value_loss           | 98.5        |
-----------------------------------------
Eval num_timesteps=1530000, episode_reward=-355.64 +/- 93.00
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -356     |
| time/              |          |
|    total_timesteps | 1530000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 187     |
|    time_elapsed    | 6901    |
|    total_timesteps | 1531904 |
--------------------------------
Eval num_timesteps=1535000, episode_reward=-310.65 +/- 93.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -311         |
| time/                   |              |
|    total_timesteps      | 1535000      |
| train/                  |              |
|    approx_kl            | 0.0046197698 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.6         |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 0.161        |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 1.58         |
|    value_loss           | 3.83         |
------------------------------------------
Eval num_timesteps=1540000, episode_reward=-422.20 +/- 180.24
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 1540000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 188     |
|    time_elapsed    | 6944    |
|    total_timesteps | 1540096 |
--------------------------------
Eval num_timesteps=1545000, episode_reward=-340.49 +/- 92.93
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -340         |
| time/                   |              |
|    total_timesteps      | 1545000      |
| train/                  |              |
|    approx_kl            | 0.0031352858 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.61        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.164        |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00171     |
|    std                  | 1.57         |
|    value_loss           | 68.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 189     |
|    time_elapsed    | 6980    |
|    total_timesteps | 1548288 |
--------------------------------
Eval num_timesteps=1550000, episode_reward=-248.37 +/- 111.93
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -248        |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.003690774 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.61       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0782      |
|    n_updates            | 1890        |
|    policy_gradient_loss | -0.00263    |
|    std                  | 1.58        |
|    value_loss           | 1.99        |
-----------------------------------------
Eval num_timesteps=1555000, episode_reward=-387.65 +/- 150.44
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 1555000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 190     |
|    time_elapsed    | 7024    |
|    total_timesteps | 1556480 |
--------------------------------
Eval num_timesteps=1560000, episode_reward=-477.26 +/- 147.46
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -477        |
| time/                   |             |
|    total_timesteps      | 1560000     |
| train/                  |             |
|    approx_kl            | 0.002783733 |
|    clip_fraction        | 0.00237     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.62       |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.65        |
|    n_updates            | 1900        |
|    policy_gradient_loss | -0.000884   |
|    std                  | 1.58        |
|    value_loss           | 7.53        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 191     |
|    time_elapsed    | 7061    |
|    total_timesteps | 1564672 |
--------------------------------
Eval num_timesteps=1565000, episode_reward=-372.26 +/- 108.75
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -372        |
| time/                   |             |
|    total_timesteps      | 1565000     |
| train/                  |             |
|    approx_kl            | 0.003009982 |
|    clip_fraction        | 0.00348     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.129       |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.001      |
|    std                  | 1.59        |
|    value_loss           | 69.2        |
-----------------------------------------
Eval num_timesteps=1570000, episode_reward=-474.85 +/- 135.96
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -475     |
| time/              |          |
|    total_timesteps | 1570000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 192     |
|    time_elapsed    | 7104    |
|    total_timesteps | 1572864 |
--------------------------------
Eval num_timesteps=1575000, episode_reward=-292.83 +/- 107.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -293         |
| time/                   |              |
|    total_timesteps      | 1575000      |
| train/                  |              |
|    approx_kl            | 0.0019408348 |
|    clip_fraction        | 0.00428      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.64        |
|    explained_variance   | 0.983        |
|    learning_rate        | 5e-05        |
|    loss                 | 30.5         |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.000824    |
|    std                  | 1.59         |
|    value_loss           | 176          |
------------------------------------------
Eval num_timesteps=1580000, episode_reward=-369.33 +/- 116.98
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 1580000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 193     |
|    time_elapsed    | 7149    |
|    total_timesteps | 1581056 |
--------------------------------
Eval num_timesteps=1585000, episode_reward=-428.26 +/- 77.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -428        |
| time/                   |             |
|    total_timesteps      | 1585000     |
| train/                  |             |
|    approx_kl            | 0.001478285 |
|    clip_fraction        | 0.000647    |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.65       |
|    explained_variance   | 0.991       |
|    learning_rate        | 5e-05       |
|    loss                 | 8.44        |
|    n_updates            | 1930        |
|    policy_gradient_loss | -0.000222   |
|    std                  | 1.6         |
|    value_loss           | 134         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 194     |
|    time_elapsed    | 7186    |
|    total_timesteps | 1589248 |
--------------------------------
Eval num_timesteps=1590000, episode_reward=-317.24 +/- 157.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -317         |
| time/                   |              |
|    total_timesteps      | 1590000      |
| train/                  |              |
|    approx_kl            | 0.0038852529 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.107        |
|    n_updates            | 1940         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 1.6          |
|    value_loss           | 101          |
------------------------------------------
Eval num_timesteps=1595000, episode_reward=-374.23 +/- 181.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 1595000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 195     |
|    time_elapsed    | 7230    |
|    total_timesteps | 1597440 |
--------------------------------
Eval num_timesteps=1600000, episode_reward=-300.72 +/- 92.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -301         |
| time/                   |              |
|    total_timesteps      | 1600000      |
| train/                  |              |
|    approx_kl            | 0.0019756919 |
|    clip_fraction        | 0.00325      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.66        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 35.1         |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.000547    |
|    std                  | 1.61         |
|    value_loss           | 146          |
------------------------------------------
Eval num_timesteps=1605000, episode_reward=-391.97 +/- 158.28
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 1605000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 196     |
|    time_elapsed    | 7275    |
|    total_timesteps | 1605632 |
--------------------------------
Eval num_timesteps=1610000, episode_reward=-191.20 +/- 94.43
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -191        |
| time/                   |             |
|    total_timesteps      | 1610000     |
| train/                  |             |
|    approx_kl            | 0.004791538 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.68       |
|    explained_variance   | 0.996       |
|    learning_rate        | 5e-05       |
|    loss                 | 37.3        |
|    n_updates            | 1960        |
|    policy_gradient_loss | -0.00215    |
|    std                  | 1.61        |
|    value_loss           | 61.4        |
-----------------------------------------
New best mean reward!
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 197     |
|    time_elapsed    | 7311    |
|    total_timesteps | 1613824 |
--------------------------------
Eval num_timesteps=1615000, episode_reward=-313.54 +/- 71.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -314         |
| time/                   |              |
|    total_timesteps      | 1615000      |
| train/                  |              |
|    approx_kl            | 0.0047951043 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0905       |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00224     |
|    std                  | 1.62         |
|    value_loss           | 0.778        |
------------------------------------------
Eval num_timesteps=1620000, episode_reward=-320.23 +/- 226.12
Episode length: 809.20 +/- 383.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | -320     |
| time/              |          |
|    total_timesteps | 1620000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 198     |
|    time_elapsed    | 7355    |
|    total_timesteps | 1622016 |
--------------------------------
Eval num_timesteps=1625000, episode_reward=-288.21 +/- 140.21
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -288        |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.001961668 |
|    clip_fraction        | 0.00432     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.7        |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.528       |
|    n_updates            | 1980        |
|    policy_gradient_loss | -0.000556   |
|    std                  | 1.62        |
|    value_loss           | 7.8         |
-----------------------------------------
Eval num_timesteps=1630000, episode_reward=-405.91 +/- 134.10
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 1630000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 199     |
|    time_elapsed    | 7399    |
|    total_timesteps | 1630208 |
--------------------------------
Eval num_timesteps=1635000, episode_reward=-311.66 +/- 72.34
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -312         |
| time/                   |              |
|    total_timesteps      | 1635000      |
| train/                  |              |
|    approx_kl            | 0.0035384607 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.7         |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.37         |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 1.62         |
|    value_loss           | 78.5         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 200     |
|    time_elapsed    | 7436    |
|    total_timesteps | 1638400 |
--------------------------------
Eval num_timesteps=1640000, episode_reward=-314.00 +/- 162.71
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -314        |
| time/                   |             |
|    total_timesteps      | 1640000     |
| train/                  |             |
|    approx_kl            | 0.004356616 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.72       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.293       |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00266    |
|    std                  | 1.64        |
|    value_loss           | 1           |
-----------------------------------------
Eval num_timesteps=1645000, episode_reward=-365.12 +/- 136.61
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -365     |
| time/              |          |
|    total_timesteps | 1645000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 201     |
|    time_elapsed    | 7480    |
|    total_timesteps | 1646592 |
--------------------------------
Eval num_timesteps=1650000, episode_reward=-350.82 +/- 80.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -351         |
| time/                   |              |
|    total_timesteps      | 1650000      |
| train/                  |              |
|    approx_kl            | 0.0006700085 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.74        |
|    explained_variance   | 0.983        |
|    learning_rate        | 5e-05        |
|    loss                 | 512          |
|    n_updates            | 2010         |
|    policy_gradient_loss | -0.000178    |
|    std                  | 1.64         |
|    value_loss           | 185          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 202     |
|    time_elapsed    | 7517    |
|    total_timesteps | 1654784 |
--------------------------------
Eval num_timesteps=1655000, episode_reward=-299.24 +/- 76.93
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -299        |
| time/                   |             |
|    total_timesteps      | 1655000     |
| train/                  |             |
|    approx_kl            | 0.001399424 |
|    clip_fraction        | 0.00303     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.74       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.292       |
|    n_updates            | 2020        |
|    policy_gradient_loss | -0.00011    |
|    std                  | 1.65        |
|    value_loss           | 111         |
-----------------------------------------
Eval num_timesteps=1660000, episode_reward=-364.43 +/- 114.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 1660000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 203     |
|    time_elapsed    | 7561    |
|    total_timesteps | 1662976 |
--------------------------------
Eval num_timesteps=1665000, episode_reward=-277.16 +/- 113.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -277        |
| time/                   |             |
|    total_timesteps      | 1665000     |
| train/                  |             |
|    approx_kl            | 0.004484241 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.75       |
|    explained_variance   | 0.979       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.553       |
|    n_updates            | 2030        |
|    policy_gradient_loss | -0.00155    |
|    std                  | 1.65        |
|    value_loss           | 8.41        |
-----------------------------------------
Eval num_timesteps=1670000, episode_reward=-377.56 +/- 63.23
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 1670000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 204     |
|    time_elapsed    | 7606    |
|    total_timesteps | 1671168 |
--------------------------------
Eval num_timesteps=1675000, episode_reward=-305.74 +/- 113.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -306         |
| time/                   |              |
|    total_timesteps      | 1675000      |
| train/                  |              |
|    approx_kl            | 0.0041316124 |
|    clip_fraction        | 0.0255       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.77        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.362        |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.00248     |
|    std                  | 1.67         |
|    value_loss           | 91.5         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 205     |
|    time_elapsed    | 7642    |
|    total_timesteps | 1679360 |
--------------------------------
Eval num_timesteps=1680000, episode_reward=-427.11 +/- 180.26
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -427         |
| time/                   |              |
|    total_timesteps      | 1680000      |
| train/                  |              |
|    approx_kl            | 0.0026573106 |
|    clip_fraction        | 0.00492      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.79        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.643        |
|    n_updates            | 2050         |
|    policy_gradient_loss | -0.00059     |
|    std                  | 1.67         |
|    value_loss           | 60.7         |
------------------------------------------
Eval num_timesteps=1685000, episode_reward=-325.34 +/- 178.90
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -325     |
| time/              |          |
|    total_timesteps | 1685000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 206     |
|    time_elapsed    | 7686    |
|    total_timesteps | 1687552 |
--------------------------------
Eval num_timesteps=1690000, episode_reward=-350.56 +/- 99.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -351         |
| time/                   |              |
|    total_timesteps      | 1690000      |
| train/                  |              |
|    approx_kl            | 0.0051070056 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.79        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.422        |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.0017      |
|    std                  | 1.68         |
|    value_loss           | 29.1         |
------------------------------------------
Eval num_timesteps=1695000, episode_reward=-400.43 +/- 150.12
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 1695000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 207     |
|    time_elapsed    | 7731    |
|    total_timesteps | 1695744 |
--------------------------------
Eval num_timesteps=1700000, episode_reward=-424.64 +/- 99.53
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -425        |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.004343152 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.81       |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.141       |
|    n_updates            | 2070        |
|    policy_gradient_loss | -0.00223    |
|    std                  | 1.69        |
|    value_loss           | 7.26        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 208     |
|    time_elapsed    | 7768    |
|    total_timesteps | 1703936 |
--------------------------------
Eval num_timesteps=1705000, episode_reward=-361.42 +/- 108.29
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -361          |
| time/                   |               |
|    total_timesteps      | 1705000       |
| train/                  |               |
|    approx_kl            | 0.00010733705 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.82         |
|    explained_variance   | 0.987         |
|    learning_rate        | 5e-05         |
|    loss                 | 59.9          |
|    n_updates            | 2080          |
|    policy_gradient_loss | -0.000111     |
|    std                  | 1.69          |
|    value_loss           | 211           |
-------------------------------------------
Eval num_timesteps=1710000, episode_reward=-471.17 +/- 103.14
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -471     |
| time/              |          |
|    total_timesteps | 1710000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 209     |
|    time_elapsed    | 7811    |
|    total_timesteps | 1712128 |
--------------------------------
Eval num_timesteps=1715000, episode_reward=-467.29 +/- 166.58
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -467         |
| time/                   |              |
|    total_timesteps      | 1715000      |
| train/                  |              |
|    approx_kl            | 0.0038572995 |
|    clip_fraction        | 0.0143       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.83        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.628        |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00232     |
|    std                  | 1.7          |
|    value_loss           | 7.4          |
------------------------------------------
Eval num_timesteps=1720000, episode_reward=-246.81 +/- 122.77
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 1720000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 210     |
|    time_elapsed    | 7857    |
|    total_timesteps | 1720320 |
--------------------------------
Eval num_timesteps=1725000, episode_reward=-268.73 +/- 128.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -269         |
| time/                   |              |
|    total_timesteps      | 1725000      |
| train/                  |              |
|    approx_kl            | 0.0021867976 |
|    clip_fraction        | 0.00142      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.83        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 28.4         |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.000666    |
|    std                  | 1.7          |
|    value_loss           | 168          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 211     |
|    time_elapsed    | 7894    |
|    total_timesteps | 1728512 |
--------------------------------
Eval num_timesteps=1730000, episode_reward=-303.88 +/- 143.07
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -304         |
| time/                   |              |
|    total_timesteps      | 1730000      |
| train/                  |              |
|    approx_kl            | 0.0023630061 |
|    clip_fraction        | 0.00928      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.84        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 75.9         |
|    n_updates            | 2110         |
|    policy_gradient_loss | -0.000602    |
|    std                  | 1.7          |
|    value_loss           | 110          |
------------------------------------------
Eval num_timesteps=1735000, episode_reward=-422.58 +/- 165.25
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 1735000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 212     |
|    time_elapsed    | 7938    |
|    total_timesteps | 1736704 |
--------------------------------
Eval num_timesteps=1740000, episode_reward=-316.49 +/- 228.30
Episode length: 809.00 +/- 384.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 809          |
|    mean_reward          | -316         |
| time/                   |              |
|    total_timesteps      | 1740000      |
| train/                  |              |
|    approx_kl            | 0.0018101016 |
|    clip_fraction        | 0.00438      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.84        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 204          |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.000272    |
|    std                  | 1.7          |
|    value_loss           | 113          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 213     |
|    time_elapsed    | 7975    |
|    total_timesteps | 1744896 |
--------------------------------
Eval num_timesteps=1745000, episode_reward=-244.59 +/- 191.57
Episode length: 830.20 +/- 341.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 830          |
|    mean_reward          | -245         |
| time/                   |              |
|    total_timesteps      | 1745000      |
| train/                  |              |
|    approx_kl            | 0.0021553587 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.85        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.277        |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00127     |
|    std                  | 1.7          |
|    value_loss           | 38.4         |
------------------------------------------
Eval num_timesteps=1750000, episode_reward=-328.68 +/- 164.93
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 1750000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 214     |
|    time_elapsed    | 8017    |
|    total_timesteps | 1753088 |
--------------------------------
Eval num_timesteps=1755000, episode_reward=-318.68 +/- 118.83
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 1755000      |
| train/                  |              |
|    approx_kl            | 0.0062198956 |
|    clip_fraction        | 0.0323       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.86        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.189        |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00321     |
|    std                  | 1.72         |
|    value_loss           | 114          |
------------------------------------------
Eval num_timesteps=1760000, episode_reward=-345.79 +/- 156.84
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -346     |
| time/              |          |
|    total_timesteps | 1760000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 215     |
|    time_elapsed    | 8061    |
|    total_timesteps | 1761280 |
--------------------------------
Eval num_timesteps=1765000, episode_reward=-335.47 +/- 147.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -335         |
| time/                   |              |
|    total_timesteps      | 1765000      |
| train/                  |              |
|    approx_kl            | 0.0033358275 |
|    clip_fraction        | 0.00751      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.88        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 1.45         |
|    n_updates            | 2150         |
|    policy_gradient_loss | -0.00095     |
|    std                  | 1.72         |
|    value_loss           | 189          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 216     |
|    time_elapsed    | 8099    |
|    total_timesteps | 1769472 |
--------------------------------
Eval num_timesteps=1770000, episode_reward=-395.82 +/- 104.69
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -396         |
| time/                   |              |
|    total_timesteps      | 1770000      |
| train/                  |              |
|    approx_kl            | 0.0042682136 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.87        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.34         |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 1.72         |
|    value_loss           | 1.32         |
------------------------------------------
Eval num_timesteps=1775000, episode_reward=-321.02 +/- 151.62
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -321     |
| time/              |          |
|    total_timesteps | 1775000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 217     |
|    time_elapsed    | 8142    |
|    total_timesteps | 1777664 |
--------------------------------
Eval num_timesteps=1780000, episode_reward=-276.39 +/- 138.53
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -276        |
| time/                   |             |
|    total_timesteps      | 1780000     |
| train/                  |             |
|    approx_kl            | 0.004477677 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.87       |
|    explained_variance   | 0.99        |
|    learning_rate        | 5e-05       |
|    loss                 | 0.884       |
|    n_updates            | 2170        |
|    policy_gradient_loss | -0.00151    |
|    std                  | 1.72        |
|    value_loss           | 81.9        |
-----------------------------------------
Eval num_timesteps=1785000, episode_reward=-394.28 +/- 144.14
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 1785000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 218     |
|    time_elapsed    | 8186    |
|    total_timesteps | 1785856 |
--------------------------------
Eval num_timesteps=1790000, episode_reward=-298.11 +/- 159.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -298        |
| time/                   |             |
|    total_timesteps      | 1790000     |
| train/                  |             |
|    approx_kl            | 0.004644273 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.88       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.169       |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 1.72        |
|    value_loss           | 1.55        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 219     |
|    time_elapsed    | 8224    |
|    total_timesteps | 1794048 |
--------------------------------
Eval num_timesteps=1795000, episode_reward=-292.23 +/- 198.74
Episode length: 807.40 +/- 387.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 807          |
|    mean_reward          | -292         |
| time/                   |              |
|    total_timesteps      | 1795000      |
| train/                  |              |
|    approx_kl            | 0.0036404384 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.88        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.669        |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 1.73         |
|    value_loss           | 5.03         |
------------------------------------------
Eval num_timesteps=1800000, episode_reward=-390.78 +/- 116.05
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 1800000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 220     |
|    time_elapsed    | 8267    |
|    total_timesteps | 1802240 |
--------------------------------
Eval num_timesteps=1805000, episode_reward=-247.86 +/- 98.79
Episode length: 952.60 +/- 96.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 953          |
|    mean_reward          | -248         |
| time/                   |              |
|    total_timesteps      | 1805000      |
| train/                  |              |
|    approx_kl            | 0.0019239711 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.89        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.362        |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.000135    |
|    std                  | 1.73         |
|    value_loss           | 6.43         |
------------------------------------------
Eval num_timesteps=1810000, episode_reward=-225.36 +/- 47.77
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 1810000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 221     |
|    time_elapsed    | 8311    |
|    total_timesteps | 1810432 |
--------------------------------
Eval num_timesteps=1815000, episode_reward=-446.09 +/- 157.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -446        |
| time/                   |             |
|    total_timesteps      | 1815000     |
| train/                  |             |
|    approx_kl            | 0.003690058 |
|    clip_fraction        | 0.00833     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.89       |
|    explained_variance   | 0.988       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.48        |
|    n_updates            | 2210        |
|    policy_gradient_loss | -0.00172    |
|    std                  | 1.73        |
|    value_loss           | 195         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 222     |
|    time_elapsed    | 8349    |
|    total_timesteps | 1818624 |
--------------------------------
Eval num_timesteps=1820000, episode_reward=-542.31 +/- 134.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -542         |
| time/                   |              |
|    total_timesteps      | 1820000      |
| train/                  |              |
|    approx_kl            | 0.0028878145 |
|    clip_fraction        | 0.00477      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.89        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.31         |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.000494    |
|    std                  | 1.73         |
|    value_loss           | 80           |
------------------------------------------
Eval num_timesteps=1825000, episode_reward=-395.87 +/- 123.23
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 1825000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 223     |
|    time_elapsed    | 8394    |
|    total_timesteps | 1826816 |
--------------------------------
Eval num_timesteps=1830000, episode_reward=-326.95 +/- 172.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -327         |
| time/                   |              |
|    total_timesteps      | 1830000      |
| train/                  |              |
|    approx_kl            | 0.0035072523 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.9         |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.34         |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.000674    |
|    std                  | 1.73         |
|    value_loss           | 37.7         |
------------------------------------------
Eval num_timesteps=1835000, episode_reward=-313.94 +/- 166.23
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -314     |
| time/              |          |
|    total_timesteps | 1835000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 224     |
|    time_elapsed    | 8439    |
|    total_timesteps | 1835008 |
--------------------------------
Eval num_timesteps=1840000, episode_reward=-287.93 +/- 133.01
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -288         |
| time/                   |              |
|    total_timesteps      | 1840000      |
| train/                  |              |
|    approx_kl            | 0.0025414196 |
|    clip_fraction        | 0.00514      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.9         |
|    explained_variance   | 0.978        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.4          |
|    n_updates            | 2240         |
|    policy_gradient_loss | -0.000407    |
|    std                  | 1.74         |
|    value_loss           | 321          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 225     |
|    time_elapsed    | 8475    |
|    total_timesteps | 1843200 |
--------------------------------
Eval num_timesteps=1845000, episode_reward=-329.44 +/- 145.42
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -329        |
| time/                   |             |
|    total_timesteps      | 1845000     |
| train/                  |             |
|    approx_kl            | 0.004686311 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.92       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.213       |
|    n_updates            | 2250        |
|    policy_gradient_loss | -0.00311    |
|    std                  | 1.75        |
|    value_loss           | 0.91        |
-----------------------------------------
Eval num_timesteps=1850000, episode_reward=-348.79 +/- 150.80
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -349     |
| time/              |          |
|    total_timesteps | 1850000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 226     |
|    time_elapsed    | 8519    |
|    total_timesteps | 1851392 |
--------------------------------
Eval num_timesteps=1855000, episode_reward=-214.13 +/- 164.23
Episode length: 804.00 +/- 394.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 804          |
|    mean_reward          | -214         |
| time/                   |              |
|    total_timesteps      | 1855000      |
| train/                  |              |
|    approx_kl            | 0.0021743488 |
|    clip_fraction        | 0.00596      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 0.982        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.35         |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 1.75         |
|    value_loss           | 305          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 227     |
|    time_elapsed    | 8555    |
|    total_timesteps | 1859584 |
--------------------------------
Eval num_timesteps=1860000, episode_reward=-291.45 +/- 81.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -291         |
| time/                   |              |
|    total_timesteps      | 1860000      |
| train/                  |              |
|    approx_kl            | 0.0042681852 |
|    clip_fraction        | 0.0155       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.677        |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 1.75         |
|    value_loss           | 117          |
------------------------------------------
Eval num_timesteps=1865000, episode_reward=-245.66 +/- 122.80
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -246     |
| time/              |          |
|    total_timesteps | 1865000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 228     |
|    time_elapsed    | 8599    |
|    total_timesteps | 1867776 |
--------------------------------
Eval num_timesteps=1870000, episode_reward=-347.09 +/- 134.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -347         |
| time/                   |              |
|    total_timesteps      | 1870000      |
| train/                  |              |
|    approx_kl            | 0.0025705174 |
|    clip_fraction        | 0.00481      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0455       |
|    n_updates            | 2280         |
|    policy_gradient_loss | -0.000248    |
|    std                  | 1.74         |
|    value_loss           | 42.2         |
------------------------------------------
Eval num_timesteps=1875000, episode_reward=-429.24 +/- 109.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 1875000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 229     |
|    time_elapsed    | 8643    |
|    total_timesteps | 1875968 |
--------------------------------
Eval num_timesteps=1880000, episode_reward=-344.79 +/- 116.40
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -345         |
| time/                   |              |
|    total_timesteps      | 1880000      |
| train/                  |              |
|    approx_kl            | 0.0035175178 |
|    clip_fraction        | 0.00502      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 71           |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.74         |
|    value_loss           | 124          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 230     |
|    time_elapsed    | 8681    |
|    total_timesteps | 1884160 |
--------------------------------
Eval num_timesteps=1885000, episode_reward=-461.16 +/- 136.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -461         |
| time/                   |              |
|    total_timesteps      | 1885000      |
| train/                  |              |
|    approx_kl            | 0.0028107762 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.626        |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 1.74         |
|    value_loss           | 22.8         |
------------------------------------------
Eval num_timesteps=1890000, episode_reward=-348.14 +/- 98.78
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -348     |
| time/              |          |
|    total_timesteps | 1890000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 231     |
|    time_elapsed    | 8725    |
|    total_timesteps | 1892352 |
--------------------------------
Eval num_timesteps=1895000, episode_reward=-309.51 +/- 139.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -310         |
| time/                   |              |
|    total_timesteps      | 1895000      |
| train/                  |              |
|    approx_kl            | 0.0038715722 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.92        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.00245      |
|    n_updates            | 2310         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 1.75         |
|    value_loss           | 1.42         |
------------------------------------------
Eval num_timesteps=1900000, episode_reward=-490.02 +/- 138.35
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -490     |
| time/              |          |
|    total_timesteps | 1900000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 232     |
|    time_elapsed    | 8769    |
|    total_timesteps | 1900544 |
--------------------------------
Eval num_timesteps=1905000, episode_reward=-293.49 +/- 168.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -293         |
| time/                   |              |
|    total_timesteps      | 1905000      |
| train/                  |              |
|    approx_kl            | 0.0032263792 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.013        |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 1.75         |
|    value_loss           | 0.407        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 233     |
|    time_elapsed    | 8808    |
|    total_timesteps | 1908736 |
--------------------------------
Eval num_timesteps=1910000, episode_reward=-382.41 +/- 149.40
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -382         |
| time/                   |              |
|    total_timesteps      | 1910000      |
| train/                  |              |
|    approx_kl            | 0.0047368463 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.105        |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.00254     |
|    std                  | 1.76         |
|    value_loss           | 2.25         |
------------------------------------------
Eval num_timesteps=1915000, episode_reward=-334.52 +/- 184.83
Episode length: 818.40 +/- 365.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 818      |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 1915000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 234     |
|    time_elapsed    | 8850    |
|    total_timesteps | 1916928 |
--------------------------------
Eval num_timesteps=1920000, episode_reward=-304.66 +/- 130.64
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -305        |
| time/                   |             |
|    total_timesteps      | 1920000     |
| train/                  |             |
|    approx_kl            | 0.002255788 |
|    clip_fraction        | 0.00222     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 0.99        |
|    learning_rate        | 5e-05       |
|    loss                 | 0.41        |
|    n_updates            | 2340        |
|    policy_gradient_loss | -0.000756   |
|    std                  | 1.75        |
|    value_loss           | 38.8        |
-----------------------------------------
Eval num_timesteps=1925000, episode_reward=-274.47 +/- 46.81
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 1925000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 235     |
|    time_elapsed    | 8894    |
|    total_timesteps | 1925120 |
--------------------------------
Eval num_timesteps=1930000, episode_reward=-277.37 +/- 125.70
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -277         |
| time/                   |              |
|    total_timesteps      | 1930000      |
| train/                  |              |
|    approx_kl            | 0.0016608661 |
|    clip_fraction        | 0.000671     |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.93        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.605        |
|    n_updates            | 2350         |
|    policy_gradient_loss | -0.00029     |
|    std                  | 1.75         |
|    value_loss           | 206          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 236     |
|    time_elapsed    | 8930    |
|    total_timesteps | 1933312 |
--------------------------------
Eval num_timesteps=1935000, episode_reward=-369.93 +/- 179.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -370         |
| time/                   |              |
|    total_timesteps      | 1935000      |
| train/                  |              |
|    approx_kl            | 0.0036756154 |
|    clip_fraction        | 0.00885      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | -0.00384     |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.00121     |
|    std                  | 1.76         |
|    value_loss           | 3.26         |
------------------------------------------
Eval num_timesteps=1940000, episode_reward=-204.27 +/- 40.39
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 1940000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 237     |
|    time_elapsed    | 8975    |
|    total_timesteps | 1941504 |
--------------------------------
Eval num_timesteps=1945000, episode_reward=-320.34 +/- 207.63
Episode length: 815.00 +/- 372.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -320         |
| time/                   |              |
|    total_timesteps      | 1945000      |
| train/                  |              |
|    approx_kl            | 0.0018798679 |
|    clip_fraction        | 0.00144      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.94        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.223        |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.000557    |
|    std                  | 1.76         |
|    value_loss           | 99.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 238     |
|    time_elapsed    | 9010    |
|    total_timesteps | 1949696 |
--------------------------------
Eval num_timesteps=1950000, episode_reward=-367.13 +/- 164.37
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -367        |
| time/                   |             |
|    total_timesteps      | 1950000     |
| train/                  |             |
|    approx_kl            | 0.004889682 |
|    clip_fraction        | 0.0397      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.94       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | -0.0399     |
|    n_updates            | 2380        |
|    policy_gradient_loss | -0.00401    |
|    std                  | 1.76        |
|    value_loss           | 0.598       |
-----------------------------------------
Eval num_timesteps=1955000, episode_reward=-134.09 +/- 80.46
Episode length: 809.20 +/- 383.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 1955000  |
---------------------------------
New best mean reward!
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 239     |
|    time_elapsed    | 9054    |
|    total_timesteps | 1957888 |
--------------------------------
Eval num_timesteps=1960000, episode_reward=-485.18 +/- 102.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -485         |
| time/                   |              |
|    total_timesteps      | 1960000      |
| train/                  |              |
|    approx_kl            | 0.0037209159 |
|    clip_fraction        | 0.00564      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.95        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 8.81         |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 1.76         |
|    value_loss           | 6.26         |
------------------------------------------
Eval num_timesteps=1965000, episode_reward=-371.33 +/- 156.31
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 1965000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 240     |
|    time_elapsed    | 9098    |
|    total_timesteps | 1966080 |
--------------------------------
Eval num_timesteps=1970000, episode_reward=-285.40 +/- 43.68
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -285        |
| time/                   |             |
|    total_timesteps      | 1970000     |
| train/                  |             |
|    approx_kl            | 0.004118578 |
|    clip_fraction        | 0.00883     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.95       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.38        |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.0009     |
|    std                  | 1.76        |
|    value_loss           | 43.1        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 241     |
|    time_elapsed    | 9134    |
|    total_timesteps | 1974272 |
--------------------------------
Eval num_timesteps=1975000, episode_reward=-324.97 +/- 147.34
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -325        |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.004330543 |
|    clip_fraction        | 0.00948     |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.95       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 221         |
|    n_updates            | 2410        |
|    policy_gradient_loss | -0.000686   |
|    std                  | 1.76        |
|    value_loss           | 48.4        |
-----------------------------------------
Eval num_timesteps=1980000, episode_reward=-276.36 +/- 137.90
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 1980000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 242     |
|    time_elapsed    | 9180    |
|    total_timesteps | 1982464 |
--------------------------------
Eval num_timesteps=1985000, episode_reward=-309.51 +/- 129.13
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -310          |
| time/                   |               |
|    total_timesteps      | 1985000       |
| train/                  |               |
|    approx_kl            | 0.00012088967 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -5.95         |
|    explained_variance   | 0.992         |
|    learning_rate        | 5e-05         |
|    loss                 | 30.2          |
|    n_updates            | 2420          |
|    policy_gradient_loss | -3.55e-05     |
|    std                  | 1.76          |
|    value_loss           | 114           |
-------------------------------------------
Eval num_timesteps=1990000, episode_reward=-313.17 +/- 157.73
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 1990000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 243     |
|    time_elapsed    | 9224    |
|    total_timesteps | 1990656 |
--------------------------------
Eval num_timesteps=1995000, episode_reward=-442.91 +/- 199.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -443         |
| time/                   |              |
|    total_timesteps      | 1995000      |
| train/                  |              |
|    approx_kl            | 0.0024713548 |
|    clip_fraction        | 0.00275      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.95        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.63         |
|    n_updates            | 2430         |
|    policy_gradient_loss | -0.000344    |
|    std                  | 1.76         |
|    value_loss           | 99.8         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 244     |
|    time_elapsed    | 9260    |
|    total_timesteps | 1998848 |
--------------------------------
Eval num_timesteps=2000000, episode_reward=-480.69 +/- 157.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -481         |
| time/                   |              |
|    total_timesteps      | 2000000      |
| train/                  |              |
|    approx_kl            | 0.0044308826 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.96        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 49.8         |
|    n_updates            | 2440         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.77         |
|    value_loss           | 47.6         |
------------------------------------------
Eval num_timesteps=2005000, episode_reward=-222.48 +/- 48.37
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 2005000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 245     |
|    time_elapsed    | 9306    |
|    total_timesteps | 2007040 |
--------------------------------
Eval num_timesteps=2010000, episode_reward=-433.60 +/- 117.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -434         |
| time/                   |              |
|    total_timesteps      | 2010000      |
| train/                  |              |
|    approx_kl            | 0.0038058045 |
|    clip_fraction        | 0.00959      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.97        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0798       |
|    n_updates            | 2450         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 1.78         |
|    value_loss           | 6.65         |
------------------------------------------
Eval num_timesteps=2015000, episode_reward=-415.67 +/- 115.47
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 2015000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 246     |
|    time_elapsed    | 9351    |
|    total_timesteps | 2015232 |
--------------------------------
Eval num_timesteps=2020000, episode_reward=-336.12 +/- 104.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -336         |
| time/                   |              |
|    total_timesteps      | 2020000      |
| train/                  |              |
|    approx_kl            | 0.0030197357 |
|    clip_fraction        | 0.00836      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.97        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 423          |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 1.77         |
|    value_loss           | 113          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 247     |
|    time_elapsed    | 9387    |
|    total_timesteps | 2023424 |
--------------------------------
Eval num_timesteps=2025000, episode_reward=-319.22 +/- 185.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 2025000      |
| train/                  |              |
|    approx_kl            | 0.0058013806 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.97        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0209       |
|    n_updates            | 2470         |
|    policy_gradient_loss | -0.00206     |
|    std                  | 1.77         |
|    value_loss           | 12.3         |
------------------------------------------
Eval num_timesteps=2030000, episode_reward=-379.57 +/- 138.55
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -380     |
| time/              |          |
|    total_timesteps | 2030000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 248     |
|    time_elapsed    | 9432    |
|    total_timesteps | 2031616 |
--------------------------------
Eval num_timesteps=2035000, episode_reward=-463.27 +/- 116.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -463         |
| time/                   |              |
|    total_timesteps      | 2035000      |
| train/                  |              |
|    approx_kl            | 0.0020317114 |
|    clip_fraction        | 0.0011       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.97        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 3.26         |
|    n_updates            | 2480         |
|    policy_gradient_loss | -0.0004      |
|    std                  | 1.77         |
|    value_loss           | 138          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 249     |
|    time_elapsed    | 9469    |
|    total_timesteps | 2039808 |
--------------------------------
Eval num_timesteps=2040000, episode_reward=-350.53 +/- 160.16
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -351        |
| time/                   |             |
|    total_timesteps      | 2040000     |
| train/                  |             |
|    approx_kl            | 0.003171238 |
|    clip_fraction        | 0.0175      |
|    clip_range           | 0.2         |
|    entropy_loss         | -5.97       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0696      |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00221    |
|    std                  | 1.77        |
|    value_loss           | 0.635       |
-----------------------------------------
Eval num_timesteps=2045000, episode_reward=-390.53 +/- 124.68
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 2045000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 250     |
|    time_elapsed    | 9513    |
|    total_timesteps | 2048000 |
--------------------------------
Eval num_timesteps=2050000, episode_reward=-278.85 +/- 97.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -279         |
| time/                   |              |
|    total_timesteps      | 2050000      |
| train/                  |              |
|    approx_kl            | 0.0035476745 |
|    clip_fraction        | 0.0093       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.97        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.194        |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00183     |
|    std                  | 1.77         |
|    value_loss           | 135          |
------------------------------------------
Eval num_timesteps=2055000, episode_reward=-312.47 +/- 59.73
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 2055000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 251     |
|    time_elapsed    | 9558    |
|    total_timesteps | 2056192 |
--------------------------------
Eval num_timesteps=2060000, episode_reward=-390.77 +/- 155.24
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -391         |
| time/                   |              |
|    total_timesteps      | 2060000      |
| train/                  |              |
|    approx_kl            | 0.0036731903 |
|    clip_fraction        | 0.00833      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.97        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 175          |
|    n_updates            | 2510         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 1.77         |
|    value_loss           | 172          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 252     |
|    time_elapsed    | 9594    |
|    total_timesteps | 2064384 |
--------------------------------
Eval num_timesteps=2065000, episode_reward=-591.25 +/- 71.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -591         |
| time/                   |              |
|    total_timesteps      | 2065000      |
| train/                  |              |
|    approx_kl            | 0.0019829657 |
|    clip_fraction        | 0.00267      |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.97        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.316        |
|    n_updates            | 2520         |
|    policy_gradient_loss | 2.44e-05     |
|    std                  | 1.77         |
|    value_loss           | 6.87         |
------------------------------------------
Eval num_timesteps=2070000, episode_reward=-266.38 +/- 94.48
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 2070000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 253     |
|    time_elapsed    | 9639    |
|    total_timesteps | 2072576 |
--------------------------------
Eval num_timesteps=2075000, episode_reward=-358.36 +/- 81.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -358         |
| time/                   |              |
|    total_timesteps      | 2075000      |
| train/                  |              |
|    approx_kl            | 0.0030626312 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.97        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 59.7         |
|    n_updates            | 2530         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 1.78         |
|    value_loss           | 148          |
------------------------------------------
Eval num_timesteps=2080000, episode_reward=-362.37 +/- 109.66
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -362     |
| time/              |          |
|    total_timesteps | 2080000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 254     |
|    time_elapsed    | 9684    |
|    total_timesteps | 2080768 |
--------------------------------
Eval num_timesteps=2085000, episode_reward=-444.90 +/- 161.23
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -445       |
| time/                   |            |
|    total_timesteps      | 2085000    |
| train/                  |            |
|    approx_kl            | 0.00324763 |
|    clip_fraction        | 0.00969    |
|    clip_range           | 0.2        |
|    entropy_loss         | -5.98      |
|    explained_variance   | 0.99       |
|    learning_rate        | 5e-05      |
|    loss                 | 20         |
|    n_updates            | 2540       |
|    policy_gradient_loss | -0.00114   |
|    std                  | 1.78       |
|    value_loss           | 124        |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 255     |
|    time_elapsed    | 9721    |
|    total_timesteps | 2088960 |
--------------------------------
Eval num_timesteps=2090000, episode_reward=-288.84 +/- 107.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -289         |
| time/                   |              |
|    total_timesteps      | 2090000      |
| train/                  |              |
|    approx_kl            | 0.0035419804 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -5.99        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0695       |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.00098     |
|    std                  | 1.79         |
|    value_loss           | 20.6         |
------------------------------------------
Eval num_timesteps=2095000, episode_reward=-328.22 +/- 99.08
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -328     |
| time/              |          |
|    total_timesteps | 2095000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 256     |
|    time_elapsed    | 9765    |
|    total_timesteps | 2097152 |
--------------------------------
Eval num_timesteps=2100000, episode_reward=-347.82 +/- 156.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -348         |
| time/                   |              |
|    total_timesteps      | 2100000      |
| train/                  |              |
|    approx_kl            | 0.0015732687 |
|    clip_fraction        | 0.00132      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6           |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.232        |
|    n_updates            | 2560         |
|    policy_gradient_loss | -0.000332    |
|    std                  | 1.79         |
|    value_loss           | 25.5         |
------------------------------------------
Eval num_timesteps=2105000, episode_reward=-417.90 +/- 161.02
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 2105000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 257     |
|    time_elapsed    | 9810    |
|    total_timesteps | 2105344 |
--------------------------------
Eval num_timesteps=2110000, episode_reward=-213.91 +/- 68.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -214         |
| time/                   |              |
|    total_timesteps      | 2110000      |
| train/                  |              |
|    approx_kl            | 0.0030016592 |
|    clip_fraction        | 0.0087       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.01        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 21.1         |
|    n_updates            | 2570         |
|    policy_gradient_loss | -0.000993    |
|    std                  | 1.8          |
|    value_loss           | 99.3         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 258     |
|    time_elapsed    | 9847    |
|    total_timesteps | 2113536 |
--------------------------------
Eval num_timesteps=2115000, episode_reward=-282.64 +/- 65.66
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -283        |
| time/                   |             |
|    total_timesteps      | 2115000     |
| train/                  |             |
|    approx_kl            | 0.004028552 |
|    clip_fraction        | 0.0245      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.03       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 9.76e-05    |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.002      |
|    std                  | 1.82        |
|    value_loss           | 0.503       |
-----------------------------------------
Eval num_timesteps=2120000, episode_reward=-284.73 +/- 36.74
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 2120000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 259     |
|    time_elapsed    | 9892    |
|    total_timesteps | 2121728 |
--------------------------------
Eval num_timesteps=2125000, episode_reward=-384.57 +/- 184.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -385         |
| time/                   |              |
|    total_timesteps      | 2125000      |
| train/                  |              |
|    approx_kl            | 0.0034064318 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.05        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | -0.0413      |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00159     |
|    std                  | 1.82         |
|    value_loss           | 0.33         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 260     |
|    time_elapsed    | 9930    |
|    total_timesteps | 2129920 |
--------------------------------
Eval num_timesteps=2130000, episode_reward=-453.77 +/- 84.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -454         |
| time/                   |              |
|    total_timesteps      | 2130000      |
| train/                  |              |
|    approx_kl            | 0.0012175941 |
|    clip_fraction        | 6.1e-05      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 34.3         |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.000293    |
|    std                  | 1.83         |
|    value_loss           | 109          |
------------------------------------------
Eval num_timesteps=2135000, episode_reward=-242.55 +/- 61.91
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -243     |
| time/              |          |
|    total_timesteps | 2135000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 261     |
|    time_elapsed    | 9975    |
|    total_timesteps | 2138112 |
--------------------------------
Eval num_timesteps=2140000, episode_reward=-413.56 +/- 152.24
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 2140000      |
| train/                  |              |
|    approx_kl            | 0.0003368812 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.98         |
|    n_updates            | 2610         |
|    policy_gradient_loss | -7.9e-05     |
|    std                  | 1.83         |
|    value_loss           | 166          |
------------------------------------------
Eval num_timesteps=2145000, episode_reward=-340.54 +/- 77.78
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -341     |
| time/              |          |
|    total_timesteps | 2145000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 262     |
|    time_elapsed    | 10021   |
|    total_timesteps | 2146304 |
--------------------------------
Eval num_timesteps=2150000, episode_reward=-400.94 +/- 134.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -401         |
| time/                   |              |
|    total_timesteps      | 2150000      |
| train/                  |              |
|    approx_kl            | 0.0004508197 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0654       |
|    n_updates            | 2620         |
|    policy_gradient_loss | -0.000176    |
|    std                  | 1.83         |
|    value_loss           | 51.1         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 263     |
|    time_elapsed    | 10059   |
|    total_timesteps | 2154496 |
--------------------------------
Eval num_timesteps=2155000, episode_reward=-329.43 +/- 148.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -329         |
| time/                   |              |
|    total_timesteps      | 2155000      |
| train/                  |              |
|    approx_kl            | 0.0045480775 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.0127      |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 1.82         |
|    value_loss           | 51.2         |
------------------------------------------
Eval num_timesteps=2160000, episode_reward=-482.89 +/- 137.41
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -483     |
| time/              |          |
|    total_timesteps | 2160000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 264     |
|    time_elapsed    | 10104   |
|    total_timesteps | 2162688 |
--------------------------------
Eval num_timesteps=2165000, episode_reward=-351.37 +/- 150.68
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -351        |
| time/                   |             |
|    total_timesteps      | 2165000     |
| train/                  |             |
|    approx_kl            | 0.004341117 |
|    clip_fraction        | 0.0177      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.05       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.166       |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.00126    |
|    std                  | 1.82        |
|    value_loss           | 1.73        |
-----------------------------------------
Eval num_timesteps=2170000, episode_reward=-233.61 +/- 136.49
Episode length: 829.60 +/- 342.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 830      |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 2170000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 265     |
|    time_elapsed    | 10149   |
|    total_timesteps | 2170880 |
--------------------------------
Eval num_timesteps=2175000, episode_reward=-321.50 +/- 112.00
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -322          |
| time/                   |               |
|    total_timesteps      | 2175000       |
| train/                  |               |
|    approx_kl            | 0.00017096914 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.06         |
|    explained_variance   | 0.989         |
|    learning_rate        | 5e-05         |
|    loss                 | 121           |
|    n_updates            | 2650          |
|    policy_gradient_loss | 3.33e-07      |
|    std                  | 1.82          |
|    value_loss           | 121           |
-------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 266     |
|    time_elapsed    | 10185   |
|    total_timesteps | 2179072 |
--------------------------------
Eval num_timesteps=2180000, episode_reward=-282.13 +/- 232.50
Episode length: 812.00 +/- 378.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | -282         |
| time/                   |              |
|    total_timesteps      | 2180000      |
| train/                  |              |
|    approx_kl            | 0.0026912973 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.06        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0925       |
|    n_updates            | 2660         |
|    policy_gradient_loss | -0.00206     |
|    std                  | 1.83         |
|    value_loss           | 0.316        |
------------------------------------------
Eval num_timesteps=2185000, episode_reward=-199.11 +/- 27.75
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 2185000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 267     |
|    time_elapsed    | 10228   |
|    total_timesteps | 2187264 |
--------------------------------
Eval num_timesteps=2190000, episode_reward=-365.69 +/- 111.49
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -366         |
| time/                   |              |
|    total_timesteps      | 2190000      |
| train/                  |              |
|    approx_kl            | 0.0028095755 |
|    clip_fraction        | 0.00353      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.07        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 20.5         |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 1.83         |
|    value_loss           | 66.4         |
------------------------------------------
Eval num_timesteps=2195000, episode_reward=-272.07 +/- 61.31
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 2195000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 268     |
|    time_elapsed    | 10275   |
|    total_timesteps | 2195456 |
--------------------------------
Eval num_timesteps=2200000, episode_reward=-270.47 +/- 94.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -270         |
| time/                   |              |
|    total_timesteps      | 2200000      |
| train/                  |              |
|    approx_kl            | 0.0030582426 |
|    clip_fraction        | 0.00756      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.08        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.956        |
|    n_updates            | 2680         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 1.84         |
|    value_loss           | 118          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 269     |
|    time_elapsed    | 10312   |
|    total_timesteps | 2203648 |
--------------------------------
Eval num_timesteps=2205000, episode_reward=-303.60 +/- 124.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -304         |
| time/                   |              |
|    total_timesteps      | 2205000      |
| train/                  |              |
|    approx_kl            | 0.0037199222 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.09        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 2            |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 1.85         |
|    value_loss           | 146          |
------------------------------------------
Eval num_timesteps=2210000, episode_reward=-358.70 +/- 173.62
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -359     |
| time/              |          |
|    total_timesteps | 2210000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 270     |
|    time_elapsed    | 10357   |
|    total_timesteps | 2211840 |
--------------------------------
Eval num_timesteps=2215000, episode_reward=-275.33 +/- 103.59
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -275        |
| time/                   |             |
|    total_timesteps      | 2215000     |
| train/                  |             |
|    approx_kl            | 0.001566827 |
|    clip_fraction        | 0.000391    |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.556       |
|    n_updates            | 2700        |
|    policy_gradient_loss | -0.00055    |
|    std                  | 1.85        |
|    value_loss           | 40.5        |
-----------------------------------------
Eval num_timesteps=2220000, episode_reward=-233.19 +/- 72.62
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 2220000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 271     |
|    time_elapsed    | 10402   |
|    total_timesteps | 2220032 |
--------------------------------
Eval num_timesteps=2225000, episode_reward=-335.50 +/- 66.38
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -335        |
| time/                   |             |
|    total_timesteps      | 2225000     |
| train/                  |             |
|    approx_kl            | 0.004088466 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | -0.00469    |
|    n_updates            | 2710        |
|    policy_gradient_loss | -0.00154    |
|    std                  | 1.85        |
|    value_loss           | 5.1         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 272     |
|    time_elapsed    | 10439   |
|    total_timesteps | 2228224 |
--------------------------------
Eval num_timesteps=2230000, episode_reward=-357.23 +/- 74.91
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -357        |
| time/                   |             |
|    total_timesteps      | 2230000     |
| train/                  |             |
|    approx_kl            | 0.003430022 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.1        |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.513       |
|    n_updates            | 2720        |
|    policy_gradient_loss | -0.00167    |
|    std                  | 1.85        |
|    value_loss           | 32.5        |
-----------------------------------------
Eval num_timesteps=2235000, episode_reward=-303.87 +/- 108.20
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 2235000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 273     |
|    time_elapsed    | 10485   |
|    total_timesteps | 2236416 |
--------------------------------
Eval num_timesteps=2240000, episode_reward=-340.32 +/- 137.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -340         |
| time/                   |              |
|    total_timesteps      | 2240000      |
| train/                  |              |
|    approx_kl            | 0.0007308952 |
|    clip_fraction        | 0.000146     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.11        |
|    explained_variance   | 0.986        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.246        |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.000497    |
|    std                  | 1.86         |
|    value_loss           | 75.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 274     |
|    time_elapsed    | 10521   |
|    total_timesteps | 2244608 |
--------------------------------
Eval num_timesteps=2245000, episode_reward=-313.12 +/- 102.16
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -313         |
| time/                   |              |
|    total_timesteps      | 2245000      |
| train/                  |              |
|    approx_kl            | 0.0053716805 |
|    clip_fraction        | 0.0189       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.11        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 135          |
|    n_updates            | 2740         |
|    policy_gradient_loss | 0.000109     |
|    std                  | 1.86         |
|    value_loss           | 18.3         |
------------------------------------------
Eval num_timesteps=2250000, episode_reward=-210.58 +/- 29.54
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 2250000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 275     |
|    time_elapsed    | 10566   |
|    total_timesteps | 2252800 |
--------------------------------
Eval num_timesteps=2255000, episode_reward=-282.03 +/- 71.00
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -282         |
| time/                   |              |
|    total_timesteps      | 2255000      |
| train/                  |              |
|    approx_kl            | 0.0027372888 |
|    clip_fraction        | 0.00856      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.13        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 57.2         |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 1.88         |
|    value_loss           | 28.7         |
------------------------------------------
Eval num_timesteps=2260000, episode_reward=-346.83 +/- 112.18
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 2260000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 276     |
|    time_elapsed    | 10612   |
|    total_timesteps | 2260992 |
--------------------------------
Eval num_timesteps=2265000, episode_reward=-245.75 +/- 175.48
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -246       |
| time/                   |            |
|    total_timesteps      | 2265000    |
| train/                  |            |
|    approx_kl            | 0.00354043 |
|    clip_fraction        | 0.00631    |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.14      |
|    explained_variance   | 0.995      |
|    learning_rate        | 5e-05      |
|    loss                 | 68.9       |
|    n_updates            | 2760       |
|    policy_gradient_loss | -0.000553  |
|    std                  | 1.88       |
|    value_loss           | 78.7       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 277     |
|    time_elapsed    | 10649   |
|    total_timesteps | 2269184 |
--------------------------------
Eval num_timesteps=2270000, episode_reward=-362.35 +/- 154.01
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -362         |
| time/                   |              |
|    total_timesteps      | 2270000      |
| train/                  |              |
|    approx_kl            | 0.0029509007 |
|    clip_fraction        | 0.00847      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.15        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.726        |
|    n_updates            | 2770         |
|    policy_gradient_loss | -0.000443    |
|    std                  | 1.88         |
|    value_loss           | 130          |
------------------------------------------
Eval num_timesteps=2275000, episode_reward=-437.91 +/- 137.78
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 2275000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 278     |
|    time_elapsed    | 10694   |
|    total_timesteps | 2277376 |
--------------------------------
Eval num_timesteps=2280000, episode_reward=-256.11 +/- 95.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -256         |
| time/                   |              |
|    total_timesteps      | 2280000      |
| train/                  |              |
|    approx_kl            | 0.0018860489 |
|    clip_fraction        | 0.000952     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.16        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.824        |
|    n_updates            | 2780         |
|    policy_gradient_loss | -0.000571    |
|    std                  | 1.89         |
|    value_loss           | 11           |
------------------------------------------
Eval num_timesteps=2285000, episode_reward=-439.24 +/- 203.84
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -439     |
| time/              |          |
|    total_timesteps | 2285000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 279     |
|    time_elapsed    | 10739   |
|    total_timesteps | 2285568 |
--------------------------------
Eval num_timesteps=2290000, episode_reward=-328.90 +/- 88.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -329         |
| time/                   |              |
|    total_timesteps      | 2290000      |
| train/                  |              |
|    approx_kl            | 0.0023317314 |
|    clip_fraction        | 0.00345      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.16        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.101        |
|    n_updates            | 2790         |
|    policy_gradient_loss | -0.000623    |
|    std                  | 1.89         |
|    value_loss           | 15.1         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 280     |
|    time_elapsed    | 10777   |
|    total_timesteps | 2293760 |
--------------------------------
Eval num_timesteps=2295000, episode_reward=-324.48 +/- 160.53
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -324        |
| time/                   |             |
|    total_timesteps      | 2295000     |
| train/                  |             |
|    approx_kl            | 0.003336784 |
|    clip_fraction        | 0.00641     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.17       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 7.37        |
|    n_updates            | 2800        |
|    policy_gradient_loss | -0.000461   |
|    std                  | 1.89        |
|    value_loss           | 3.38        |
-----------------------------------------
Eval num_timesteps=2300000, episode_reward=-412.96 +/- 160.53
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 2300000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 281     |
|    time_elapsed    | 10822   |
|    total_timesteps | 2301952 |
--------------------------------
Eval num_timesteps=2305000, episode_reward=-369.12 +/- 84.69
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -369         |
| time/                   |              |
|    total_timesteps      | 2305000      |
| train/                  |              |
|    approx_kl            | 0.0028321052 |
|    clip_fraction        | 0.00216      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.17        |
|    explained_variance   | 0.962        |
|    learning_rate        | 5e-05        |
|    loss                 | 58.4         |
|    n_updates            | 2810         |
|    policy_gradient_loss | -0.000626    |
|    std                  | 1.9          |
|    value_loss           | 128          |
------------------------------------------
Eval num_timesteps=2310000, episode_reward=-393.43 +/- 129.20
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 2310000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 282     |
|    time_elapsed    | 10869   |
|    total_timesteps | 2310144 |
--------------------------------
Eval num_timesteps=2315000, episode_reward=-332.02 +/- 195.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 2315000      |
| train/                  |              |
|    approx_kl            | 0.0036534085 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.18        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 0.997        |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 1.91         |
|    value_loss           | 215          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 283     |
|    time_elapsed    | 10906   |
|    total_timesteps | 2318336 |
--------------------------------
Eval num_timesteps=2320000, episode_reward=-293.31 +/- 62.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -293         |
| time/                   |              |
|    total_timesteps      | 2320000      |
| train/                  |              |
|    approx_kl            | 0.0053600227 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.2         |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0382       |
|    n_updates            | 2830         |
|    policy_gradient_loss | -0.00228     |
|    std                  | 1.91         |
|    value_loss           | 0.858        |
------------------------------------------
Eval num_timesteps=2325000, episode_reward=-280.46 +/- 100.91
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 2325000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 284     |
|    time_elapsed    | 10951   |
|    total_timesteps | 2326528 |
--------------------------------
Eval num_timesteps=2330000, episode_reward=-392.16 +/- 191.77
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -392         |
| time/                   |              |
|    total_timesteps      | 2330000      |
| train/                  |              |
|    approx_kl            | 0.0028298348 |
|    clip_fraction        | 0.00364      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.2         |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 9.37         |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.000894    |
|    std                  | 1.91         |
|    value_loss           | 19.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 285     |
|    time_elapsed    | 10989   |
|    total_timesteps | 2334720 |
--------------------------------
Eval num_timesteps=2335000, episode_reward=-230.42 +/- 130.63
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -230        |
| time/                   |             |
|    total_timesteps      | 2335000     |
| train/                  |             |
|    approx_kl            | 0.004559088 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.21       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.45e+03    |
|    n_updates            | 2850        |
|    policy_gradient_loss | -0.0027     |
|    std                  | 1.92        |
|    value_loss           | 164         |
-----------------------------------------
Eval num_timesteps=2340000, episode_reward=-282.47 +/- 203.60
Episode length: 807.20 +/- 387.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 2340000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 286     |
|    time_elapsed    | 11033   |
|    total_timesteps | 2342912 |
--------------------------------
Eval num_timesteps=2345000, episode_reward=-319.02 +/- 63.73
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -319          |
| time/                   |               |
|    total_timesteps      | 2345000       |
| train/                  |               |
|    approx_kl            | 0.00066738355 |
|    clip_fraction        | 6.1e-05       |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.22         |
|    explained_variance   | 0.988         |
|    learning_rate        | 5e-05         |
|    loss                 | 0.583         |
|    n_updates            | 2860          |
|    policy_gradient_loss | -0.000639     |
|    std                  | 1.92          |
|    value_loss           | 228           |
-------------------------------------------
Eval num_timesteps=2350000, episode_reward=-352.84 +/- 165.44
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -353     |
| time/              |          |
|    total_timesteps | 2350000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 287     |
|    time_elapsed    | 11078   |
|    total_timesteps | 2351104 |
--------------------------------
Eval num_timesteps=2355000, episode_reward=-345.41 +/- 88.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -345         |
| time/                   |              |
|    total_timesteps      | 2355000      |
| train/                  |              |
|    approx_kl            | 0.0034935279 |
|    clip_fraction        | 0.00684      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.22        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 26.1         |
|    n_updates            | 2870         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.93         |
|    value_loss           | 115          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 288     |
|    time_elapsed    | 11116   |
|    total_timesteps | 2359296 |
--------------------------------
Eval num_timesteps=2360000, episode_reward=-351.09 +/- 156.97
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -351         |
| time/                   |              |
|    total_timesteps      | 2360000      |
| train/                  |              |
|    approx_kl            | 0.0030660522 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.24        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.112        |
|    n_updates            | 2880         |
|    policy_gradient_loss | -0.000557    |
|    std                  | 1.94         |
|    value_loss           | 63.6         |
------------------------------------------
Eval num_timesteps=2365000, episode_reward=-265.49 +/- 126.25
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 2365000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 289     |
|    time_elapsed    | 11161   |
|    total_timesteps | 2367488 |
--------------------------------
Eval num_timesteps=2370000, episode_reward=-386.53 +/- 192.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -387         |
| time/                   |              |
|    total_timesteps      | 2370000      |
| train/                  |              |
|    approx_kl            | 0.0031294168 |
|    clip_fraction        | 0.00735      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.25        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 202          |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.000981    |
|    std                  | 1.94         |
|    value_loss           | 44           |
------------------------------------------
Eval num_timesteps=2375000, episode_reward=-375.64 +/- 151.76
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 2375000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 290     |
|    time_elapsed    | 11208   |
|    total_timesteps | 2375680 |
--------------------------------
Eval num_timesteps=2380000, episode_reward=-334.36 +/- 138.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -334         |
| time/                   |              |
|    total_timesteps      | 2380000      |
| train/                  |              |
|    approx_kl            | 0.0017360908 |
|    clip_fraction        | 0.000916     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.25        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 126          |
|    n_updates            | 2900         |
|    policy_gradient_loss | -0.000953    |
|    std                  | 1.95         |
|    value_loss           | 87.5         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 291     |
|    time_elapsed    | 11246   |
|    total_timesteps | 2383872 |
--------------------------------
Eval num_timesteps=2385000, episode_reward=-415.81 +/- 189.26
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 2385000     |
| train/                  |             |
|    approx_kl            | 0.002292986 |
|    clip_fraction        | 0.00439     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 76.1        |
|    n_updates            | 2910        |
|    policy_gradient_loss | -0.000764   |
|    std                  | 1.96        |
|    value_loss           | 98.3        |
-----------------------------------------
Eval num_timesteps=2390000, episode_reward=-424.45 +/- 155.19
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 2390000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 292     |
|    time_elapsed    | 11292   |
|    total_timesteps | 2392064 |
--------------------------------
Eval num_timesteps=2395000, episode_reward=-360.69 +/- 168.92
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -361        |
| time/                   |             |
|    total_timesteps      | 2395000     |
| train/                  |             |
|    approx_kl            | 0.004465296 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.26       |
|    explained_variance   | 0.99        |
|    learning_rate        | 5e-05       |
|    loss                 | 4.83        |
|    n_updates            | 2920        |
|    policy_gradient_loss | -0.000762   |
|    std                  | 1.95        |
|    value_loss           | 105         |
-----------------------------------------
Eval num_timesteps=2400000, episode_reward=-334.78 +/- 160.75
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 2400000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 293     |
|    time_elapsed    | 11338   |
|    total_timesteps | 2400256 |
--------------------------------
Eval num_timesteps=2405000, episode_reward=-405.59 +/- 157.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 2405000     |
| train/                  |             |
|    approx_kl            | 0.004109906 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.25       |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.599       |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00106    |
|    std                  | 1.94        |
|    value_loss           | 24.3        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 294     |
|    time_elapsed    | 11375   |
|    total_timesteps | 2408448 |
--------------------------------
Eval num_timesteps=2410000, episode_reward=-218.46 +/- 54.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -218         |
| time/                   |              |
|    total_timesteps      | 2410000      |
| train/                  |              |
|    approx_kl            | 0.0029203957 |
|    clip_fraction        | 0.0117       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.26        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 1.37         |
|    n_updates            | 2940         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 1.95         |
|    value_loss           | 0.889        |
------------------------------------------
Eval num_timesteps=2415000, episode_reward=-331.59 +/- 195.91
Episode length: 814.00 +/- 374.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 814      |
|    mean_reward     | -332     |
| time/              |          |
|    total_timesteps | 2415000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 295     |
|    time_elapsed    | 11418   |
|    total_timesteps | 2416640 |
--------------------------------
Eval num_timesteps=2420000, episode_reward=-263.41 +/- 88.62
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -263         |
| time/                   |              |
|    total_timesteps      | 2420000      |
| train/                  |              |
|    approx_kl            | 0.0010509132 |
|    clip_fraction        | 0.000244     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.26        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 579          |
|    n_updates            | 2950         |
|    policy_gradient_loss | -0.000484    |
|    std                  | 1.95         |
|    value_loss           | 118          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 296     |
|    time_elapsed    | 11457   |
|    total_timesteps | 2424832 |
--------------------------------
Eval num_timesteps=2425000, episode_reward=-406.37 +/- 188.27
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -406        |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.004187069 |
|    clip_fraction        | 0.0255      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.27       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.186       |
|    n_updates            | 2960        |
|    policy_gradient_loss | -0.000825   |
|    std                  | 1.96        |
|    value_loss           | 98.5        |
-----------------------------------------
Eval num_timesteps=2430000, episode_reward=-420.51 +/- 88.79
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 2430000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 297     |
|    time_elapsed    | 11503   |
|    total_timesteps | 2433024 |
--------------------------------
Eval num_timesteps=2435000, episode_reward=-212.95 +/- 77.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -213         |
| time/                   |              |
|    total_timesteps      | 2435000      |
| train/                  |              |
|    approx_kl            | 0.0039835493 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.28        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.319        |
|    n_updates            | 2970         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 1.96         |
|    value_loss           | 1.99         |
------------------------------------------
Eval num_timesteps=2440000, episode_reward=-273.70 +/- 30.39
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 2440000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 298     |
|    time_elapsed    | 11549   |
|    total_timesteps | 2441216 |
--------------------------------
Eval num_timesteps=2445000, episode_reward=-398.94 +/- 211.92
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -399        |
| time/                   |             |
|    total_timesteps      | 2445000     |
| train/                  |             |
|    approx_kl            | 0.004509027 |
|    clip_fraction        | 0.00947     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.28       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.588       |
|    n_updates            | 2980        |
|    policy_gradient_loss | -0.00193    |
|    std                  | 1.96        |
|    value_loss           | 106         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 299     |
|    time_elapsed    | 11588   |
|    total_timesteps | 2449408 |
--------------------------------
Eval num_timesteps=2450000, episode_reward=-415.88 +/- 104.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -416         |
| time/                   |              |
|    total_timesteps      | 2450000      |
| train/                  |              |
|    approx_kl            | 0.0038355934 |
|    clip_fraction        | 0.0135       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.28        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | -0.0128      |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 1.97         |
|    value_loss           | 0.314        |
------------------------------------------
Eval num_timesteps=2455000, episode_reward=-432.84 +/- 224.58
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 2455000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 300     |
|    time_elapsed    | 11633   |
|    total_timesteps | 2457600 |
--------------------------------
Eval num_timesteps=2460000, episode_reward=-421.16 +/- 192.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -421         |
| time/                   |              |
|    total_timesteps      | 2460000      |
| train/                  |              |
|    approx_kl            | 0.0036061727 |
|    clip_fraction        | 0.00623      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.29        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.318        |
|    n_updates            | 3000         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 1.97         |
|    value_loss           | 18.5         |
------------------------------------------
Eval num_timesteps=2465000, episode_reward=-437.94 +/- 81.29
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 2465000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 301     |
|    time_elapsed    | 11679   |
|    total_timesteps | 2465792 |
--------------------------------
Eval num_timesteps=2470000, episode_reward=-330.73 +/- 89.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -331         |
| time/                   |              |
|    total_timesteps      | 2470000      |
| train/                  |              |
|    approx_kl            | 0.0031063943 |
|    clip_fraction        | 0.00767      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.3         |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 54.2         |
|    n_updates            | 3010         |
|    policy_gradient_loss | -0.000625    |
|    std                  | 1.98         |
|    value_loss           | 194          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 302     |
|    time_elapsed    | 11718   |
|    total_timesteps | 2473984 |
--------------------------------
Eval num_timesteps=2475000, episode_reward=-370.41 +/- 161.53
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -370      |
| time/                   |           |
|    total_timesteps      | 2475000   |
| train/                  |           |
|    approx_kl            | 0.0036979 |
|    clip_fraction        | 0.00983   |
|    clip_range           | 0.2       |
|    entropy_loss         | -6.31     |
|    explained_variance   | 0.993     |
|    learning_rate        | 5e-05     |
|    loss                 | 71.2      |
|    n_updates            | 3020      |
|    policy_gradient_loss | -0.00119  |
|    std                  | 1.99      |
|    value_loss           | 125       |
---------------------------------------
Eval num_timesteps=2480000, episode_reward=-368.77 +/- 195.03
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 2480000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 303     |
|    time_elapsed    | 11763   |
|    total_timesteps | 2482176 |
--------------------------------
Eval num_timesteps=2485000, episode_reward=-229.82 +/- 154.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -230         |
| time/                   |              |
|    total_timesteps      | 2485000      |
| train/                  |              |
|    approx_kl            | 0.0036555764 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.32        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0985       |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 1.99         |
|    value_loss           | 98.8         |
------------------------------------------
Eval num_timesteps=2490000, episode_reward=-230.63 +/- 94.81
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 2490000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 304     |
|    time_elapsed    | 11809   |
|    total_timesteps | 2490368 |
--------------------------------
Eval num_timesteps=2495000, episode_reward=-265.03 +/- 79.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -265        |
| time/                   |             |
|    total_timesteps      | 2495000     |
| train/                  |             |
|    approx_kl            | 0.002282014 |
|    clip_fraction        | 0.00415     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.32       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0862      |
|    n_updates            | 3040        |
|    policy_gradient_loss | -0.000912   |
|    std                  | 1.99        |
|    value_loss           | 61.3        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 305     |
|    time_elapsed    | 11848   |
|    total_timesteps | 2498560 |
--------------------------------
Eval num_timesteps=2500000, episode_reward=-340.91 +/- 139.49
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -341         |
| time/                   |              |
|    total_timesteps      | 2500000      |
| train/                  |              |
|    approx_kl            | 0.0050044367 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.32        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.851        |
|    n_updates            | 3050         |
|    policy_gradient_loss | -0.0021      |
|    std                  | 1.99         |
|    value_loss           | 60.6         |
------------------------------------------
Eval num_timesteps=2505000, episode_reward=-383.48 +/- 180.23
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 2505000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 306     |
|    time_elapsed    | 11893   |
|    total_timesteps | 2506752 |
--------------------------------
Eval num_timesteps=2510000, episode_reward=-227.67 +/- 87.06
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -228         |
| time/                   |              |
|    total_timesteps      | 2510000      |
| train/                  |              |
|    approx_kl            | 0.0015345928 |
|    clip_fraction        | 0.000562     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.33        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.17         |
|    n_updates            | 3060         |
|    policy_gradient_loss | -0.000531    |
|    std                  | 1.99         |
|    value_loss           | 91.4         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 307     |
|    time_elapsed    | 11931   |
|    total_timesteps | 2514944 |
--------------------------------
Eval num_timesteps=2515000, episode_reward=-231.84 +/- 142.01
Episode length: 807.80 +/- 386.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -232         |
| time/                   |              |
|    total_timesteps      | 2515000      |
| train/                  |              |
|    approx_kl            | 0.0014900558 |
|    clip_fraction        | 0.0017       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.33        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.224        |
|    n_updates            | 3070         |
|    policy_gradient_loss | 0.000265     |
|    std                  | 2            |
|    value_loss           | 80.9         |
------------------------------------------
Eval num_timesteps=2520000, episode_reward=-336.73 +/- 119.15
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -337     |
| time/              |          |
|    total_timesteps | 2520000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 308     |
|    time_elapsed    | 11977   |
|    total_timesteps | 2523136 |
--------------------------------
Eval num_timesteps=2525000, episode_reward=-212.42 +/- 111.14
Episode length: 817.80 +/- 366.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 818          |
|    mean_reward          | -212         |
| time/                   |              |
|    total_timesteps      | 2525000      |
| train/                  |              |
|    approx_kl            | 0.0041984557 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.33        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0481       |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 1.99         |
|    value_loss           | 67.7         |
------------------------------------------
Eval num_timesteps=2530000, episode_reward=-320.55 +/- 138.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -321     |
| time/              |          |
|    total_timesteps | 2530000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 309     |
|    time_elapsed    | 12021   |
|    total_timesteps | 2531328 |
--------------------------------
Eval num_timesteps=2535000, episode_reward=-306.54 +/- 138.09
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -307        |
| time/                   |             |
|    total_timesteps      | 2535000     |
| train/                  |             |
|    approx_kl            | 0.004176178 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.33       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 18.2        |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00188    |
|    std                  | 2           |
|    value_loss           | 121         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 310     |
|    time_elapsed    | 12060   |
|    total_timesteps | 2539520 |
--------------------------------
Eval num_timesteps=2540000, episode_reward=-388.25 +/- 127.16
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -388        |
| time/                   |             |
|    total_timesteps      | 2540000     |
| train/                  |             |
|    approx_kl            | 0.004290979 |
|    clip_fraction        | 0.014       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.34       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.261       |
|    n_updates            | 3100        |
|    policy_gradient_loss | -0.00179    |
|    std                  | 2           |
|    value_loss           | 8.89        |
-----------------------------------------
Eval num_timesteps=2545000, episode_reward=-291.22 +/- 70.41
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 2545000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 311     |
|    time_elapsed    | 12106   |
|    total_timesteps | 2547712 |
--------------------------------
Eval num_timesteps=2550000, episode_reward=-262.83 +/- 147.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -263         |
| time/                   |              |
|    total_timesteps      | 2550000      |
| train/                  |              |
|    approx_kl            | 0.0023586862 |
|    clip_fraction        | 0.00165      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.34        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.132        |
|    n_updates            | 3110         |
|    policy_gradient_loss | -0.000255    |
|    std                  | 2.01         |
|    value_loss           | 61.7         |
------------------------------------------
Eval num_timesteps=2555000, episode_reward=-288.11 +/- 98.53
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 2555000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 312     |
|    time_elapsed    | 12152   |
|    total_timesteps | 2555904 |
--------------------------------
Eval num_timesteps=2560000, episode_reward=-386.72 +/- 210.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -387         |
| time/                   |              |
|    total_timesteps      | 2560000      |
| train/                  |              |
|    approx_kl            | 0.0038938425 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.35        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.531        |
|    n_updates            | 3120         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 2.02         |
|    value_loss           | 152          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 313     |
|    time_elapsed    | 12192   |
|    total_timesteps | 2564096 |
--------------------------------
Eval num_timesteps=2565000, episode_reward=-329.58 +/- 144.88
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -330        |
| time/                   |             |
|    total_timesteps      | 2565000     |
| train/                  |             |
|    approx_kl            | 0.004470662 |
|    clip_fraction        | 0.0149      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.37       |
|    explained_variance   | 0.977       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.178       |
|    n_updates            | 3130        |
|    policy_gradient_loss | -0.00141    |
|    std                  | 2.03        |
|    value_loss           | 63.1        |
-----------------------------------------
Eval num_timesteps=2570000, episode_reward=-262.64 +/- 116.23
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 2570000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 314     |
|    time_elapsed    | 12237   |
|    total_timesteps | 2572288 |
--------------------------------
Eval num_timesteps=2575000, episode_reward=-337.28 +/- 110.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -337         |
| time/                   |              |
|    total_timesteps      | 2575000      |
| train/                  |              |
|    approx_kl            | 0.0039642253 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.38        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.493        |
|    n_updates            | 3140         |
|    policy_gradient_loss | -0.0021      |
|    std                  | 2.03         |
|    value_loss           | 1.45         |
------------------------------------------
Eval num_timesteps=2580000, episode_reward=-307.95 +/- 178.76
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 2580000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 315     |
|    time_elapsed    | 12283   |
|    total_timesteps | 2580480 |
--------------------------------
Eval num_timesteps=2585000, episode_reward=-323.67 +/- 111.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 2585000      |
| train/                  |              |
|    approx_kl            | 0.0021071767 |
|    clip_fraction        | 0.00215      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.38        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.102        |
|    n_updates            | 3150         |
|    policy_gradient_loss | -0.000717    |
|    std                  | 2.03         |
|    value_loss           | 22           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 316     |
|    time_elapsed    | 12323   |
|    total_timesteps | 2588672 |
--------------------------------
Eval num_timesteps=2590000, episode_reward=-297.72 +/- 79.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -298         |
| time/                   |              |
|    total_timesteps      | 2590000      |
| train/                  |              |
|    approx_kl            | 0.0029340016 |
|    clip_fraction        | 0.00492      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.38        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.968        |
|    n_updates            | 3160         |
|    policy_gradient_loss | -0.000335    |
|    std                  | 2.03         |
|    value_loss           | 83.3         |
------------------------------------------
Eval num_timesteps=2595000, episode_reward=-293.07 +/- 95.39
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 2595000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 317     |
|    time_elapsed    | 12369   |
|    total_timesteps | 2596864 |
--------------------------------
Eval num_timesteps=2600000, episode_reward=-406.18 +/- 156.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -406         |
| time/                   |              |
|    total_timesteps      | 2600000      |
| train/                  |              |
|    approx_kl            | 0.0036022696 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.38        |
|    explained_variance   | 0.987        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.598        |
|    n_updates            | 3170         |
|    policy_gradient_loss | -0.000818    |
|    std                  | 2.03         |
|    value_loss           | 173          |
------------------------------------------
Eval num_timesteps=2605000, episode_reward=-325.31 +/- 149.09
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -325     |
| time/              |          |
|    total_timesteps | 2605000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 318     |
|    time_elapsed    | 12416   |
|    total_timesteps | 2605056 |
--------------------------------
Eval num_timesteps=2610000, episode_reward=-280.23 +/- 34.07
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -280         |
| time/                   |              |
|    total_timesteps      | 2610000      |
| train/                  |              |
|    approx_kl            | 0.0042443494 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.38        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 1.25         |
|    n_updates            | 3180         |
|    policy_gradient_loss | -0.00201     |
|    std                  | 2.03         |
|    value_loss           | 1.35         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 319     |
|    time_elapsed    | 12454   |
|    total_timesteps | 2613248 |
--------------------------------
Eval num_timesteps=2615000, episode_reward=-238.54 +/- 186.05
Episode length: 813.00 +/- 376.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -239         |
| time/                   |              |
|    total_timesteps      | 2615000      |
| train/                  |              |
|    approx_kl            | 0.0034653656 |
|    clip_fraction        | 0.011        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.38        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.62         |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 2.03         |
|    value_loss           | 0.633        |
------------------------------------------
Eval num_timesteps=2620000, episode_reward=-384.29 +/- 144.87
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -384     |
| time/              |          |
|    total_timesteps | 2620000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 320     |
|    time_elapsed    | 12499   |
|    total_timesteps | 2621440 |
--------------------------------
Eval num_timesteps=2625000, episode_reward=-297.66 +/- 190.45
Episode length: 807.80 +/- 386.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -298         |
| time/                   |              |
|    total_timesteps      | 2625000      |
| train/                  |              |
|    approx_kl            | 0.0019004955 |
|    clip_fraction        | 0.002        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.38        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0323       |
|    n_updates            | 3200         |
|    policy_gradient_loss | -0.000463    |
|    std                  | 2.03         |
|    value_loss           | 21.4         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 321     |
|    time_elapsed    | 12535   |
|    total_timesteps | 2629632 |
--------------------------------
Eval num_timesteps=2630000, episode_reward=-353.07 +/- 149.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -353         |
| time/                   |              |
|    total_timesteps      | 2630000      |
| train/                  |              |
|    approx_kl            | 0.0033370536 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.4         |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.257        |
|    n_updates            | 3210         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 2.05         |
|    value_loss           | 0.531        |
------------------------------------------
Eval num_timesteps=2635000, episode_reward=-355.30 +/- 93.00
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -355     |
| time/              |          |
|    total_timesteps | 2635000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 322     |
|    time_elapsed    | 12580   |
|    total_timesteps | 2637824 |
--------------------------------
Eval num_timesteps=2640000, episode_reward=-285.42 +/- 77.34
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -285         |
| time/                   |              |
|    total_timesteps      | 2640000      |
| train/                  |              |
|    approx_kl            | 0.0028172934 |
|    clip_fraction        | 0.00284      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.42        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.534        |
|    n_updates            | 3220         |
|    policy_gradient_loss | -0.000827    |
|    std                  | 2.06         |
|    value_loss           | 137          |
------------------------------------------
Eval num_timesteps=2645000, episode_reward=-339.08 +/- 189.73
Episode length: 803.00 +/- 396.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 2645000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 323     |
|    time_elapsed    | 12625   |
|    total_timesteps | 2646016 |
--------------------------------
Eval num_timesteps=2650000, episode_reward=-320.41 +/- 60.15
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -320        |
| time/                   |             |
|    total_timesteps      | 2650000     |
| train/                  |             |
|    approx_kl            | 0.002996319 |
|    clip_fraction        | 0.00558     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.43       |
|    explained_variance   | 0.988       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.677       |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.000504   |
|    std                  | 2.07        |
|    value_loss           | 120         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 324     |
|    time_elapsed    | 12664   |
|    total_timesteps | 2654208 |
--------------------------------
Eval num_timesteps=2655000, episode_reward=-217.98 +/- 160.03
Episode length: 808.20 +/- 385.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -218         |
| time/                   |              |
|    total_timesteps      | 2655000      |
| train/                  |              |
|    approx_kl            | 0.0039977534 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.43        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0386       |
|    n_updates            | 3240         |
|    policy_gradient_loss | -0.00133     |
|    std                  | 2.07         |
|    value_loss           | 0.419        |
------------------------------------------
Eval num_timesteps=2660000, episode_reward=-272.73 +/- 75.25
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 2660000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 325     |
|    time_elapsed    | 12709   |
|    total_timesteps | 2662400 |
--------------------------------
Eval num_timesteps=2665000, episode_reward=-448.28 +/- 156.50
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -448          |
| time/                   |               |
|    total_timesteps      | 2665000       |
| train/                  |               |
|    approx_kl            | 0.00063802954 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.43         |
|    explained_variance   | 0.995         |
|    learning_rate        | 5e-05         |
|    loss                 | 271           |
|    n_updates            | 3250          |
|    policy_gradient_loss | -0.000186     |
|    std                  | 2.07          |
|    value_loss           | 111           |
-------------------------------------------
Eval num_timesteps=2670000, episode_reward=-305.42 +/- 235.77
Episode length: 809.80 +/- 382.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 2670000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 326     |
|    time_elapsed    | 12754   |
|    total_timesteps | 2670592 |
--------------------------------
Eval num_timesteps=2675000, episode_reward=-289.37 +/- 148.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -289         |
| time/                   |              |
|    total_timesteps      | 2675000      |
| train/                  |              |
|    approx_kl            | 0.0037189825 |
|    clip_fraction        | 0.00533      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.43        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 25.2         |
|    n_updates            | 3260         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 2.07         |
|    value_loss           | 38.1         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 327     |
|    time_elapsed    | 12792   |
|    total_timesteps | 2678784 |
--------------------------------
Eval num_timesteps=2680000, episode_reward=-332.41 +/- 89.34
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 2680000      |
| train/                  |              |
|    approx_kl            | 0.0031556492 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.45        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.00904      |
|    n_updates            | 3270         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 2.08         |
|    value_loss           | 0.738        |
------------------------------------------
Eval num_timesteps=2685000, episode_reward=-311.35 +/- 148.94
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 2685000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 328     |
|    time_elapsed    | 12838   |
|    total_timesteps | 2686976 |
--------------------------------
Eval num_timesteps=2690000, episode_reward=-326.15 +/- 184.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -326         |
| time/                   |              |
|    total_timesteps      | 2690000      |
| train/                  |              |
|    approx_kl            | 0.0012808801 |
|    clip_fraction        | 0.000476     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.46        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 1            |
|    n_updates            | 3280         |
|    policy_gradient_loss | -0.000356    |
|    std                  | 2.08         |
|    value_loss           | 113          |
------------------------------------------
Eval num_timesteps=2695000, episode_reward=-308.06 +/- 70.27
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 2695000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 329     |
|    time_elapsed    | 12884   |
|    total_timesteps | 2695168 |
--------------------------------
Eval num_timesteps=2700000, episode_reward=-323.61 +/- 120.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 2700000      |
| train/                  |              |
|    approx_kl            | 0.0006735359 |
|    clip_fraction        | 6.1e-05      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.46        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 17.7         |
|    n_updates            | 3290         |
|    policy_gradient_loss | -0.00073     |
|    std                  | 2.08         |
|    value_loss           | 115          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 330     |
|    time_elapsed    | 12923   |
|    total_timesteps | 2703360 |
--------------------------------
Eval num_timesteps=2705000, episode_reward=-212.07 +/- 195.83
Episode length: 812.60 +/- 376.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -212         |
| time/                   |              |
|    total_timesteps      | 2705000      |
| train/                  |              |
|    approx_kl            | 0.0029209103 |
|    clip_fraction        | 0.00813      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.77         |
|    n_updates            | 3300         |
|    policy_gradient_loss | -0.000916    |
|    std                  | 2.09         |
|    value_loss           | 2.38         |
------------------------------------------
Eval num_timesteps=2710000, episode_reward=-343.70 +/- 145.22
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 2710000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 331     |
|    time_elapsed    | 12968   |
|    total_timesteps | 2711552 |
--------------------------------
Eval num_timesteps=2715000, episode_reward=-319.15 +/- 146.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 2715000      |
| train/                  |              |
|    approx_kl            | 0.0031461953 |
|    clip_fraction        | 0.00503      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.98         |
|    n_updates            | 3310         |
|    policy_gradient_loss | -0.0017      |
|    std                  | 2.09         |
|    value_loss           | 72.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 332     |
|    time_elapsed    | 13006   |
|    total_timesteps | 2719744 |
--------------------------------
Eval num_timesteps=2720000, episode_reward=-346.18 +/- 105.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -346         |
| time/                   |              |
|    total_timesteps      | 2720000      |
| train/                  |              |
|    approx_kl            | 0.0035966109 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.514        |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 2.09         |
|    value_loss           | 122          |
------------------------------------------
Eval num_timesteps=2725000, episode_reward=-205.56 +/- 53.17
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 2725000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 333     |
|    time_elapsed    | 13052   |
|    total_timesteps | 2727936 |
--------------------------------
Eval num_timesteps=2730000, episode_reward=-307.71 +/- 67.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -308         |
| time/                   |              |
|    total_timesteps      | 2730000      |
| train/                  |              |
|    approx_kl            | 0.0042192694 |
|    clip_fraction        | 0.00828      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.47        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 5.21         |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 2.09         |
|    value_loss           | 163          |
------------------------------------------
Eval num_timesteps=2735000, episode_reward=-329.58 +/- 242.13
Episode length: 822.40 +/- 357.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 822      |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 2735000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 334     |
|    time_elapsed    | 13097   |
|    total_timesteps | 2736128 |
--------------------------------
Eval num_timesteps=2740000, episode_reward=-323.38 +/- 179.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -323         |
| time/                   |              |
|    total_timesteps      | 2740000      |
| train/                  |              |
|    approx_kl            | 0.0041945903 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.48        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 8.16         |
|    n_updates            | 3340         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 2.1          |
|    value_loss           | 166          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 335     |
|    time_elapsed    | 13137   |
|    total_timesteps | 2744320 |
--------------------------------
Eval num_timesteps=2745000, episode_reward=-339.48 +/- 157.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -339         |
| time/                   |              |
|    total_timesteps      | 2745000      |
| train/                  |              |
|    approx_kl            | 0.0023235101 |
|    clip_fraction        | 0.0046       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.48        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 145          |
|    n_updates            | 3350         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 2.1          |
|    value_loss           | 146          |
------------------------------------------
Eval num_timesteps=2750000, episode_reward=-284.92 +/- 102.68
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 2750000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 336     |
|    time_elapsed    | 13183   |
|    total_timesteps | 2752512 |
--------------------------------
Eval num_timesteps=2755000, episode_reward=-391.48 +/- 137.98
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -391         |
| time/                   |              |
|    total_timesteps      | 2755000      |
| train/                  |              |
|    approx_kl            | 0.0033330123 |
|    clip_fraction        | 0.00658      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.49        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.31         |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00053     |
|    std                  | 2.11         |
|    value_loss           | 78.5         |
------------------------------------------
Eval num_timesteps=2760000, episode_reward=-470.44 +/- 159.54
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -470     |
| time/              |          |
|    total_timesteps | 2760000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 337     |
|    time_elapsed    | 13230   |
|    total_timesteps | 2760704 |
--------------------------------
Eval num_timesteps=2765000, episode_reward=-479.93 +/- 155.33
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -480         |
| time/                   |              |
|    total_timesteps      | 2765000      |
| train/                  |              |
|    approx_kl            | 0.0035873705 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.49        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.692        |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 2.11         |
|    value_loss           | 132          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 338     |
|    time_elapsed    | 13269   |
|    total_timesteps | 2768896 |
--------------------------------
Eval num_timesteps=2770000, episode_reward=-459.75 +/- 86.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -460         |
| time/                   |              |
|    total_timesteps      | 2770000      |
| train/                  |              |
|    approx_kl            | 0.0039281114 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.51        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | -0.00861     |
|    n_updates            | 3380         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 2.12         |
|    value_loss           | 1.88         |
------------------------------------------
Eval num_timesteps=2775000, episode_reward=-279.77 +/- 135.79
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 2775000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 339     |
|    time_elapsed    | 13314   |
|    total_timesteps | 2777088 |
--------------------------------
Eval num_timesteps=2780000, episode_reward=-197.06 +/- 55.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -197        |
| time/                   |             |
|    total_timesteps      | 2780000     |
| train/                  |             |
|    approx_kl            | 0.001520602 |
|    clip_fraction        | 0.000171    |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.51       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.241       |
|    n_updates            | 3390        |
|    policy_gradient_loss | -0.000261   |
|    std                  | 2.12        |
|    value_loss           | 41.8        |
-----------------------------------------
Eval num_timesteps=2785000, episode_reward=-252.87 +/- 95.44
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 2785000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 340     |
|    time_elapsed    | 13361   |
|    total_timesteps | 2785280 |
--------------------------------
Eval num_timesteps=2790000, episode_reward=-279.20 +/- 116.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -279         |
| time/                   |              |
|    total_timesteps      | 2790000      |
| train/                  |              |
|    approx_kl            | 0.0035128337 |
|    clip_fraction        | 0.0151       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.51        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 692          |
|    n_updates            | 3400         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 2.12         |
|    value_loss           | 149          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 341     |
|    time_elapsed    | 13400   |
|    total_timesteps | 2793472 |
--------------------------------
Eval num_timesteps=2795000, episode_reward=-340.76 +/- 128.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -341         |
| time/                   |              |
|    total_timesteps      | 2795000      |
| train/                  |              |
|    approx_kl            | 0.0025382247 |
|    clip_fraction        | 0.00321      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.52        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0719       |
|    n_updates            | 3410         |
|    policy_gradient_loss | -0.000688    |
|    std                  | 2.13         |
|    value_loss           | 64.3         |
------------------------------------------
Eval num_timesteps=2800000, episode_reward=-256.94 +/- 41.86
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 2800000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 342     |
|    time_elapsed    | 13445   |
|    total_timesteps | 2801664 |
--------------------------------
Eval num_timesteps=2805000, episode_reward=-279.07 +/- 119.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -279         |
| time/                   |              |
|    total_timesteps      | 2805000      |
| train/                  |              |
|    approx_kl            | 0.0032356286 |
|    clip_fraction        | 0.00555      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.53        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.28         |
|    n_updates            | 3420         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 2.14         |
|    value_loss           | 62.7         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 343     |
|    time_elapsed    | 13484   |
|    total_timesteps | 2809856 |
--------------------------------
Eval num_timesteps=2810000, episode_reward=-464.37 +/- 191.67
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -464        |
| time/                   |             |
|    total_timesteps      | 2810000     |
| train/                  |             |
|    approx_kl            | 0.004345914 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.54       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0332      |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00169    |
|    std                  | 2.15        |
|    value_loss           | 26.8        |
-----------------------------------------
Eval num_timesteps=2815000, episode_reward=-332.42 +/- 161.67
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -332     |
| time/              |          |
|    total_timesteps | 2815000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 344     |
|    time_elapsed    | 13531   |
|    total_timesteps | 2818048 |
--------------------------------
Eval num_timesteps=2820000, episode_reward=-312.12 +/- 182.24
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -312         |
| time/                   |              |
|    total_timesteps      | 2820000      |
| train/                  |              |
|    approx_kl            | 0.0031370786 |
|    clip_fraction        | 0.00831      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.56        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.154        |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 2.16         |
|    value_loss           | 2.46         |
------------------------------------------
Eval num_timesteps=2825000, episode_reward=-166.99 +/- 114.44
Episode length: 805.60 +/- 390.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 2825000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 345     |
|    time_elapsed    | 13577   |
|    total_timesteps | 2826240 |
--------------------------------
Eval num_timesteps=2830000, episode_reward=-326.24 +/- 89.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -326         |
| time/                   |              |
|    total_timesteps      | 2830000      |
| train/                  |              |
|    approx_kl            | 0.0031669415 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.56        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.126        |
|    n_updates            | 3450         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 2.16         |
|    value_loss           | 91           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 346     |
|    time_elapsed    | 13617   |
|    total_timesteps | 2834432 |
--------------------------------
Eval num_timesteps=2835000, episode_reward=-401.72 +/- 199.83
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -402         |
| time/                   |              |
|    total_timesteps      | 2835000      |
| train/                  |              |
|    approx_kl            | 0.0036306698 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.58        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 4.88         |
|    n_updates            | 3460         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 2.18         |
|    value_loss           | 0.977        |
------------------------------------------
Eval num_timesteps=2840000, episode_reward=-365.37 +/- 145.99
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -365     |
| time/              |          |
|    total_timesteps | 2840000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 347     |
|    time_elapsed    | 13664   |
|    total_timesteps | 2842624 |
--------------------------------
Eval num_timesteps=2845000, episode_reward=-328.09 +/- 126.06
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -328         |
| time/                   |              |
|    total_timesteps      | 2845000      |
| train/                  |              |
|    approx_kl            | 0.0017685334 |
|    clip_fraction        | 0.000879     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.59        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.16         |
|    n_updates            | 3470         |
|    policy_gradient_loss | 0.00013      |
|    std                  | 2.18         |
|    value_loss           | 182          |
------------------------------------------
Eval num_timesteps=2850000, episode_reward=-322.74 +/- 163.64
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -323     |
| time/              |          |
|    total_timesteps | 2850000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 348     |
|    time_elapsed    | 13709   |
|    total_timesteps | 2850816 |
--------------------------------
Eval num_timesteps=2855000, episode_reward=-510.73 +/- 135.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -511        |
| time/                   |             |
|    total_timesteps      | 2855000     |
| train/                  |             |
|    approx_kl            | 0.002753872 |
|    clip_fraction        | 0.00674     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.59       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.288       |
|    n_updates            | 3480        |
|    policy_gradient_loss | -0.000409   |
|    std                  | 2.18        |
|    value_loss           | 79.2        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 349     |
|    time_elapsed    | 13748   |
|    total_timesteps | 2859008 |
--------------------------------
Eval num_timesteps=2860000, episode_reward=-374.90 +/- 191.40
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -375        |
| time/                   |             |
|    total_timesteps      | 2860000     |
| train/                  |             |
|    approx_kl            | 0.004174934 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.6        |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.41        |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00257    |
|    std                  | 2.18        |
|    value_loss           | 1.04        |
-----------------------------------------
Eval num_timesteps=2865000, episode_reward=-333.86 +/- 143.08
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -334     |
| time/              |          |
|    total_timesteps | 2865000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 350     |
|    time_elapsed    | 13794   |
|    total_timesteps | 2867200 |
--------------------------------
Eval num_timesteps=2870000, episode_reward=-322.69 +/- 148.70
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -323         |
| time/                   |              |
|    total_timesteps      | 2870000      |
| train/                  |              |
|    approx_kl            | 0.0010400503 |
|    clip_fraction        | 0.000183     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.6         |
|    explained_variance   | 0.987        |
|    learning_rate        | 5e-05        |
|    loss                 | 126          |
|    n_updates            | 3500         |
|    policy_gradient_loss | -0.000153    |
|    std                  | 2.18         |
|    value_loss           | 223          |
------------------------------------------
Eval num_timesteps=2875000, episode_reward=-447.68 +/- 138.70
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -448     |
| time/              |          |
|    total_timesteps | 2875000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 351     |
|    time_elapsed    | 13842   |
|    total_timesteps | 2875392 |
--------------------------------
Eval num_timesteps=2880000, episode_reward=-413.92 +/- 183.30
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -414        |
| time/                   |             |
|    total_timesteps      | 2880000     |
| train/                  |             |
|    approx_kl            | 0.002061906 |
|    clip_fraction        | 0.001       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.6        |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.102       |
|    n_updates            | 3510        |
|    policy_gradient_loss | -0.000837   |
|    std                  | 2.18        |
|    value_loss           | 93          |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 352     |
|    time_elapsed    | 13881   |
|    total_timesteps | 2883584 |
--------------------------------
Eval num_timesteps=2885000, episode_reward=-329.35 +/- 92.73
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -329        |
| time/                   |             |
|    total_timesteps      | 2885000     |
| train/                  |             |
|    approx_kl            | 0.004666918 |
|    clip_fraction        | 0.021       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.61       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0826      |
|    n_updates            | 3520        |
|    policy_gradient_loss | -0.00101    |
|    std                  | 2.2         |
|    value_loss           | 10.7        |
-----------------------------------------
Eval num_timesteps=2890000, episode_reward=-342.91 +/- 181.22
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -343     |
| time/              |          |
|    total_timesteps | 2890000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 353     |
|    time_elapsed    | 13928   |
|    total_timesteps | 2891776 |
--------------------------------
Eval num_timesteps=2895000, episode_reward=-454.73 +/- 205.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -455         |
| time/                   |              |
|    total_timesteps      | 2895000      |
| train/                  |              |
|    approx_kl            | 0.0030212887 |
|    clip_fraction        | 0.00789      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.62        |
|    explained_variance   | 0.981        |
|    learning_rate        | 5e-05        |
|    loss                 | 17.2         |
|    n_updates            | 3530         |
|    policy_gradient_loss | -0.00101     |
|    std                  | 2.2          |
|    value_loss           | 65.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 354     |
|    time_elapsed    | 13967   |
|    total_timesteps | 2899968 |
--------------------------------
Eval num_timesteps=2900000, episode_reward=-194.09 +/- 118.04
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -194        |
| time/                   |             |
|    total_timesteps      | 2900000     |
| train/                  |             |
|    approx_kl            | 0.004322312 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.63       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.062       |
|    n_updates            | 3540        |
|    policy_gradient_loss | -0.000812   |
|    std                  | 2.21        |
|    value_loss           | 64.9        |
-----------------------------------------
Eval num_timesteps=2905000, episode_reward=-362.16 +/- 148.24
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -362     |
| time/              |          |
|    total_timesteps | 2905000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 355     |
|    time_elapsed    | 14014   |
|    total_timesteps | 2908160 |
--------------------------------
Eval num_timesteps=2910000, episode_reward=-309.18 +/- 145.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -309         |
| time/                   |              |
|    total_timesteps      | 2910000      |
| train/                  |              |
|    approx_kl            | 0.0051549203 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.64        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.0338      |
|    n_updates            | 3550         |
|    policy_gradient_loss | -0.00162     |
|    std                  | 2.22         |
|    value_loss           | 5.98         |
------------------------------------------
Eval num_timesteps=2915000, episode_reward=-348.56 +/- 130.20
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -349     |
| time/              |          |
|    total_timesteps | 2915000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 356     |
|    time_elapsed    | 14061   |
|    total_timesteps | 2916352 |
--------------------------------
Eval num_timesteps=2920000, episode_reward=-431.73 +/- 111.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -432        |
| time/                   |             |
|    total_timesteps      | 2920000     |
| train/                  |             |
|    approx_kl            | 0.003833984 |
|    clip_fraction        | 0.0158      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.65       |
|    explained_variance   | 0.992       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0661      |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00123    |
|    std                  | 2.23        |
|    value_loss           | 112         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 357     |
|    time_elapsed    | 14100   |
|    total_timesteps | 2924544 |
--------------------------------
Eval num_timesteps=2925000, episode_reward=-307.05 +/- 145.32
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -307        |
| time/                   |             |
|    total_timesteps      | 2925000     |
| train/                  |             |
|    approx_kl            | 0.005088388 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.66       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 14.9        |
|    n_updates            | 3570        |
|    policy_gradient_loss | -0.00205    |
|    std                  | 2.23        |
|    value_loss           | 120         |
-----------------------------------------
Eval num_timesteps=2930000, episode_reward=-318.78 +/- 175.58
Episode length: 803.00 +/- 396.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | -319     |
| time/              |          |
|    total_timesteps | 2930000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 358     |
|    time_elapsed    | 14145   |
|    total_timesteps | 2932736 |
--------------------------------
Eval num_timesteps=2935000, episode_reward=-330.25 +/- 180.79
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -330        |
| time/                   |             |
|    total_timesteps      | 2935000     |
| train/                  |             |
|    approx_kl            | 0.005217771 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.67       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0472      |
|    n_updates            | 3580        |
|    policy_gradient_loss | -0.0028     |
|    std                  | 2.24        |
|    value_loss           | 149         |
-----------------------------------------
Eval num_timesteps=2940000, episode_reward=-308.82 +/- 150.29
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 2940000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 359     |
|    time_elapsed    | 14192   |
|    total_timesteps | 2940928 |
--------------------------------
Eval num_timesteps=2945000, episode_reward=-216.09 +/- 250.42
Episode length: 625.40 +/- 460.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 625          |
|    mean_reward          | -216         |
| time/                   |              |
|    total_timesteps      | 2945000      |
| train/                  |              |
|    approx_kl            | 0.0027817413 |
|    clip_fraction        | 0.00413      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.68        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.183        |
|    n_updates            | 3590         |
|    policy_gradient_loss | -0.00057     |
|    std                  | 2.24         |
|    value_loss           | 146          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 360     |
|    time_elapsed    | 14229   |
|    total_timesteps | 2949120 |
--------------------------------
Eval num_timesteps=2950000, episode_reward=-253.92 +/- 183.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -254        |
| time/                   |             |
|    total_timesteps      | 2950000     |
| train/                  |             |
|    approx_kl            | 0.005013966 |
|    clip_fraction        | 0.0282      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.992       |
|    learning_rate        | 5e-05       |
|    loss                 | 13.8        |
|    n_updates            | 3600        |
|    policy_gradient_loss | -0.00134    |
|    std                  | 2.26        |
|    value_loss           | 191         |
-----------------------------------------
Eval num_timesteps=2955000, episode_reward=-297.91 +/- 105.83
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -298     |
| time/              |          |
|    total_timesteps | 2955000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 361     |
|    time_elapsed    | 14275   |
|    total_timesteps | 2957312 |
--------------------------------
Eval num_timesteps=2960000, episode_reward=-277.28 +/- 130.33
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -277        |
| time/                   |             |
|    total_timesteps      | 2960000     |
| train/                  |             |
|    approx_kl            | 0.005134996 |
|    clip_fraction        | 0.0213      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.7        |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.76        |
|    n_updates            | 3610        |
|    policy_gradient_loss | -0.00186    |
|    std                  | 2.26        |
|    value_loss           | 9.08        |
-----------------------------------------
Eval num_timesteps=2965000, episode_reward=-151.62 +/- 30.01
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 2965000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 362     |
|    time_elapsed    | 14322   |
|    total_timesteps | 2965504 |
--------------------------------
Eval num_timesteps=2970000, episode_reward=-488.72 +/- 127.65
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -489         |
| time/                   |              |
|    total_timesteps      | 2970000      |
| train/                  |              |
|    approx_kl            | 0.0027146395 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.7         |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.58         |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.000896    |
|    std                  | 2.26         |
|    value_loss           | 181          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 363     |
|    time_elapsed    | 14360   |
|    total_timesteps | 2973696 |
--------------------------------
Eval num_timesteps=2975000, episode_reward=-255.70 +/- 125.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -256         |
| time/                   |              |
|    total_timesteps      | 2975000      |
| train/                  |              |
|    approx_kl            | 0.0035767998 |
|    clip_fraction        | 0.00928      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.7         |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 84.2         |
|    n_updates            | 3630         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 2.26         |
|    value_loss           | 112          |
------------------------------------------
Eval num_timesteps=2980000, episode_reward=-292.25 +/- 96.63
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 2980000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 364     |
|    time_elapsed    | 14406   |
|    total_timesteps | 2981888 |
--------------------------------
Eval num_timesteps=2985000, episode_reward=-208.54 +/- 144.80
Episode length: 807.00 +/- 388.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 807          |
|    mean_reward          | -209         |
| time/                   |              |
|    total_timesteps      | 2985000      |
| train/                  |              |
|    approx_kl            | 0.0043243556 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.7         |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.312        |
|    n_updates            | 3640         |
|    policy_gradient_loss | -0.00295     |
|    std                  | 2.26         |
|    value_loss           | 27.1         |
------------------------------------------
Eval num_timesteps=2990000, episode_reward=-398.15 +/- 167.87
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 2990000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 365     |
|    time_elapsed    | 14452   |
|    total_timesteps | 2990080 |
--------------------------------
Eval num_timesteps=2995000, episode_reward=-240.27 +/- 46.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -240        |
| time/                   |             |
|    total_timesteps      | 2995000     |
| train/                  |             |
|    approx_kl            | 0.004355468 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.7        |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 3.59        |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 2.26        |
|    value_loss           | 4.9         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 366     |
|    time_elapsed    | 14490   |
|    total_timesteps | 2998272 |
--------------------------------
Eval num_timesteps=3000000, episode_reward=-425.55 +/- 240.75
Episode length: 804.40 +/- 393.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 804          |
|    mean_reward          | -426         |
| time/                   |              |
|    total_timesteps      | 3000000      |
| train/                  |              |
|    approx_kl            | 0.0030761508 |
|    clip_fraction        | 0.00714      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.71        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.458        |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.000934    |
|    std                  | 2.27         |
|    value_loss           | 112          |
------------------------------------------
Eval num_timesteps=3005000, episode_reward=-256.75 +/- 117.96
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 3005000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 367     |
|    time_elapsed    | 14535   |
|    total_timesteps | 3006464 |
--------------------------------
Eval num_timesteps=3010000, episode_reward=-419.57 +/- 163.08
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 3010000     |
| train/                  |             |
|    approx_kl            | 0.002820896 |
|    clip_fraction        | 0.00811     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.71       |
|    explained_variance   | 0.986       |
|    learning_rate        | 5e-05       |
|    loss                 | 6.07        |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.00146    |
|    std                  | 2.27        |
|    value_loss           | 136         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 368     |
|    time_elapsed    | 14575   |
|    total_timesteps | 3014656 |
--------------------------------
Eval num_timesteps=3015000, episode_reward=-295.49 +/- 125.69
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -295         |
| time/                   |              |
|    total_timesteps      | 3015000      |
| train/                  |              |
|    approx_kl            | 0.0034528454 |
|    clip_fraction        | 0.00367      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.71        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0285       |
|    n_updates            | 3680         |
|    policy_gradient_loss | -0.000647    |
|    std                  | 2.26         |
|    value_loss           | 47.4         |
------------------------------------------
Eval num_timesteps=3020000, episode_reward=-395.94 +/- 179.82
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 3020000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 369     |
|    time_elapsed    | 14621   |
|    total_timesteps | 3022848 |
--------------------------------
Eval num_timesteps=3025000, episode_reward=-351.73 +/- 129.78
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -352        |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.002282699 |
|    clip_fraction        | 0.00175     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.7        |
|    explained_variance   | 0.986       |
|    learning_rate        | 5e-05       |
|    loss                 | 17.3        |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.00105    |
|    std                  | 2.26        |
|    value_loss           | 291         |
-----------------------------------------
Eval num_timesteps=3030000, episode_reward=-259.09 +/- 80.97
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 3030000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 370     |
|    time_elapsed    | 14669   |
|    total_timesteps | 3031040 |
--------------------------------
Eval num_timesteps=3035000, episode_reward=-255.43 +/- 143.78
Episode length: 805.80 +/- 390.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 806          |
|    mean_reward          | -255         |
| time/                   |              |
|    total_timesteps      | 3035000      |
| train/                  |              |
|    approx_kl            | 0.0049744556 |
|    clip_fraction        | 0.0197       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.72        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.66         |
|    n_updates            | 3700         |
|    policy_gradient_loss | -0.000249    |
|    std                  | 2.28         |
|    value_loss           | 303          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 371     |
|    time_elapsed    | 14706   |
|    total_timesteps | 3039232 |
--------------------------------
Eval num_timesteps=3040000, episode_reward=-260.06 +/- 92.76
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -260        |
| time/                   |             |
|    total_timesteps      | 3040000     |
| train/                  |             |
|    approx_kl            | 0.004195937 |
|    clip_fraction        | 0.0241      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.73       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.15        |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00243    |
|    std                  | 2.29        |
|    value_loss           | 45.2        |
-----------------------------------------
Eval num_timesteps=3045000, episode_reward=-352.86 +/- 144.14
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -353     |
| time/              |          |
|    total_timesteps | 3045000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 372     |
|    time_elapsed    | 14752   |
|    total_timesteps | 3047424 |
--------------------------------
Eval num_timesteps=3050000, episode_reward=-306.20 +/- 104.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -306         |
| time/                   |              |
|    total_timesteps      | 3050000      |
| train/                  |              |
|    approx_kl            | 0.0025937238 |
|    clip_fraction        | 0.00403      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.74        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.396        |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.00096     |
|    std                  | 2.29         |
|    value_loss           | 173          |
------------------------------------------
Eval num_timesteps=3055000, episode_reward=-313.14 +/- 49.80
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 3055000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 373     |
|    time_elapsed    | 14800   |
|    total_timesteps | 3055616 |
--------------------------------
Eval num_timesteps=3060000, episode_reward=-312.32 +/- 47.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -312         |
| time/                   |              |
|    total_timesteps      | 3060000      |
| train/                  |              |
|    approx_kl            | 0.0039757513 |
|    clip_fraction        | 0.0159       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.74        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.187        |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 2.29         |
|    value_loss           | 55.8         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 374     |
|    time_elapsed    | 14840   |
|    total_timesteps | 3063808 |
--------------------------------
Eval num_timesteps=3065000, episode_reward=-373.25 +/- 144.60
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -373         |
| time/                   |              |
|    total_timesteps      | 3065000      |
| train/                  |              |
|    approx_kl            | 0.0041250344 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.75        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 960          |
|    n_updates            | 3740         |
|    policy_gradient_loss | -0.000952    |
|    std                  | 2.3          |
|    value_loss           | 151          |
------------------------------------------
Eval num_timesteps=3070000, episode_reward=-276.15 +/- 136.65
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 3070000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 375     |
|    time_elapsed    | 14887   |
|    total_timesteps | 3072000 |
--------------------------------
Eval num_timesteps=3075000, episode_reward=-375.99 +/- 94.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -376         |
| time/                   |              |
|    total_timesteps      | 3075000      |
| train/                  |              |
|    approx_kl            | 0.0036855065 |
|    clip_fraction        | 0.00541      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.75        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 714          |
|    n_updates            | 3750         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 2.3          |
|    value_loss           | 139          |
------------------------------------------
Eval num_timesteps=3080000, episode_reward=-331.11 +/- 262.41
Episode length: 806.20 +/- 389.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 806      |
|    mean_reward     | -331     |
| time/              |          |
|    total_timesteps | 3080000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 376     |
|    time_elapsed    | 14934   |
|    total_timesteps | 3080192 |
--------------------------------
Eval num_timesteps=3085000, episode_reward=-497.10 +/- 171.64
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -497         |
| time/                   |              |
|    total_timesteps      | 3085000      |
| train/                  |              |
|    approx_kl            | 0.0021077842 |
|    clip_fraction        | 0.00214      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.75        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 297          |
|    n_updates            | 3760         |
|    policy_gradient_loss | -0.000222    |
|    std                  | 2.29         |
|    value_loss           | 82.5         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 377     |
|    time_elapsed    | 14972   |
|    total_timesteps | 3088384 |
--------------------------------
Eval num_timesteps=3090000, episode_reward=-325.77 +/- 162.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -326         |
| time/                   |              |
|    total_timesteps      | 3090000      |
| train/                  |              |
|    approx_kl            | 0.0044476106 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.74        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.214        |
|    n_updates            | 3770         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 2.3          |
|    value_loss           | 3.74         |
------------------------------------------
Eval num_timesteps=3095000, episode_reward=-252.31 +/- 164.70
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 3095000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 378     |
|    time_elapsed    | 15019   |
|    total_timesteps | 3096576 |
--------------------------------
Eval num_timesteps=3100000, episode_reward=-337.15 +/- 154.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -337         |
| time/                   |              |
|    total_timesteps      | 3100000      |
| train/                  |              |
|    approx_kl            | 0.0042680474 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.75        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.47         |
|    n_updates            | 3780         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 2.3          |
|    value_loss           | 121          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 379     |
|    time_elapsed    | 15059   |
|    total_timesteps | 3104768 |
--------------------------------
Eval num_timesteps=3105000, episode_reward=-334.36 +/- 123.06
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -334        |
| time/                   |             |
|    total_timesteps      | 3105000     |
| train/                  |             |
|    approx_kl            | 0.004080699 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.76       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.374       |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00155    |
|    std                  | 2.3         |
|    value_loss           | 168         |
-----------------------------------------
Eval num_timesteps=3110000, episode_reward=-313.16 +/- 76.15
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 3110000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 380     |
|    time_elapsed    | 15105   |
|    total_timesteps | 3112960 |
--------------------------------
Eval num_timesteps=3115000, episode_reward=-261.30 +/- 128.53
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -261         |
| time/                   |              |
|    total_timesteps      | 3115000      |
| train/                  |              |
|    approx_kl            | 0.0027087172 |
|    clip_fraction        | 0.00923      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 13.7         |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 2.3          |
|    value_loss           | 11.8         |
------------------------------------------
Eval num_timesteps=3120000, episode_reward=-295.48 +/- 171.58
Episode length: 810.40 +/- 381.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 3120000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 381     |
|    time_elapsed    | 15150   |
|    total_timesteps | 3121152 |
--------------------------------
Eval num_timesteps=3125000, episode_reward=-241.73 +/- 102.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -242         |
| time/                   |              |
|    total_timesteps      | 3125000      |
| train/                  |              |
|    approx_kl            | 0.0018443665 |
|    clip_fraction        | 0.00265      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 56.1         |
|    n_updates            | 3810         |
|    policy_gradient_loss | -0.000613    |
|    std                  | 2.31         |
|    value_loss           | 107          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 382     |
|    time_elapsed    | 15190   |
|    total_timesteps | 3129344 |
--------------------------------
Eval num_timesteps=3130000, episode_reward=-320.42 +/- 131.57
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -320         |
| time/                   |              |
|    total_timesteps      | 3130000      |
| train/                  |              |
|    approx_kl            | 0.0038313747 |
|    clip_fraction        | 0.0148       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.76        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 48.9         |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 2.31         |
|    value_loss           | 149          |
------------------------------------------
Eval num_timesteps=3135000, episode_reward=-414.95 +/- 190.97
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 3135000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 383     |
|    time_elapsed    | 15236   |
|    total_timesteps | 3137536 |
--------------------------------
Eval num_timesteps=3140000, episode_reward=-474.82 +/- 139.23
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -475        |
| time/                   |             |
|    total_timesteps      | 3140000     |
| train/                  |             |
|    approx_kl            | 0.003955514 |
|    clip_fraction        | 0.0173      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.77       |
|    explained_variance   | 0.986       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0728      |
|    n_updates            | 3830        |
|    policy_gradient_loss | -0.00105    |
|    std                  | 2.32        |
|    value_loss           | 194         |
-----------------------------------------
Eval num_timesteps=3145000, episode_reward=-349.78 +/- 111.27
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 3145000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 384     |
|    time_elapsed    | 15283   |
|    total_timesteps | 3145728 |
--------------------------------
Eval num_timesteps=3150000, episode_reward=-279.77 +/- 149.25
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -280        |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.003865762 |
|    clip_fraction        | 0.00867     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.78       |
|    explained_variance   | 0.99        |
|    learning_rate        | 5e-05       |
|    loss                 | 0.276       |
|    n_updates            | 3840        |
|    policy_gradient_loss | -0.000203   |
|    std                  | 2.32        |
|    value_loss           | 214         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 385     |
|    time_elapsed    | 15322   |
|    total_timesteps | 3153920 |
--------------------------------
Eval num_timesteps=3155000, episode_reward=-281.49 +/- 162.93
Episode length: 810.60 +/- 380.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -281        |
| time/                   |             |
|    total_timesteps      | 3155000     |
| train/                  |             |
|    approx_kl            | 0.004270087 |
|    clip_fraction        | 0.016       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.79       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.196       |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00111    |
|    std                  | 2.33        |
|    value_loss           | 58.1        |
-----------------------------------------
Eval num_timesteps=3160000, episode_reward=-414.62 +/- 137.82
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 3160000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 386     |
|    time_elapsed    | 15368   |
|    total_timesteps | 3162112 |
--------------------------------
Eval num_timesteps=3165000, episode_reward=-299.15 +/- 78.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -299         |
| time/                   |              |
|    total_timesteps      | 3165000      |
| train/                  |              |
|    approx_kl            | 0.0032934197 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.8         |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0642       |
|    n_updates            | 3860         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 2.34         |
|    value_loss           | 97.1         |
------------------------------------------
Eval num_timesteps=3170000, episode_reward=-412.59 +/- 203.62
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 3170000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 387     |
|    time_elapsed    | 15415   |
|    total_timesteps | 3170304 |
--------------------------------
Eval num_timesteps=3175000, episode_reward=-347.14 +/- 157.65
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -347         |
| time/                   |              |
|    total_timesteps      | 3175000      |
| train/                  |              |
|    approx_kl            | 0.0039827432 |
|    clip_fraction        | 0.0121       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.8         |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.575        |
|    n_updates            | 3870         |
|    policy_gradient_loss | -0.000475    |
|    std                  | 2.34         |
|    value_loss           | 71.6         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 388     |
|    time_elapsed    | 15454   |
|    total_timesteps | 3178496 |
--------------------------------
Eval num_timesteps=3180000, episode_reward=-310.90 +/- 107.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -311         |
| time/                   |              |
|    total_timesteps      | 3180000      |
| train/                  |              |
|    approx_kl            | 0.0031617088 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.8         |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0289       |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 2.34         |
|    value_loss           | 0.314        |
------------------------------------------
Eval num_timesteps=3185000, episode_reward=-410.39 +/- 136.60
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -410     |
| time/              |          |
|    total_timesteps | 3185000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 389     |
|    time_elapsed    | 15501   |
|    total_timesteps | 3186688 |
--------------------------------
Eval num_timesteps=3190000, episode_reward=-394.33 +/- 87.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -394         |
| time/                   |              |
|    total_timesteps      | 3190000      |
| train/                  |              |
|    approx_kl            | 0.0015980152 |
|    clip_fraction        | 0.00179      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.8         |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 38.2         |
|    n_updates            | 3890         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 2.34         |
|    value_loss           | 106          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 390     |
|    time_elapsed    | 15541   |
|    total_timesteps | 3194880 |
--------------------------------
Eval num_timesteps=3195000, episode_reward=-320.75 +/- 162.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -321         |
| time/                   |              |
|    total_timesteps      | 3195000      |
| train/                  |              |
|    approx_kl            | 0.0039072824 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.81        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.13         |
|    n_updates            | 3900         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 2.35         |
|    value_loss           | 0.688        |
------------------------------------------
Eval num_timesteps=3200000, episode_reward=-383.01 +/- 168.54
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 3200000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 391     |
|    time_elapsed    | 15589   |
|    total_timesteps | 3203072 |
--------------------------------
Eval num_timesteps=3205000, episode_reward=-299.91 +/- 64.84
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 3205000      |
| train/                  |              |
|    approx_kl            | 0.0027514843 |
|    clip_fraction        | 0.00541      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.82        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.76         |
|    n_updates            | 3910         |
|    policy_gradient_loss | -0.000694    |
|    std                  | 2.35         |
|    value_loss           | 85.7         |
------------------------------------------
Eval num_timesteps=3210000, episode_reward=-376.60 +/- 187.26
Episode length: 813.40 +/- 375.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 3210000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 392     |
|    time_elapsed    | 15635   |
|    total_timesteps | 3211264 |
--------------------------------
Eval num_timesteps=3215000, episode_reward=-294.74 +/- 124.01
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -295        |
| time/                   |             |
|    total_timesteps      | 3215000     |
| train/                  |             |
|    approx_kl            | 0.001451456 |
|    clip_fraction        | 0.00272     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.82       |
|    explained_variance   | 0.99        |
|    learning_rate        | 5e-05       |
|    loss                 | 0.453       |
|    n_updates            | 3920        |
|    policy_gradient_loss | -0.000908   |
|    std                  | 2.35        |
|    value_loss           | 266         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 393     |
|    time_elapsed    | 15674   |
|    total_timesteps | 3219456 |
--------------------------------
Eval num_timesteps=3220000, episode_reward=-286.18 +/- 145.67
Episode length: 804.80 +/- 392.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 805          |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 3220000      |
| train/                  |              |
|    approx_kl            | 0.0057909656 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.83        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.284        |
|    n_updates            | 3930         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 2.37         |
|    value_loss           | 2.47         |
------------------------------------------
Eval num_timesteps=3225000, episode_reward=-282.98 +/- 58.13
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 3225000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 394     |
|    time_elapsed    | 15719   |
|    total_timesteps | 3227648 |
--------------------------------
Eval num_timesteps=3230000, episode_reward=-343.68 +/- 60.69
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -344         |
| time/                   |              |
|    total_timesteps      | 3230000      |
| train/                  |              |
|    approx_kl            | 0.0031061792 |
|    clip_fraction        | 0.00767      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.85        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.352        |
|    n_updates            | 3940         |
|    policy_gradient_loss | -0.00133     |
|    std                  | 2.38         |
|    value_loss           | 145          |
------------------------------------------
Eval num_timesteps=3235000, episode_reward=-235.67 +/- 163.78
Episode length: 810.40 +/- 381.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 3235000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 395     |
|    time_elapsed    | 15765   |
|    total_timesteps | 3235840 |
--------------------------------
Eval num_timesteps=3240000, episode_reward=-279.90 +/- 246.49
Episode length: 807.40 +/- 387.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 807          |
|    mean_reward          | -280         |
| time/                   |              |
|    total_timesteps      | 3240000      |
| train/                  |              |
|    approx_kl            | 0.0018634934 |
|    clip_fraction        | 0.00233      |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.86        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.774        |
|    n_updates            | 3950         |
|    policy_gradient_loss | -0.000799    |
|    std                  | 2.39         |
|    value_loss           | 163          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 396     |
|    time_elapsed    | 15803   |
|    total_timesteps | 3244032 |
--------------------------------
Eval num_timesteps=3245000, episode_reward=-365.53 +/- 127.37
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -366        |
| time/                   |             |
|    total_timesteps      | 3245000     |
| train/                  |             |
|    approx_kl            | 0.003906264 |
|    clip_fraction        | 0.0195      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.87       |
|    explained_variance   | 0.892       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.187       |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00106    |
|    std                  | 2.39        |
|    value_loss           | 94          |
-----------------------------------------
Eval num_timesteps=3250000, episode_reward=-316.18 +/- 145.65
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 3250000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 397     |
|    time_elapsed    | 15850   |
|    total_timesteps | 3252224 |
--------------------------------
Eval num_timesteps=3255000, episode_reward=-347.03 +/- 116.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -347        |
| time/                   |             |
|    total_timesteps      | 3255000     |
| train/                  |             |
|    approx_kl            | 0.004442682 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.87       |
|    explained_variance   | 0.996       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.205       |
|    n_updates            | 3970        |
|    policy_gradient_loss | -0.00125    |
|    std                  | 2.4         |
|    value_loss           | 57.8        |
-----------------------------------------
Eval num_timesteps=3260000, episode_reward=-329.26 +/- 180.31
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 3260000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 398     |
|    time_elapsed    | 15899   |
|    total_timesteps | 3260416 |
--------------------------------
Eval num_timesteps=3265000, episode_reward=-483.80 +/- 113.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -484         |
| time/                   |              |
|    total_timesteps      | 3265000      |
| train/                  |              |
|    approx_kl            | 0.0027241092 |
|    clip_fraction        | 0.0054       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.87        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 42.4         |
|    n_updates            | 3980         |
|    policy_gradient_loss | -0.000663    |
|    std                  | 2.39         |
|    value_loss           | 110          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 399     |
|    time_elapsed    | 15937   |
|    total_timesteps | 3268608 |
--------------------------------
Eval num_timesteps=3270000, episode_reward=-335.53 +/- 185.90
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -336         |
| time/                   |              |
|    total_timesteps      | 3270000      |
| train/                  |              |
|    approx_kl            | 0.0041839825 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.87        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0816       |
|    n_updates            | 3990         |
|    policy_gradient_loss | -0.0016      |
|    std                  | 2.38         |
|    value_loss           | 46.7         |
------------------------------------------
Eval num_timesteps=3275000, episode_reward=-308.60 +/- 175.69
Episode length: 811.20 +/- 379.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 811      |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 3275000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 400     |
|    time_elapsed    | 15983   |
|    total_timesteps | 3276800 |
--------------------------------
Eval num_timesteps=3280000, episode_reward=-494.41 +/- 105.10
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -494        |
| time/                   |             |
|    total_timesteps      | 3280000     |
| train/                  |             |
|    approx_kl            | 0.002262653 |
|    clip_fraction        | 0.00176     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.86       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.263       |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.000603   |
|    std                  | 2.39        |
|    value_loss           | 89.1        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 401     |
|    time_elapsed    | 16024   |
|    total_timesteps | 3284992 |
--------------------------------
Eval num_timesteps=3285000, episode_reward=-347.55 +/- 177.10
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -348         |
| time/                   |              |
|    total_timesteps      | 3285000      |
| train/                  |              |
|    approx_kl            | 0.0039641066 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.87        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 8.02         |
|    n_updates            | 4010         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 2.4          |
|    value_loss           | 45.8         |
------------------------------------------
Eval num_timesteps=3290000, episode_reward=-263.12 +/- 206.45
Episode length: 809.60 +/- 382.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 3290000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 402     |
|    time_elapsed    | 16070   |
|    total_timesteps | 3293184 |
--------------------------------
Eval num_timesteps=3295000, episode_reward=-477.58 +/- 50.12
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -478         |
| time/                   |              |
|    total_timesteps      | 3295000      |
| train/                  |              |
|    approx_kl            | 0.0035061203 |
|    clip_fraction        | 0.0124       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.88        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 265          |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.000346    |
|    std                  | 2.41         |
|    value_loss           | 152          |
------------------------------------------
Eval num_timesteps=3300000, episode_reward=-341.60 +/- 97.97
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -342     |
| time/              |          |
|    total_timesteps | 3300000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 403     |
|    time_elapsed    | 16117   |
|    total_timesteps | 3301376 |
--------------------------------
Eval num_timesteps=3305000, episode_reward=-223.62 +/- 110.91
Episode length: 813.80 +/- 374.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -224         |
| time/                   |              |
|    total_timesteps      | 3305000      |
| train/                  |              |
|    approx_kl            | 0.0005041644 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.89        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.72         |
|    n_updates            | 4030         |
|    policy_gradient_loss | 8.27e-05     |
|    std                  | 2.41         |
|    value_loss           | 222          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 404     |
|    time_elapsed    | 16154   |
|    total_timesteps | 3309568 |
--------------------------------
Eval num_timesteps=3310000, episode_reward=-251.92 +/- 151.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -252         |
| time/                   |              |
|    total_timesteps      | 3310000      |
| train/                  |              |
|    approx_kl            | 0.0035888131 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.89        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0278       |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 2.4          |
|    value_loss           | 10.3         |
------------------------------------------
Eval num_timesteps=3315000, episode_reward=-370.90 +/- 154.68
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 3315000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 405     |
|    time_elapsed    | 16201   |
|    total_timesteps | 3317760 |
--------------------------------
Eval num_timesteps=3320000, episode_reward=-318.76 +/- 139.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -319        |
| time/                   |             |
|    total_timesteps      | 3320000     |
| train/                  |             |
|    approx_kl            | 0.003109543 |
|    clip_fraction        | 0.00908     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.88       |
|    explained_variance   | 0.991       |
|    learning_rate        | 5e-05       |
|    loss                 | 80.6        |
|    n_updates            | 4050        |
|    policy_gradient_loss | -0.00112    |
|    std                  | 2.4         |
|    value_loss           | 109         |
-----------------------------------------
Eval num_timesteps=3325000, episode_reward=-445.90 +/- 161.37
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -446     |
| time/              |          |
|    total_timesteps | 3325000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 406     |
|    time_elapsed    | 16249   |
|    total_timesteps | 3325952 |
--------------------------------
Eval num_timesteps=3330000, episode_reward=-397.88 +/- 120.89
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -398        |
| time/                   |             |
|    total_timesteps      | 3330000     |
| train/                  |             |
|    approx_kl            | 0.004177283 |
|    clip_fraction        | 0.0106      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.88       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.491       |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.00114    |
|    std                  | 2.4         |
|    value_loss           | 155         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 407     |
|    time_elapsed    | 16289   |
|    total_timesteps | 3334144 |
--------------------------------
Eval num_timesteps=3335000, episode_reward=-303.60 +/- 166.13
Episode length: 807.60 +/- 386.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 808         |
|    mean_reward          | -304        |
| time/                   |             |
|    total_timesteps      | 3335000     |
| train/                  |             |
|    approx_kl            | 0.004798982 |
|    clip_fraction        | 0.0149      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.88       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.161       |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.00135    |
|    std                  | 2.41        |
|    value_loss           | 126         |
-----------------------------------------
Eval num_timesteps=3340000, episode_reward=-302.37 +/- 161.08
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 3340000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 408     |
|    time_elapsed    | 16334   |
|    total_timesteps | 3342336 |
--------------------------------
Eval num_timesteps=3345000, episode_reward=-337.18 +/- 107.31
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -337        |
| time/                   |             |
|    total_timesteps      | 3345000     |
| train/                  |             |
|    approx_kl            | 0.004181466 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.89       |
|    explained_variance   | 0.992       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.715       |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.00172    |
|    std                  | 2.41        |
|    value_loss           | 200         |
-----------------------------------------
Eval num_timesteps=3350000, episode_reward=-254.53 +/- 60.02
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 3350000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 409     |
|    time_elapsed    | 16383   |
|    total_timesteps | 3350528 |
--------------------------------
Eval num_timesteps=3355000, episode_reward=-533.21 +/- 73.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -533         |
| time/                   |              |
|    total_timesteps      | 3355000      |
| train/                  |              |
|    approx_kl            | 0.0048368424 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.89        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.172        |
|    n_updates            | 4090         |
|    policy_gradient_loss | -0.0032      |
|    std                  | 2.4          |
|    value_loss           | 1.88         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 410     |
|    time_elapsed    | 16423   |
|    total_timesteps | 3358720 |
--------------------------------
Eval num_timesteps=3360000, episode_reward=-193.79 +/- 32.74
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -194          |
| time/                   |               |
|    total_timesteps      | 3360000       |
| train/                  |               |
|    approx_kl            | 0.00040967224 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -6.89         |
|    explained_variance   | 0.991         |
|    learning_rate        | 5e-05         |
|    loss                 | 1.79          |
|    n_updates            | 4100          |
|    policy_gradient_loss | -7.78e-05     |
|    std                  | 2.41          |
|    value_loss           | 220           |
-------------------------------------------
Eval num_timesteps=3365000, episode_reward=-327.58 +/- 191.01
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -328     |
| time/              |          |
|    total_timesteps | 3365000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 411     |
|    time_elapsed    | 16471   |
|    total_timesteps | 3366912 |
--------------------------------
Eval num_timesteps=3370000, episode_reward=-485.67 +/- 62.05
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -486        |
| time/                   |             |
|    total_timesteps      | 3370000     |
| train/                  |             |
|    approx_kl            | 0.002985876 |
|    clip_fraction        | 0.00735     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.89       |
|    explained_variance   | 0.992       |
|    learning_rate        | 5e-05       |
|    loss                 | 9.57        |
|    n_updates            | 4110        |
|    policy_gradient_loss | -0.00113    |
|    std                  | 2.4         |
|    value_loss           | 125         |
-----------------------------------------
Eval num_timesteps=3375000, episode_reward=-248.13 +/- 168.92
Episode length: 807.60 +/- 386.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 3375000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 412     |
|    time_elapsed    | 16519   |
|    total_timesteps | 3375104 |
--------------------------------
Eval num_timesteps=3380000, episode_reward=-228.84 +/- 114.39
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -229         |
| time/                   |              |
|    total_timesteps      | 3380000      |
| train/                  |              |
|    approx_kl            | 0.0057887877 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.89        |
|    explained_variance   | 0.985        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.491        |
|    n_updates            | 4120         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 2.41         |
|    value_loss           | 322          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 413     |
|    time_elapsed    | 16558   |
|    total_timesteps | 3383296 |
--------------------------------
Eval num_timesteps=3385000, episode_reward=-218.34 +/- 90.81
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -218        |
| time/                   |             |
|    total_timesteps      | 3385000     |
| train/                  |             |
|    approx_kl            | 0.003381024 |
|    clip_fraction        | 0.0122      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.9        |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | -0.00406    |
|    n_updates            | 4130        |
|    policy_gradient_loss | -0.00104    |
|    std                  | 2.41        |
|    value_loss           | 25.4        |
-----------------------------------------
Eval num_timesteps=3390000, episode_reward=-325.50 +/- 141.09
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -325     |
| time/              |          |
|    total_timesteps | 3390000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 414     |
|    time_elapsed    | 16605   |
|    total_timesteps | 3391488 |
--------------------------------
Eval num_timesteps=3395000, episode_reward=-431.34 +/- 121.39
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -431         |
| time/                   |              |
|    total_timesteps      | 3395000      |
| train/                  |              |
|    approx_kl            | 0.0013187099 |
|    clip_fraction        | 0.000647     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.9         |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 215          |
|    n_updates            | 4140         |
|    policy_gradient_loss | -0.000535    |
|    std                  | 2.42         |
|    value_loss           | 176          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 415     |
|    time_elapsed    | 16644   |
|    total_timesteps | 3399680 |
--------------------------------
Eval num_timesteps=3400000, episode_reward=-348.76 +/- 140.10
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -349         |
| time/                   |              |
|    total_timesteps      | 3400000      |
| train/                  |              |
|    approx_kl            | 0.0041914308 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.91        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | -0.0181      |
|    n_updates            | 4150         |
|    policy_gradient_loss | -0.00207     |
|    std                  | 2.43         |
|    value_loss           | 0.748        |
------------------------------------------
Eval num_timesteps=3405000, episode_reward=-339.95 +/- 99.36
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 3405000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 416     |
|    time_elapsed    | 16691   |
|    total_timesteps | 3407872 |
--------------------------------
Eval num_timesteps=3410000, episode_reward=-250.52 +/- 124.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -251         |
| time/                   |              |
|    total_timesteps      | 3410000      |
| train/                  |              |
|    approx_kl            | 0.0005824438 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.92        |
|    explained_variance   | 0.984        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.85         |
|    n_updates            | 4160         |
|    policy_gradient_loss | -0.000502    |
|    std                  | 2.43         |
|    value_loss           | 293          |
------------------------------------------
Eval num_timesteps=3415000, episode_reward=-312.26 +/- 171.76
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 3415000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 417     |
|    time_elapsed    | 16740   |
|    total_timesteps | 3416064 |
--------------------------------
Eval num_timesteps=3420000, episode_reward=-379.98 +/- 210.31
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -380        |
| time/                   |             |
|    total_timesteps      | 3420000     |
| train/                  |             |
|    approx_kl            | 0.004514392 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.92       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.00235     |
|    n_updates            | 4170        |
|    policy_gradient_loss | -0.00162    |
|    std                  | 2.43        |
|    value_loss           | 0.454       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 418     |
|    time_elapsed    | 16779   |
|    total_timesteps | 3424256 |
--------------------------------
Eval num_timesteps=3425000, episode_reward=-414.47 +/- 137.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 3425000      |
| train/                  |              |
|    approx_kl            | 0.0041380557 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.93        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | -0.0344      |
|    n_updates            | 4180         |
|    policy_gradient_loss | -0.000971    |
|    std                  | 2.44         |
|    value_loss           | 2.26         |
------------------------------------------
Eval num_timesteps=3430000, episode_reward=-413.45 +/- 188.71
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 3430000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 419     |
|    time_elapsed    | 16827   |
|    total_timesteps | 3432448 |
--------------------------------
Eval num_timesteps=3435000, episode_reward=-407.11 +/- 183.39
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -407         |
| time/                   |              |
|    total_timesteps      | 3435000      |
| train/                  |              |
|    approx_kl            | 0.0011124208 |
|    clip_fraction        | 7.32e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.93        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.622        |
|    n_updates            | 4190         |
|    policy_gradient_loss | -0.000307    |
|    std                  | 2.44         |
|    value_loss           | 122          |
------------------------------------------
Eval num_timesteps=3440000, episode_reward=-407.53 +/- 220.73
Episode length: 803.00 +/- 396.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 803      |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 3440000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 420     |
|    time_elapsed    | 16875   |
|    total_timesteps | 3440640 |
--------------------------------
Eval num_timesteps=3445000, episode_reward=-264.56 +/- 47.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -265         |
| time/                   |              |
|    total_timesteps      | 3445000      |
| train/                  |              |
|    approx_kl            | 0.0018323574 |
|    clip_fraction        | 0.000464     |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.93        |
|    explained_variance   | 0.987        |
|    learning_rate        | 5e-05        |
|    loss                 | 23.3         |
|    n_updates            | 4200         |
|    policy_gradient_loss | -0.000715    |
|    std                  | 2.44         |
|    value_loss           | 333          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 421     |
|    time_elapsed    | 16913   |
|    total_timesteps | 3448832 |
--------------------------------
Eval num_timesteps=3450000, episode_reward=-264.54 +/- 214.85
Episode length: 810.60 +/- 380.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -265        |
| time/                   |             |
|    total_timesteps      | 3450000     |
| train/                  |             |
|    approx_kl            | 0.003631647 |
|    clip_fraction        | 0.0161      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.93       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.238       |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00137    |
|    std                  | 2.44        |
|    value_loss           | 52.4        |
-----------------------------------------
Eval num_timesteps=3455000, episode_reward=-354.83 +/- 217.36
Episode length: 807.40 +/- 387.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | -355     |
| time/              |          |
|    total_timesteps | 3455000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 422     |
|    time_elapsed    | 16958   |
|    total_timesteps | 3457024 |
--------------------------------
Eval num_timesteps=3460000, episode_reward=-444.42 +/- 198.07
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -444       |
| time/                   |            |
|    total_timesteps      | 3460000    |
| train/                  |            |
|    approx_kl            | 0.00408661 |
|    clip_fraction        | 0.0111     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.93      |
|    explained_variance   | 0.991      |
|    learning_rate        | 5e-05      |
|    loss                 | 3.13       |
|    n_updates            | 4220       |
|    policy_gradient_loss | -0.00214   |
|    std                  | 2.44       |
|    value_loss           | 188        |
----------------------------------------
Eval num_timesteps=3465000, episode_reward=-297.15 +/- 128.29
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 3465000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 423     |
|    time_elapsed    | 17007   |
|    total_timesteps | 3465216 |
--------------------------------
Eval num_timesteps=3470000, episode_reward=-249.88 +/- 164.50
Episode length: 807.40 +/- 387.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 807          |
|    mean_reward          | -250         |
| time/                   |              |
|    total_timesteps      | 3470000      |
| train/                  |              |
|    approx_kl            | 0.0039153406 |
|    clip_fraction        | 0.0189       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.94        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.133        |
|    n_updates            | 4230         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 2.45         |
|    value_loss           | 51           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 424     |
|    time_elapsed    | 17044   |
|    total_timesteps | 3473408 |
--------------------------------
Eval num_timesteps=3475000, episode_reward=-323.54 +/- 43.16
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 3475000      |
| train/                  |              |
|    approx_kl            | 0.0039787777 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.95        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0148       |
|    n_updates            | 4240         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 2.46         |
|    value_loss           | 27.5         |
------------------------------------------
Eval num_timesteps=3480000, episode_reward=-352.41 +/- 119.01
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -352     |
| time/              |          |
|    total_timesteps | 3480000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 425     |
|    time_elapsed    | 17091   |
|    total_timesteps | 3481600 |
--------------------------------
Eval num_timesteps=3485000, episode_reward=-185.10 +/- 70.86
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -185         |
| time/                   |              |
|    total_timesteps      | 3485000      |
| train/                  |              |
|    approx_kl            | 0.0051432312 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.96        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 5.21         |
|    n_updates            | 4250         |
|    policy_gradient_loss | -0.00271     |
|    std                  | 2.46         |
|    value_loss           | 217          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 426     |
|    time_elapsed    | 17131   |
|    total_timesteps | 3489792 |
--------------------------------
Eval num_timesteps=3490000, episode_reward=-300.13 +/- 183.40
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 3490000     |
| train/                  |             |
|    approx_kl            | 0.004574289 |
|    clip_fraction        | 0.0323      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.96       |
|    explained_variance   | 0.989       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.138       |
|    n_updates            | 4260        |
|    policy_gradient_loss | -0.00208    |
|    std                  | 2.46        |
|    value_loss           | 242         |
-----------------------------------------
Eval num_timesteps=3495000, episode_reward=-340.43 +/- 108.92
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 3495000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 427     |
|    time_elapsed    | 17179   |
|    total_timesteps | 3497984 |
--------------------------------
Eval num_timesteps=3500000, episode_reward=-308.77 +/- 88.40
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -309       |
| time/                   |            |
|    total_timesteps      | 3500000    |
| train/                  |            |
|    approx_kl            | 0.00117589 |
|    clip_fraction        | 9.77e-05   |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.96      |
|    explained_variance   | 0.99       |
|    learning_rate        | 5e-05      |
|    loss                 | 7.02       |
|    n_updates            | 4270       |
|    policy_gradient_loss | -0.000421  |
|    std                  | 2.46       |
|    value_loss           | 231        |
----------------------------------------
Eval num_timesteps=3505000, episode_reward=-368.26 +/- 159.15
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 3505000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 428     |
|    time_elapsed    | 17226   |
|    total_timesteps | 3506176 |
--------------------------------
Eval num_timesteps=3510000, episode_reward=-320.47 +/- 194.17
Episode length: 811.60 +/- 378.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 812        |
|    mean_reward          | -320       |
| time/                   |            |
|    total_timesteps      | 3510000    |
| train/                  |            |
|    approx_kl            | 0.00523696 |
|    clip_fraction        | 0.0249     |
|    clip_range           | 0.2        |
|    entropy_loss         | -6.97      |
|    explained_variance   | 1          |
|    learning_rate        | 5e-05      |
|    loss                 | 10.6       |
|    n_updates            | 4280       |
|    policy_gradient_loss | -0.00134   |
|    std                  | 2.48       |
|    value_loss           | 4.66       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 429     |
|    time_elapsed    | 17264   |
|    total_timesteps | 3514368 |
--------------------------------
Eval num_timesteps=3515000, episode_reward=-277.78 +/- 153.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -278         |
| time/                   |              |
|    total_timesteps      | 3515000      |
| train/                  |              |
|    approx_kl            | 0.0036120168 |
|    clip_fraction        | 0.0112       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.97        |
|    explained_variance   | 0.972        |
|    learning_rate        | 5e-05        |
|    loss                 | 24.9         |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 2.47         |
|    value_loss           | 81.4         |
------------------------------------------
Eval num_timesteps=3520000, episode_reward=-217.58 +/- 183.10
Episode length: 809.40 +/- 383.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 809      |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 3520000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 430     |
|    time_elapsed    | 17310   |
|    total_timesteps | 3522560 |
--------------------------------
Eval num_timesteps=3525000, episode_reward=-247.93 +/- 160.78
Episode length: 811.00 +/- 380.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -248        |
| time/                   |             |
|    total_timesteps      | 3525000     |
| train/                  |             |
|    approx_kl            | 0.003938053 |
|    clip_fraction        | 0.0143      |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.97       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 15.3        |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.00164    |
|    std                  | 2.47        |
|    value_loss           | 30.6        |
-----------------------------------------
Eval num_timesteps=3530000, episode_reward=-116.49 +/- 71.86
Episode length: 803.80 +/- 394.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 804      |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 3530000  |
---------------------------------
New best mean reward!
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 431     |
|    time_elapsed    | 17356   |
|    total_timesteps | 3530752 |
--------------------------------
Eval num_timesteps=3535000, episode_reward=-309.38 +/- 176.54
Episode length: 813.40 +/- 375.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -309         |
| time/                   |              |
|    total_timesteps      | 3535000      |
| train/                  |              |
|    approx_kl            | 0.0034817327 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.97        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 164          |
|    n_updates            | 4310         |
|    policy_gradient_loss | -0.00206     |
|    std                  | 2.47         |
|    value_loss           | 192          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 432     |
|    time_elapsed    | 17394   |
|    total_timesteps | 3538944 |
--------------------------------
Eval num_timesteps=3540000, episode_reward=-459.52 +/- 96.53
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -460        |
| time/                   |             |
|    total_timesteps      | 3540000     |
| train/                  |             |
|    approx_kl            | 0.003559105 |
|    clip_fraction        | 0.00981     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.97       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0969      |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.000591   |
|    std                  | 2.48        |
|    value_loss           | 43          |
-----------------------------------------
Eval num_timesteps=3545000, episode_reward=-307.81 +/- 148.33
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 3545000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 433     |
|    time_elapsed    | 17443   |
|    total_timesteps | 3547136 |
--------------------------------
Eval num_timesteps=3550000, episode_reward=-329.85 +/- 117.34
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -330        |
| time/                   |             |
|    total_timesteps      | 3550000     |
| train/                  |             |
|    approx_kl            | 0.003338636 |
|    clip_fraction        | 0.00863     |
|    clip_range           | 0.2         |
|    entropy_loss         | -6.98       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.133       |
|    n_updates            | 4330        |
|    policy_gradient_loss | -0.000991   |
|    std                  | 2.48        |
|    value_loss           | 53.2        |
-----------------------------------------
Eval num_timesteps=3555000, episode_reward=-222.78 +/- 170.92
Episode length: 813.20 +/- 375.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 3555000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 434     |
|    time_elapsed    | 17490   |
|    total_timesteps | 3555328 |
--------------------------------
Eval num_timesteps=3560000, episode_reward=-471.70 +/- 112.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -472         |
| time/                   |              |
|    total_timesteps      | 3560000      |
| train/                  |              |
|    approx_kl            | 0.0043268646 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -6.99        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 7.81         |
|    n_updates            | 4340         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 2.49         |
|    value_loss           | 17.6         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 435     |
|    time_elapsed    | 17531   |
|    total_timesteps | 3563520 |
--------------------------------
Eval num_timesteps=3565000, episode_reward=-362.34 +/- 201.23
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -362        |
| time/                   |             |
|    total_timesteps      | 3565000     |
| train/                  |             |
|    approx_kl            | 0.004867699 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7          |
|    explained_variance   | 0.996       |
|    learning_rate        | 5e-05       |
|    loss                 | 17.5        |
|    n_updates            | 4350        |
|    policy_gradient_loss | -0.00118    |
|    std                  | 2.5         |
|    value_loss           | 48.7        |
-----------------------------------------
Eval num_timesteps=3570000, episode_reward=-355.21 +/- 91.31
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -355     |
| time/              |          |
|    total_timesteps | 3570000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 436     |
|    time_elapsed    | 17580   |
|    total_timesteps | 3571712 |
--------------------------------
Eval num_timesteps=3575000, episode_reward=-267.20 +/- 112.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -267         |
| time/                   |              |
|    total_timesteps      | 3575000      |
| train/                  |              |
|    approx_kl            | 0.0053057782 |
|    clip_fraction        | 0.0466       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.01        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0822       |
|    n_updates            | 4360         |
|    policy_gradient_loss | -0.00366     |
|    std                  | 2.5          |
|    value_loss           | 0.613        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 437     |
|    time_elapsed    | 17619   |
|    total_timesteps | 3579904 |
--------------------------------
Eval num_timesteps=3580000, episode_reward=-408.84 +/- 125.12
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -409         |
| time/                   |              |
|    total_timesteps      | 3580000      |
| train/                  |              |
|    approx_kl            | 0.0025095264 |
|    clip_fraction        | 0.00425      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.01        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.157        |
|    n_updates            | 4370         |
|    policy_gradient_loss | -0.000854    |
|    std                  | 2.5          |
|    value_loss           | 2.04         |
------------------------------------------
Eval num_timesteps=3585000, episode_reward=-289.63 +/- 115.40
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -290     |
| time/              |          |
|    total_timesteps | 3585000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 438     |
|    time_elapsed    | 17667   |
|    total_timesteps | 3588096 |
--------------------------------
Eval num_timesteps=3590000, episode_reward=-323.27 +/- 151.54
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -323         |
| time/                   |              |
|    total_timesteps      | 3590000      |
| train/                  |              |
|    approx_kl            | 0.0033895178 |
|    clip_fraction        | 0.0101       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.01        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.402        |
|    n_updates            | 4380         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 2.51         |
|    value_loss           | 58.6         |
------------------------------------------
Eval num_timesteps=3595000, episode_reward=-318.98 +/- 146.39
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -319     |
| time/              |          |
|    total_timesteps | 3595000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 439     |
|    time_elapsed    | 17717   |
|    total_timesteps | 3596288 |
--------------------------------
Eval num_timesteps=3600000, episode_reward=-298.99 +/- 146.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -299        |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.001482163 |
|    clip_fraction        | 0.000806    |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.02       |
|    explained_variance   | 0.982       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.466       |
|    n_updates            | 4390        |
|    policy_gradient_loss | -0.00014    |
|    std                  | 2.51        |
|    value_loss           | 177         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 440     |
|    time_elapsed    | 17755   |
|    total_timesteps | 3604480 |
--------------------------------
Eval num_timesteps=3605000, episode_reward=-278.90 +/- 114.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -279        |
| time/                   |             |
|    total_timesteps      | 3605000     |
| train/                  |             |
|    approx_kl            | 0.003281679 |
|    clip_fraction        | 0.0146      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.02       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 10.8        |
|    n_updates            | 4400        |
|    policy_gradient_loss | -0.00087    |
|    std                  | 2.52        |
|    value_loss           | 43.4        |
-----------------------------------------
Eval num_timesteps=3610000, episode_reward=-251.46 +/- 166.92
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 3610000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 441     |
|    time_elapsed    | 17804   |
|    total_timesteps | 3612672 |
--------------------------------
Eval num_timesteps=3615000, episode_reward=-254.94 +/- 120.65
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -255         |
| time/                   |              |
|    total_timesteps      | 3615000      |
| train/                  |              |
|    approx_kl            | 0.0033323038 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.03        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 301          |
|    n_updates            | 4410         |
|    policy_gradient_loss | -0.000745    |
|    std                  | 2.52         |
|    value_loss           | 254          |
------------------------------------------
Eval num_timesteps=3620000, episode_reward=-349.87 +/- 160.07
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 3620000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 442     |
|    time_elapsed    | 17852   |
|    total_timesteps | 3620864 |
--------------------------------
Eval num_timesteps=3625000, episode_reward=-239.11 +/- 78.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -239         |
| time/                   |              |
|    total_timesteps      | 3625000      |
| train/                  |              |
|    approx_kl            | 0.0031109508 |
|    clip_fraction        | 0.00912      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.57         |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.000376    |
|    std                  | 2.53         |
|    value_loss           | 116          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 443     |
|    time_elapsed    | 17892   |
|    total_timesteps | 3629056 |
--------------------------------
Eval num_timesteps=3630000, episode_reward=-339.58 +/- 42.38
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -340        |
| time/                   |             |
|    total_timesteps      | 3630000     |
| train/                  |             |
|    approx_kl            | 0.005400027 |
|    clip_fraction        | 0.0225      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.04       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0281      |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00116    |
|    std                  | 2.53        |
|    value_loss           | 50          |
-----------------------------------------
Eval num_timesteps=3635000, episode_reward=-425.60 +/- 171.50
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 3635000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 444     |
|    time_elapsed    | 17941   |
|    total_timesteps | 3637248 |
--------------------------------
Eval num_timesteps=3640000, episode_reward=-315.85 +/- 227.16
Episode length: 803.80 +/- 394.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 804          |
|    mean_reward          | -316         |
| time/                   |              |
|    total_timesteps      | 3640000      |
| train/                  |              |
|    approx_kl            | 0.0032200203 |
|    clip_fraction        | 0.00505      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.04        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 0.529        |
|    n_updates            | 4440         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 2.53         |
|    value_loss           | 150          |
------------------------------------------
Eval num_timesteps=3645000, episode_reward=-399.90 +/- 141.85
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 3645000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 445     |
|    time_elapsed    | 17988   |
|    total_timesteps | 3645440 |
--------------------------------
Eval num_timesteps=3650000, episode_reward=-275.77 +/- 138.62
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -276         |
| time/                   |              |
|    total_timesteps      | 3650000      |
| train/                  |              |
|    approx_kl            | 0.0059911655 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.91         |
|    n_updates            | 4450         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 2.55         |
|    value_loss           | 162          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 446     |
|    time_elapsed    | 18028   |
|    total_timesteps | 3653632 |
--------------------------------
Eval num_timesteps=3655000, episode_reward=-337.21 +/- 189.02
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -337        |
| time/                   |             |
|    total_timesteps      | 3655000     |
| train/                  |             |
|    approx_kl            | 0.004037395 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.06       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | -0.0196     |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.00133    |
|    std                  | 2.54        |
|    value_loss           | 10.4        |
-----------------------------------------
Eval num_timesteps=3660000, episode_reward=-166.18 +/- 56.62
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 3660000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 447     |
|    time_elapsed    | 18076   |
|    total_timesteps | 3661824 |
--------------------------------
Eval num_timesteps=3665000, episode_reward=-334.52 +/- 168.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -335         |
| time/                   |              |
|    total_timesteps      | 3665000      |
| train/                  |              |
|    approx_kl            | 0.0037466683 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.05        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0468       |
|    n_updates            | 4470         |
|    policy_gradient_loss | -0.00208     |
|    std                  | 2.55         |
|    value_loss           | 1.84         |
------------------------------------------
Eval num_timesteps=3670000, episode_reward=-293.70 +/- 139.61
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 3670000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 448     |
|    time_elapsed    | 18124   |
|    total_timesteps | 3670016 |
--------------------------------
Eval num_timesteps=3675000, episode_reward=-363.60 +/- 77.89
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -364        |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.001743091 |
|    clip_fraction        | 0.00144     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.06       |
|    explained_variance   | 0.991       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.254       |
|    n_updates            | 4480        |
|    policy_gradient_loss | -0.000805   |
|    std                  | 2.55        |
|    value_loss           | 198         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 449     |
|    time_elapsed    | 18165   |
|    total_timesteps | 3678208 |
--------------------------------
Eval num_timesteps=3680000, episode_reward=-228.28 +/- 192.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -228         |
| time/                   |              |
|    total_timesteps      | 3680000      |
| train/                  |              |
|    approx_kl            | 0.0042724228 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.07        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.129        |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 2.56         |
|    value_loss           | 22           |
------------------------------------------
Eval num_timesteps=3685000, episode_reward=-411.50 +/- 194.04
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 3685000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 450     |
|    time_elapsed    | 18214   |
|    total_timesteps | 3686400 |
--------------------------------
Eval num_timesteps=3690000, episode_reward=-181.83 +/- 48.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -182         |
| time/                   |              |
|    total_timesteps      | 3690000      |
| train/                  |              |
|    approx_kl            | 0.0034005505 |
|    clip_fraction        | 0.00465      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.08        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.16         |
|    n_updates            | 4500         |
|    policy_gradient_loss | -0.000642    |
|    std                  | 2.56         |
|    value_loss           | 156          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 451     |
|    time_elapsed    | 18253   |
|    total_timesteps | 3694592 |
--------------------------------
Eval num_timesteps=3695000, episode_reward=-292.56 +/- 51.79
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -293        |
| time/                   |             |
|    total_timesteps      | 3695000     |
| train/                  |             |
|    approx_kl            | 0.003404742 |
|    clip_fraction        | 0.0117      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.09       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0814      |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.000428   |
|    std                  | 2.58        |
|    value_loss           | 188         |
-----------------------------------------
Eval num_timesteps=3700000, episode_reward=-226.21 +/- 93.57
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 3700000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 452     |
|    time_elapsed    | 18301   |
|    total_timesteps | 3702784 |
--------------------------------
Eval num_timesteps=3705000, episode_reward=-318.74 +/- 206.52
Episode length: 807.40 +/- 387.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 807          |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 3705000      |
| train/                  |              |
|    approx_kl            | 0.0020437818 |
|    clip_fraction        | 0.00511      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.09        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.37         |
|    n_updates            | 4520         |
|    policy_gradient_loss | -0.000352    |
|    std                  | 2.58         |
|    value_loss           | 76.2         |
------------------------------------------
Eval num_timesteps=3710000, episode_reward=-412.23 +/- 170.25
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 3710000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 453     |
|    time_elapsed    | 18348   |
|    total_timesteps | 3710976 |
--------------------------------
Eval num_timesteps=3715000, episode_reward=-372.95 +/- 182.39
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -373        |
| time/                   |             |
|    total_timesteps      | 3715000     |
| train/                  |             |
|    approx_kl            | 0.003988266 |
|    clip_fraction        | 0.017       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.09       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.48        |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.00155    |
|    std                  | 2.58        |
|    value_loss           | 52.8        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 454     |
|    time_elapsed    | 18388   |
|    total_timesteps | 3719168 |
--------------------------------
Eval num_timesteps=3720000, episode_reward=-367.10 +/- 107.77
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -367         |
| time/                   |              |
|    total_timesteps      | 3720000      |
| train/                  |              |
|    approx_kl            | 0.0051770257 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.1         |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 12.7         |
|    n_updates            | 4540         |
|    policy_gradient_loss | -0.00236     |
|    std                  | 2.59         |
|    value_loss           | 69           |
------------------------------------------
Eval num_timesteps=3725000, episode_reward=-313.73 +/- 230.12
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -314     |
| time/              |          |
|    total_timesteps | 3725000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 455     |
|    time_elapsed    | 18437   |
|    total_timesteps | 3727360 |
--------------------------------
Eval num_timesteps=3730000, episode_reward=-309.38 +/- 155.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -309         |
| time/                   |              |
|    total_timesteps      | 3730000      |
| train/                  |              |
|    approx_kl            | 0.0042374404 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.11        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 0.205        |
|    n_updates            | 4550         |
|    policy_gradient_loss | -0.00081     |
|    std                  | 2.59         |
|    value_loss           | 244          |
------------------------------------------
Eval num_timesteps=3735000, episode_reward=-418.31 +/- 164.08
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -418     |
| time/              |          |
|    total_timesteps | 3735000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 456     |
|    time_elapsed    | 18485   |
|    total_timesteps | 3735552 |
--------------------------------
Eval num_timesteps=3740000, episode_reward=-388.01 +/- 160.91
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -388         |
| time/                   |              |
|    total_timesteps      | 3740000      |
| train/                  |              |
|    approx_kl            | 0.0024901468 |
|    clip_fraction        | 0.0154       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.11        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 9.28         |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 2.59         |
|    value_loss           | 122          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 457     |
|    time_elapsed    | 18526   |
|    total_timesteps | 3743744 |
--------------------------------
Eval num_timesteps=3745000, episode_reward=-339.05 +/- 135.54
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -339         |
| time/                   |              |
|    total_timesteps      | 3745000      |
| train/                  |              |
|    approx_kl            | 0.0044465307 |
|    clip_fraction        | 0.0358       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.12        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 318          |
|    n_updates            | 4570         |
|    policy_gradient_loss | -0.00225     |
|    std                  | 2.6          |
|    value_loss           | 39.7         |
------------------------------------------
Eval num_timesteps=3750000, episode_reward=-319.47 +/- 85.37
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -319     |
| time/              |          |
|    total_timesteps | 3750000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 458     |
|    time_elapsed    | 18574   |
|    total_timesteps | 3751936 |
--------------------------------
Eval num_timesteps=3755000, episode_reward=-267.32 +/- 102.01
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -267         |
| time/                   |              |
|    total_timesteps      | 3755000      |
| train/                  |              |
|    approx_kl            | 0.0033559552 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.12        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 32.2         |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.000549    |
|    std                  | 2.6          |
|    value_loss           | 176          |
------------------------------------------
Eval num_timesteps=3760000, episode_reward=-279.08 +/- 166.65
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 3760000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 459     |
|    time_elapsed    | 18622   |
|    total_timesteps | 3760128 |
--------------------------------
Eval num_timesteps=3765000, episode_reward=-415.28 +/- 195.06
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -415         |
| time/                   |              |
|    total_timesteps      | 3765000      |
| train/                  |              |
|    approx_kl            | 0.0048329853 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.12        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0714       |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2.59         |
|    value_loss           | 16.8         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 460     |
|    time_elapsed    | 18663   |
|    total_timesteps | 3768320 |
--------------------------------
Eval num_timesteps=3770000, episode_reward=-378.23 +/- 126.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -378         |
| time/                   |              |
|    total_timesteps      | 3770000      |
| train/                  |              |
|    approx_kl            | 0.0028243356 |
|    clip_fraction        | 0.00967      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.11        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0583       |
|    n_updates            | 4600         |
|    policy_gradient_loss | -0.000727    |
|    std                  | 2.59         |
|    value_loss           | 7.09         |
------------------------------------------
Eval num_timesteps=3775000, episode_reward=-347.72 +/- 130.73
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -348     |
| time/              |          |
|    total_timesteps | 3775000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 461     |
|    time_elapsed    | 18711   |
|    total_timesteps | 3776512 |
--------------------------------
Eval num_timesteps=3780000, episode_reward=-292.14 +/- 103.93
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -292        |
| time/                   |             |
|    total_timesteps      | 3780000     |
| train/                  |             |
|    approx_kl            | 0.003421696 |
|    clip_fraction        | 0.0159      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.12       |
|    explained_variance   | 0.996       |
|    learning_rate        | 5e-05       |
|    loss                 | 4.47        |
|    n_updates            | 4610        |
|    policy_gradient_loss | -0.000631   |
|    std                  | 2.6         |
|    value_loss           | 133         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 462     |
|    time_elapsed    | 18751   |
|    total_timesteps | 3784704 |
--------------------------------
Eval num_timesteps=3785000, episode_reward=-269.87 +/- 176.55
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -270         |
| time/                   |              |
|    total_timesteps      | 3785000      |
| train/                  |              |
|    approx_kl            | 0.0030216018 |
|    clip_fraction        | 0.00664      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.13        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.999        |
|    n_updates            | 4620         |
|    policy_gradient_loss | -0.000948    |
|    std                  | 2.6          |
|    value_loss           | 38.5         |
------------------------------------------
Eval num_timesteps=3790000, episode_reward=-275.74 +/- 192.30
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -276     |
| time/              |          |
|    total_timesteps | 3790000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 463     |
|    time_elapsed    | 18800   |
|    total_timesteps | 3792896 |
--------------------------------
Eval num_timesteps=3795000, episode_reward=-312.89 +/- 111.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -313         |
| time/                   |              |
|    total_timesteps      | 3795000      |
| train/                  |              |
|    approx_kl            | 0.0011551956 |
|    clip_fraction        | 0.000354     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.13        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.37         |
|    n_updates            | 4630         |
|    policy_gradient_loss | -0.000592    |
|    std                  | 2.6          |
|    value_loss           | 155          |
------------------------------------------
Eval num_timesteps=3800000, episode_reward=-293.97 +/- 204.97
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 3800000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 464     |
|    time_elapsed    | 18849   |
|    total_timesteps | 3801088 |
--------------------------------
Eval num_timesteps=3805000, episode_reward=-241.74 +/- 108.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -242        |
| time/                   |             |
|    total_timesteps      | 3805000     |
| train/                  |             |
|    approx_kl            | 0.002358785 |
|    clip_fraction        | 0.00551     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.13       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 3.12        |
|    n_updates            | 4640        |
|    policy_gradient_loss | -0.000549   |
|    std                  | 2.61        |
|    value_loss           | 41.2        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 465     |
|    time_elapsed    | 18888   |
|    total_timesteps | 3809280 |
--------------------------------
Eval num_timesteps=3810000, episode_reward=-244.68 +/- 111.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -245         |
| time/                   |              |
|    total_timesteps      | 3810000      |
| train/                  |              |
|    approx_kl            | 0.0030889572 |
|    clip_fraction        | 0.00762      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.13        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0246       |
|    n_updates            | 4650         |
|    policy_gradient_loss | -0.00109     |
|    std                  | 2.62         |
|    value_loss           | 17           |
------------------------------------------
Eval num_timesteps=3815000, episode_reward=-339.58 +/- 134.93
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 3815000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 466     |
|    time_elapsed    | 18936   |
|    total_timesteps | 3817472 |
--------------------------------
Eval num_timesteps=3820000, episode_reward=-259.57 +/- 76.92
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -260        |
| time/                   |             |
|    total_timesteps      | 3820000     |
| train/                  |             |
|    approx_kl            | 0.004177951 |
|    clip_fraction        | 0.0155      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.14       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 8.62        |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.00139    |
|    std                  | 2.62        |
|    value_loss           | 43.1        |
-----------------------------------------
Eval num_timesteps=3825000, episode_reward=-351.38 +/- 124.49
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -351     |
| time/              |          |
|    total_timesteps | 3825000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 467     |
|    time_elapsed    | 18983   |
|    total_timesteps | 3825664 |
--------------------------------
Eval num_timesteps=3830000, episode_reward=-230.26 +/- 103.04
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -230        |
| time/                   |             |
|    total_timesteps      | 3830000     |
| train/                  |             |
|    approx_kl            | 0.003596435 |
|    clip_fraction        | 0.0108      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.14       |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.101       |
|    n_updates            | 4670        |
|    policy_gradient_loss | -0.00114    |
|    std                  | 2.62        |
|    value_loss           | 49.2        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 468     |
|    time_elapsed    | 19024   |
|    total_timesteps | 3833856 |
--------------------------------
Eval num_timesteps=3835000, episode_reward=-229.95 +/- 40.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -230         |
| time/                   |              |
|    total_timesteps      | 3835000      |
| train/                  |              |
|    approx_kl            | 0.0032774422 |
|    clip_fraction        | 0.00726      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.15        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.185        |
|    n_updates            | 4680         |
|    policy_gradient_loss | -0.000679    |
|    std                  | 2.62         |
|    value_loss           | 106          |
------------------------------------------
Eval num_timesteps=3840000, episode_reward=-348.50 +/- 205.81
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -348     |
| time/              |          |
|    total_timesteps | 3840000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 469     |
|    time_elapsed    | 19073   |
|    total_timesteps | 3842048 |
--------------------------------
Eval num_timesteps=3845000, episode_reward=-393.87 +/- 114.04
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -394        |
| time/                   |             |
|    total_timesteps      | 3845000     |
| train/                  |             |
|    approx_kl            | 0.001177059 |
|    clip_fraction        | 0.00011     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.15       |
|    explained_variance   | 0.996       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.293       |
|    n_updates            | 4690        |
|    policy_gradient_loss | -0.000409   |
|    std                  | 2.63        |
|    value_loss           | 56          |
-----------------------------------------
Eval num_timesteps=3850000, episode_reward=-302.71 +/- 166.95
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 3850000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 470     |
|    time_elapsed    | 19122   |
|    total_timesteps | 3850240 |
--------------------------------
Eval num_timesteps=3855000, episode_reward=-165.94 +/- 135.54
Episode length: 625.40 +/- 460.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 625          |
|    mean_reward          | -166         |
| time/                   |              |
|    total_timesteps      | 3855000      |
| train/                  |              |
|    approx_kl            | 0.0035443278 |
|    clip_fraction        | 0.0254       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.15        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.05         |
|    n_updates            | 4700         |
|    policy_gradient_loss | -0.00132     |
|    std                  | 2.63         |
|    value_loss           | 148          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 471     |
|    time_elapsed    | 19160   |
|    total_timesteps | 3858432 |
--------------------------------
Eval num_timesteps=3860000, episode_reward=-298.03 +/- 194.52
Episode length: 811.60 +/- 378.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | -298         |
| time/                   |              |
|    total_timesteps      | 3860000      |
| train/                  |              |
|    approx_kl            | 0.0034912638 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.16        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0467       |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 2.63         |
|    value_loss           | 15.4         |
------------------------------------------
Eval num_timesteps=3865000, episode_reward=-439.34 +/- 151.43
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -439     |
| time/              |          |
|    total_timesteps | 3865000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 472     |
|    time_elapsed    | 19206   |
|    total_timesteps | 3866624 |
--------------------------------
Eval num_timesteps=3870000, episode_reward=-352.64 +/- 156.32
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -353        |
| time/                   |             |
|    total_timesteps      | 3870000     |
| train/                  |             |
|    approx_kl            | 0.005342479 |
|    clip_fraction        | 0.0298      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 22.7        |
|    n_updates            | 4720        |
|    policy_gradient_loss | -0.00293    |
|    std                  | 2.63        |
|    value_loss           | 22.9        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 473     |
|    time_elapsed    | 19247   |
|    total_timesteps | 3874816 |
--------------------------------
Eval num_timesteps=3875000, episode_reward=-348.34 +/- 131.84
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -348        |
| time/                   |             |
|    total_timesteps      | 3875000     |
| train/                  |             |
|    approx_kl            | 0.005322919 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.16       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0181      |
|    n_updates            | 4730        |
|    policy_gradient_loss | -0.00331    |
|    std                  | 2.64        |
|    value_loss           | 0.503       |
-----------------------------------------
Eval num_timesteps=3880000, episode_reward=-354.06 +/- 134.30
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 3880000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 474     |
|    time_elapsed    | 19295   |
|    total_timesteps | 3883008 |
--------------------------------
Eval num_timesteps=3885000, episode_reward=-299.86 +/- 140.76
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -300          |
| time/                   |               |
|    total_timesteps      | 3885000       |
| train/                  |               |
|    approx_kl            | 0.00069652754 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.17         |
|    explained_variance   | 0.99          |
|    learning_rate        | 5e-05         |
|    loss                 | 0.926         |
|    n_updates            | 4740          |
|    policy_gradient_loss | -0.000457     |
|    std                  | 2.64          |
|    value_loss           | 134           |
-------------------------------------------
Eval num_timesteps=3890000, episode_reward=-346.75 +/- 233.29
Episode length: 813.20 +/- 375.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 3890000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 475     |
|    time_elapsed    | 19342   |
|    total_timesteps | 3891200 |
--------------------------------
Eval num_timesteps=3895000, episode_reward=-240.89 +/- 88.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -241         |
| time/                   |              |
|    total_timesteps      | 3895000      |
| train/                  |              |
|    approx_kl            | 0.0013164583 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.17        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 6.71         |
|    n_updates            | 4750         |
|    policy_gradient_loss | 7.25e-05     |
|    std                  | 2.64         |
|    value_loss           | 237          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 476     |
|    time_elapsed    | 19384   |
|    total_timesteps | 3899392 |
--------------------------------
Eval num_timesteps=3900000, episode_reward=-205.60 +/- 72.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -206         |
| time/                   |              |
|    total_timesteps      | 3900000      |
| train/                  |              |
|    approx_kl            | 0.0018639718 |
|    clip_fraction        | 0.00812      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.18        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.36         |
|    n_updates            | 4760         |
|    policy_gradient_loss | 0.000108     |
|    std                  | 2.65         |
|    value_loss           | 156          |
------------------------------------------
Eval num_timesteps=3905000, episode_reward=-284.83 +/- 139.43
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 3905000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 477     |
|    time_elapsed    | 19432   |
|    total_timesteps | 3907584 |
--------------------------------
Eval num_timesteps=3910000, episode_reward=-354.11 +/- 129.22
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -354        |
| time/                   |             |
|    total_timesteps      | 3910000     |
| train/                  |             |
|    approx_kl            | 0.003014531 |
|    clip_fraction        | 0.00775     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.18       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 93          |
|    n_updates            | 4770        |
|    policy_gradient_loss | -7.07e-05   |
|    std                  | 2.66        |
|    value_loss           | 123         |
-----------------------------------------
Eval num_timesteps=3915000, episode_reward=-416.52 +/- 116.94
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 3915000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 478     |
|    time_elapsed    | 19482   |
|    total_timesteps | 3915776 |
--------------------------------
Eval num_timesteps=3920000, episode_reward=-332.13 +/- 172.98
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -332        |
| time/                   |             |
|    total_timesteps      | 3920000     |
| train/                  |             |
|    approx_kl            | 0.004856872 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.19       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.107       |
|    n_updates            | 4780        |
|    policy_gradient_loss | -0.00153    |
|    std                  | 2.66        |
|    value_loss           | 3.19        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 479     |
|    time_elapsed    | 19522   |
|    total_timesteps | 3923968 |
--------------------------------
Eval num_timesteps=3925000, episode_reward=-243.54 +/- 253.10
Episode length: 806.20 +/- 389.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 806         |
|    mean_reward          | -244        |
| time/                   |             |
|    total_timesteps      | 3925000     |
| train/                  |             |
|    approx_kl            | 0.005115631 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.19       |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 122         |
|    n_updates            | 4790        |
|    policy_gradient_loss | -0.00163    |
|    std                  | 2.66        |
|    value_loss           | 111         |
-----------------------------------------
Eval num_timesteps=3930000, episode_reward=-356.30 +/- 232.91
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -356     |
| time/              |          |
|    total_timesteps | 3930000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 480     |
|    time_elapsed    | 19569   |
|    total_timesteps | 3932160 |
--------------------------------
Eval num_timesteps=3935000, episode_reward=-204.21 +/- 146.99
Episode length: 804.60 +/- 392.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 805          |
|    mean_reward          | -204         |
| time/                   |              |
|    total_timesteps      | 3935000      |
| train/                  |              |
|    approx_kl            | 0.0008087002 |
|    clip_fraction        | 2.44e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.2         |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 18.1         |
|    n_updates            | 4800         |
|    policy_gradient_loss | -0.000282    |
|    std                  | 2.66         |
|    value_loss           | 170          |
------------------------------------------
Eval num_timesteps=3940000, episode_reward=-248.74 +/- 64.79
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 3940000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 481     |
|    time_elapsed    | 19618   |
|    total_timesteps | 3940352 |
--------------------------------
Eval num_timesteps=3945000, episode_reward=-282.67 +/- 52.60
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -283        |
| time/                   |             |
|    total_timesteps      | 3945000     |
| train/                  |             |
|    approx_kl            | 0.004061594 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.19       |
|    explained_variance   | 0.989       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.19        |
|    n_updates            | 4810        |
|    policy_gradient_loss | -0.00199    |
|    std                  | 2.67        |
|    value_loss           | 175         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 482     |
|    time_elapsed    | 19659   |
|    total_timesteps | 3948544 |
--------------------------------
Eval num_timesteps=3950000, episode_reward=-347.06 +/- 105.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -347         |
| time/                   |              |
|    total_timesteps      | 3950000      |
| train/                  |              |
|    approx_kl            | 0.0041683624 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.2         |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.00925      |
|    n_updates            | 4820         |
|    policy_gradient_loss | -0.00186     |
|    std                  | 2.67         |
|    value_loss           | 4.23         |
------------------------------------------
Eval num_timesteps=3955000, episode_reward=-234.22 +/- 112.82
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 3955000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 483     |
|    time_elapsed    | 19707   |
|    total_timesteps | 3956736 |
--------------------------------
Eval num_timesteps=3960000, episode_reward=-262.24 +/- 99.40
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -262         |
| time/                   |              |
|    total_timesteps      | 3960000      |
| train/                  |              |
|    approx_kl            | 0.0031726095 |
|    clip_fraction        | 0.0102       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.2         |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 25.7         |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.000961    |
|    std                  | 2.67         |
|    value_loss           | 43.8         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 484     |
|    time_elapsed    | 19747   |
|    total_timesteps | 3964928 |
--------------------------------
Eval num_timesteps=3965000, episode_reward=-274.91 +/- 123.04
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -275        |
| time/                   |             |
|    total_timesteps      | 3965000     |
| train/                  |             |
|    approx_kl            | 0.004593777 |
|    clip_fraction        | 0.0215      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.21       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.178       |
|    n_updates            | 4840        |
|    policy_gradient_loss | -0.00243    |
|    std                  | 2.68        |
|    value_loss           | 36.6        |
-----------------------------------------
Eval num_timesteps=3970000, episode_reward=-366.83 +/- 128.11
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -367     |
| time/              |          |
|    total_timesteps | 3970000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 485     |
|    time_elapsed    | 19794   |
|    total_timesteps | 3973120 |
--------------------------------
Eval num_timesteps=3975000, episode_reward=-306.79 +/- 88.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -307         |
| time/                   |              |
|    total_timesteps      | 3975000      |
| train/                  |              |
|    approx_kl            | 0.0017063671 |
|    clip_fraction        | 0.000806     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.22        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 8.02         |
|    n_updates            | 4850         |
|    policy_gradient_loss | -0.000565    |
|    std                  | 2.68         |
|    value_loss           | 124          |
------------------------------------------
Eval num_timesteps=3980000, episode_reward=-287.01 +/- 79.13
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 3980000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 486     |
|    time_elapsed    | 19845   |
|    total_timesteps | 3981312 |
--------------------------------
Eval num_timesteps=3985000, episode_reward=-324.40 +/- 53.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 3985000      |
| train/                  |              |
|    approx_kl            | 0.0014954952 |
|    clip_fraction        | 0.000305     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.22        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 7.52         |
|    n_updates            | 4860         |
|    policy_gradient_loss | -0.000738    |
|    std                  | 2.69         |
|    value_loss           | 183          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 487     |
|    time_elapsed    | 19886   |
|    total_timesteps | 3989504 |
--------------------------------
Eval num_timesteps=3990000, episode_reward=-272.68 +/- 26.37
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -273        |
| time/                   |             |
|    total_timesteps      | 3990000     |
| train/                  |             |
|    approx_kl            | 0.004550591 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.24       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | -0.0126     |
|    n_updates            | 4870        |
|    policy_gradient_loss | -0.00359    |
|    std                  | 2.71        |
|    value_loss           | 3.7         |
-----------------------------------------
Eval num_timesteps=3995000, episode_reward=-369.10 +/- 165.02
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 3995000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 488     |
|    time_elapsed    | 19935   |
|    total_timesteps | 3997696 |
--------------------------------
Eval num_timesteps=4000000, episode_reward=-256.08 +/- 38.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -256         |
| time/                   |              |
|    total_timesteps      | 4000000      |
| train/                  |              |
|    approx_kl            | 0.0038477005 |
|    clip_fraction        | 0.00946      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.25        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.358        |
|    n_updates            | 4880         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 2.71         |
|    value_loss           | 78.6         |
------------------------------------------
Eval num_timesteps=4005000, episode_reward=-453.25 +/- 158.33
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -453     |
| time/              |          |
|    total_timesteps | 4005000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 489     |
|    time_elapsed    | 19985   |
|    total_timesteps | 4005888 |
--------------------------------
Eval num_timesteps=4010000, episode_reward=-268.01 +/- 96.77
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -268        |
| time/                   |             |
|    total_timesteps      | 4010000     |
| train/                  |             |
|    approx_kl            | 0.004019805 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.25       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.0965      |
|    n_updates            | 4890        |
|    policy_gradient_loss | -0.00159    |
|    std                  | 2.71        |
|    value_loss           | 65.7        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 490     |
|    time_elapsed    | 20025   |
|    total_timesteps | 4014080 |
--------------------------------
Eval num_timesteps=4015000, episode_reward=-382.07 +/- 161.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -382         |
| time/                   |              |
|    total_timesteps      | 4015000      |
| train/                  |              |
|    approx_kl            | 0.0047565494 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.26        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.585        |
|    n_updates            | 4900         |
|    policy_gradient_loss | -0.00246     |
|    std                  | 2.72         |
|    value_loss           | 122          |
------------------------------------------
Eval num_timesteps=4020000, episode_reward=-387.53 +/- 166.76
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 4020000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 491     |
|    time_elapsed    | 20074   |
|    total_timesteps | 4022272 |
--------------------------------
Eval num_timesteps=4025000, episode_reward=-240.80 +/- 106.64
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -241         |
| time/                   |              |
|    total_timesteps      | 4025000      |
| train/                  |              |
|    approx_kl            | 0.0029208357 |
|    clip_fraction        | 0.00895      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.26        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 11.1         |
|    n_updates            | 4910         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 2.73         |
|    value_loss           | 171          |
------------------------------------------
Eval num_timesteps=4030000, episode_reward=-374.13 +/- 140.82
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 4030000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 492     |
|    time_elapsed    | 20124   |
|    total_timesteps | 4030464 |
--------------------------------
Eval num_timesteps=4035000, episode_reward=-252.59 +/- 113.89
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -253        |
| time/                   |             |
|    total_timesteps      | 4035000     |
| train/                  |             |
|    approx_kl            | 0.005204169 |
|    clip_fraction        | 0.0166      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.27       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.142       |
|    n_updates            | 4920        |
|    policy_gradient_loss | -0.00133    |
|    std                  | 2.73        |
|    value_loss           | 16.4        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 493     |
|    time_elapsed    | 20164   |
|    total_timesteps | 4038656 |
--------------------------------
Eval num_timesteps=4040000, episode_reward=-364.95 +/- 134.63
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -365      |
| time/                   |           |
|    total_timesteps      | 4040000   |
| train/                  |           |
|    approx_kl            | 0.0045775 |
|    clip_fraction        | 0.026     |
|    clip_range           | 0.2       |
|    entropy_loss         | -7.27     |
|    explained_variance   | 0.995     |
|    learning_rate        | 5e-05     |
|    loss                 | 38        |
|    n_updates            | 4930      |
|    policy_gradient_loss | -0.00244  |
|    std                  | 2.73      |
|    value_loss           | 156       |
---------------------------------------
Eval num_timesteps=4045000, episode_reward=-316.77 +/- 121.67
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -317     |
| time/              |          |
|    total_timesteps | 4045000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 494     |
|    time_elapsed    | 20214   |
|    total_timesteps | 4046848 |
--------------------------------
Eval num_timesteps=4050000, episode_reward=-351.60 +/- 131.59
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -352        |
| time/                   |             |
|    total_timesteps      | 4050000     |
| train/                  |             |
|    approx_kl            | 0.004705209 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.27       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 3.63        |
|    n_updates            | 4940        |
|    policy_gradient_loss | -0.00229    |
|    std                  | 2.73        |
|    value_loss           | 4.45        |
-----------------------------------------
Eval num_timesteps=4055000, episode_reward=-376.42 +/- 155.77
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 4055000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 495     |
|    time_elapsed    | 20263   |
|    total_timesteps | 4055040 |
--------------------------------
Eval num_timesteps=4060000, episode_reward=-345.08 +/- 126.50
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -345       |
| time/                   |            |
|    total_timesteps      | 4060000    |
| train/                  |            |
|    approx_kl            | 0.00157314 |
|    clip_fraction        | 0.00159    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.28      |
|    explained_variance   | 0.995      |
|    learning_rate        | 5e-05      |
|    loss                 | 0.0209     |
|    n_updates            | 4950       |
|    policy_gradient_loss | -0.000238  |
|    std                  | 2.74       |
|    value_loss           | 96.6       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 496     |
|    time_elapsed    | 20303   |
|    total_timesteps | 4063232 |
--------------------------------
Eval num_timesteps=4065000, episode_reward=-372.57 +/- 142.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -373         |
| time/                   |              |
|    total_timesteps      | 4065000      |
| train/                  |              |
|    approx_kl            | 0.0029873722 |
|    clip_fraction        | 0.00553      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.215        |
|    n_updates            | 4960         |
|    policy_gradient_loss | -0.000888    |
|    std                  | 2.74         |
|    value_loss           | 96.4         |
------------------------------------------
Eval num_timesteps=4070000, episode_reward=-278.38 +/- 96.44
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 4070000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 497     |
|    time_elapsed    | 20354   |
|    total_timesteps | 4071424 |
--------------------------------
Eval num_timesteps=4075000, episode_reward=-321.11 +/- 113.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -321         |
| time/                   |              |
|    total_timesteps      | 4075000      |
| train/                  |              |
|    approx_kl            | 0.0040112715 |
|    clip_fraction        | 0.0216       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.28        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.287        |
|    n_updates            | 4970         |
|    policy_gradient_loss | -0.00133     |
|    std                  | 2.74         |
|    value_loss           | 42.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 498     |
|    time_elapsed    | 20393   |
|    total_timesteps | 4079616 |
--------------------------------
Eval num_timesteps=4080000, episode_reward=-201.62 +/- 77.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -202         |
| time/                   |              |
|    total_timesteps      | 4080000      |
| train/                  |              |
|    approx_kl            | 0.0039730007 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.29        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.142        |
|    n_updates            | 4980         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 2.75         |
|    value_loss           | 16.4         |
------------------------------------------
Eval num_timesteps=4085000, episode_reward=-215.62 +/- 110.21
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 4085000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 499     |
|    time_elapsed    | 20443   |
|    total_timesteps | 4087808 |
--------------------------------
Eval num_timesteps=4090000, episode_reward=-329.02 +/- 117.55
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -329         |
| time/                   |              |
|    total_timesteps      | 4090000      |
| train/                  |              |
|    approx_kl            | 0.0033322312 |
|    clip_fraction        | 0.00621      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.29        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.278        |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.000742    |
|    std                  | 2.75         |
|    value_loss           | 66.9         |
------------------------------------------
Eval num_timesteps=4095000, episode_reward=-392.14 +/- 182.55
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 4095000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 500     |
|    time_elapsed    | 20493   |
|    total_timesteps | 4096000 |
--------------------------------
Eval num_timesteps=4100000, episode_reward=-286.54 +/- 90.57
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -287         |
| time/                   |              |
|    total_timesteps      | 4100000      |
| train/                  |              |
|    approx_kl            | 0.0024375701 |
|    clip_fraction        | 0.00443      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.3         |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.72         |
|    n_updates            | 5000         |
|    policy_gradient_loss | -0.000641    |
|    std                  | 2.76         |
|    value_loss           | 176          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 501     |
|    time_elapsed    | 20533   |
|    total_timesteps | 4104192 |
--------------------------------
Eval num_timesteps=4105000, episode_reward=-289.59 +/- 177.35
Episode length: 809.80 +/- 382.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 810          |
|    mean_reward          | -290         |
| time/                   |              |
|    total_timesteps      | 4105000      |
| train/                  |              |
|    approx_kl            | 0.0034141806 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.3         |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0795       |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.00136     |
|    std                  | 2.76         |
|    value_loss           | 29.3         |
------------------------------------------
Eval num_timesteps=4110000, episode_reward=-237.31 +/- 66.00
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -237     |
| time/              |          |
|    total_timesteps | 4110000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 502     |
|    time_elapsed    | 20582   |
|    total_timesteps | 4112384 |
--------------------------------
Eval num_timesteps=4115000, episode_reward=-188.81 +/- 144.26
Episode length: 813.80 +/- 374.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -189         |
| time/                   |              |
|    total_timesteps      | 4115000      |
| train/                  |              |
|    approx_kl            | 0.0019032869 |
|    clip_fraction        | 0.000916     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.3         |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.261        |
|    n_updates            | 5020         |
|    policy_gradient_loss | -0.00024     |
|    std                  | 2.76         |
|    value_loss           | 72.7         |
------------------------------------------
Eval num_timesteps=4120000, episode_reward=-374.05 +/- 91.27
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 4120000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 503     |
|    time_elapsed    | 20629   |
|    total_timesteps | 4120576 |
--------------------------------
Eval num_timesteps=4125000, episode_reward=-364.79 +/- 67.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 4125000      |
| train/                  |              |
|    approx_kl            | 0.0018565477 |
|    clip_fraction        | 0.00671      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.31        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.497        |
|    n_updates            | 5030         |
|    policy_gradient_loss | -0.000296    |
|    std                  | 2.77         |
|    value_loss           | 256          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 504     |
|    time_elapsed    | 20670   |
|    total_timesteps | 4128768 |
--------------------------------
Eval num_timesteps=4130000, episode_reward=-414.75 +/- 182.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -415         |
| time/                   |              |
|    total_timesteps      | 4130000      |
| train/                  |              |
|    approx_kl            | 0.0027571633 |
|    clip_fraction        | 0.00779      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.31        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0154       |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.000936    |
|    std                  | 2.77         |
|    value_loss           | 29.3         |
------------------------------------------
Eval num_timesteps=4135000, episode_reward=-294.04 +/- 183.02
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 4135000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 505     |
|    time_elapsed    | 20719   |
|    total_timesteps | 4136960 |
--------------------------------
Eval num_timesteps=4140000, episode_reward=-432.17 +/- 94.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -432         |
| time/                   |              |
|    total_timesteps      | 4140000      |
| train/                  |              |
|    approx_kl            | 0.0045515625 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.31        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.63         |
|    n_updates            | 5050         |
|    policy_gradient_loss | -0.00208     |
|    std                  | 2.77         |
|    value_loss           | 55.8         |
------------------------------------------
Eval num_timesteps=4145000, episode_reward=-370.60 +/- 107.59
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 4145000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 506     |
|    time_elapsed    | 20768   |
|    total_timesteps | 4145152 |
--------------------------------
Eval num_timesteps=4150000, episode_reward=-382.32 +/- 163.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -382         |
| time/                   |              |
|    total_timesteps      | 4150000      |
| train/                  |              |
|    approx_kl            | 0.0016638687 |
|    clip_fraction        | 0.00276      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.32        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.112        |
|    n_updates            | 5060         |
|    policy_gradient_loss | -0.000227    |
|    std                  | 2.78         |
|    value_loss           | 45.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 507     |
|    time_elapsed    | 20809   |
|    total_timesteps | 4153344 |
--------------------------------
Eval num_timesteps=4155000, episode_reward=-322.11 +/- 190.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -322         |
| time/                   |              |
|    total_timesteps      | 4155000      |
| train/                  |              |
|    approx_kl            | 0.0034909872 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.32        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.00882     |
|    n_updates            | 5070         |
|    policy_gradient_loss | -0.000328    |
|    std                  | 2.78         |
|    value_loss           | 92.6         |
------------------------------------------
Eval num_timesteps=4160000, episode_reward=-338.90 +/- 159.32
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 4160000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 508     |
|    time_elapsed    | 20860   |
|    total_timesteps | 4161536 |
--------------------------------
Eval num_timesteps=4165000, episode_reward=-318.28 +/- 87.24
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -318         |
| time/                   |              |
|    total_timesteps      | 4165000      |
| train/                  |              |
|    approx_kl            | 0.0031777485 |
|    clip_fraction        | 0.00236      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.33        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.387        |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.00108     |
|    std                  | 2.78         |
|    value_loss           | 98.4         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 509     |
|    time_elapsed    | 20900   |
|    total_timesteps | 4169728 |
--------------------------------
Eval num_timesteps=4170000, episode_reward=-349.56 +/- 179.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -350         |
| time/                   |              |
|    total_timesteps      | 4170000      |
| train/                  |              |
|    approx_kl            | 0.0034805136 |
|    clip_fraction        | 0.0181       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.33        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.121        |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 2.79         |
|    value_loss           | 153          |
------------------------------------------
Eval num_timesteps=4175000, episode_reward=-280.03 +/- 141.50
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 4175000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 510     |
|    time_elapsed    | 20949   |
|    total_timesteps | 4177920 |
--------------------------------
Eval num_timesteps=4180000, episode_reward=-286.71 +/- 142.49
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -287         |
| time/                   |              |
|    total_timesteps      | 4180000      |
| train/                  |              |
|    approx_kl            | 0.0032483807 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.34        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.221        |
|    n_updates            | 5100         |
|    policy_gradient_loss | -0.00132     |
|    std                  | 2.79         |
|    value_loss           | 33.6         |
------------------------------------------
Eval num_timesteps=4185000, episode_reward=-584.97 +/- 60.69
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -585     |
| time/              |          |
|    total_timesteps | 4185000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 511     |
|    time_elapsed    | 20998   |
|    total_timesteps | 4186112 |
--------------------------------
Eval num_timesteps=4190000, episode_reward=-204.51 +/- 62.24
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -205         |
| time/                   |              |
|    total_timesteps      | 4190000      |
| train/                  |              |
|    approx_kl            | 0.0046310592 |
|    clip_fraction        | 0.0281       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.36        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.841        |
|    n_updates            | 5110         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 2.83         |
|    value_loss           | 4.05         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 512     |
|    time_elapsed    | 21040   |
|    total_timesteps | 4194304 |
--------------------------------
Eval num_timesteps=4195000, episode_reward=-241.61 +/- 77.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -242         |
| time/                   |              |
|    total_timesteps      | 4195000      |
| train/                  |              |
|    approx_kl            | 0.0041583586 |
|    clip_fraction        | 0.0224       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.38        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.228        |
|    n_updates            | 5120         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 2.83         |
|    value_loss           | 1.37         |
------------------------------------------
Eval num_timesteps=4200000, episode_reward=-391.94 +/- 153.01
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 4200000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 513     |
|    time_elapsed    | 21091   |
|    total_timesteps | 4202496 |
--------------------------------
Eval num_timesteps=4205000, episode_reward=-397.81 +/- 211.10
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -398          |
| time/                   |               |
|    total_timesteps      | 4205000       |
| train/                  |               |
|    approx_kl            | 0.00084662373 |
|    clip_fraction        | 0.000171      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.38         |
|    explained_variance   | 0.989         |
|    learning_rate        | 5e-05         |
|    loss                 | 239           |
|    n_updates            | 5130          |
|    policy_gradient_loss | -0.000183     |
|    std                  | 2.84          |
|    value_loss           | 318           |
-------------------------------------------
Eval num_timesteps=4210000, episode_reward=-244.31 +/- 125.02
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 4210000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 514     |
|    time_elapsed    | 21139   |
|    total_timesteps | 4210688 |
--------------------------------
Eval num_timesteps=4215000, episode_reward=-219.03 +/- 76.60
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -219         |
| time/                   |              |
|    total_timesteps      | 4215000      |
| train/                  |              |
|    approx_kl            | 0.0056896345 |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.39        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 48.7         |
|    n_updates            | 5140         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 2.85         |
|    value_loss           | 81.7         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 515     |
|    time_elapsed    | 21180   |
|    total_timesteps | 4218880 |
--------------------------------
Eval num_timesteps=4220000, episode_reward=-379.62 +/- 230.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -380         |
| time/                   |              |
|    total_timesteps      | 4220000      |
| train/                  |              |
|    approx_kl            | 0.0038980376 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.4         |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.203        |
|    n_updates            | 5150         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 2.86         |
|    value_loss           | 167          |
------------------------------------------
Eval num_timesteps=4225000, episode_reward=-311.79 +/- 200.02
Episode length: 811.60 +/- 378.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 812      |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 4225000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 516     |
|    time_elapsed    | 21228   |
|    total_timesteps | 4227072 |
--------------------------------
Eval num_timesteps=4230000, episode_reward=-258.34 +/- 187.08
Episode length: 817.20 +/- 367.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 817          |
|    mean_reward          | -258         |
| time/                   |              |
|    total_timesteps      | 4230000      |
| train/                  |              |
|    approx_kl            | 0.0059528975 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.41        |
|    explained_variance   | 0.98         |
|    learning_rate        | 5e-05        |
|    loss                 | 17.1         |
|    n_updates            | 5160         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 2.86         |
|    value_loss           | 69.1         |
------------------------------------------
Eval num_timesteps=4235000, episode_reward=-397.06 +/- 119.91
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 4235000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 517     |
|    time_elapsed    | 21275   |
|    total_timesteps | 4235264 |
--------------------------------
Eval num_timesteps=4240000, episode_reward=-270.82 +/- 112.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -271         |
| time/                   |              |
|    total_timesteps      | 4240000      |
| train/                  |              |
|    approx_kl            | 0.0039414684 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.42        |
|    explained_variance   | 0.987        |
|    learning_rate        | 5e-05        |
|    loss                 | 15.6         |
|    n_updates            | 5170         |
|    policy_gradient_loss | -0.000919    |
|    std                  | 2.88         |
|    value_loss           | 182          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 518     |
|    time_elapsed    | 21318   |
|    total_timesteps | 4243456 |
--------------------------------
Eval num_timesteps=4245000, episode_reward=-185.62 +/- 110.03
Episode length: 808.00 +/- 386.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 808         |
|    mean_reward          | -186        |
| time/                   |             |
|    total_timesteps      | 4245000     |
| train/                  |             |
|    approx_kl            | 0.004849602 |
|    clip_fraction        | 0.0339      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.43       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | 109         |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.00273    |
|    std                  | 2.88        |
|    value_loss           | 36.2        |
-----------------------------------------
Eval num_timesteps=4250000, episode_reward=-229.14 +/- 121.63
Episode length: 813.20 +/- 375.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 4250000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 519     |
|    time_elapsed    | 21364   |
|    total_timesteps | 4251648 |
--------------------------------
Eval num_timesteps=4255000, episode_reward=-349.31 +/- 205.95
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -349        |
| time/                   |             |
|    total_timesteps      | 4255000     |
| train/                  |             |
|    approx_kl            | 0.004565101 |
|    clip_fraction        | 0.0169      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.43       |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 153         |
|    n_updates            | 5190        |
|    policy_gradient_loss | -0.00165    |
|    std                  | 2.88        |
|    value_loss           | 31.2        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 520     |
|    time_elapsed    | 21405   |
|    total_timesteps | 4259840 |
--------------------------------
Eval num_timesteps=4260000, episode_reward=-315.75 +/- 227.70
Episode length: 818.40 +/- 365.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 818          |
|    mean_reward          | -316         |
| time/                   |              |
|    total_timesteps      | 4260000      |
| train/                  |              |
|    approx_kl            | 0.0047894223 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.43        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 781          |
|    n_updates            | 5200         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 2.88         |
|    value_loss           | 186          |
------------------------------------------
Eval num_timesteps=4265000, episode_reward=-367.69 +/- 191.26
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 4265000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 521     |
|    time_elapsed    | 21453   |
|    total_timesteps | 4268032 |
--------------------------------
Eval num_timesteps=4270000, episode_reward=-265.98 +/- 183.90
Episode length: 805.80 +/- 390.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 806         |
|    mean_reward          | -266        |
| time/                   |             |
|    total_timesteps      | 4270000     |
| train/                  |             |
|    approx_kl            | 0.003807252 |
|    clip_fraction        | 0.0255      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.42       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 0.00677     |
|    n_updates            | 5210        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 2.87        |
|    value_loss           | 0.587       |
-----------------------------------------
Eval num_timesteps=4275000, episode_reward=-173.19 +/- 98.39
Episode length: 807.40 +/- 387.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 4275000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 522     |
|    time_elapsed    | 21499   |
|    total_timesteps | 4276224 |
--------------------------------
Eval num_timesteps=4280000, episode_reward=-379.38 +/- 146.10
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -379         |
| time/                   |              |
|    total_timesteps      | 4280000      |
| train/                  |              |
|    approx_kl            | 0.0034872722 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.42        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0198       |
|    n_updates            | 5220         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2.87         |
|    value_loss           | 1.61         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 523     |
|    time_elapsed    | 21542   |
|    total_timesteps | 4284416 |
--------------------------------
Eval num_timesteps=4285000, episode_reward=-230.82 +/- 103.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -231         |
| time/                   |              |
|    total_timesteps      | 4285000      |
| train/                  |              |
|    approx_kl            | 0.0038959235 |
|    clip_fraction        | 0.0093       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.42        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0696       |
|    n_updates            | 5230         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 2.87         |
|    value_loss           | 15.3         |
------------------------------------------
Eval num_timesteps=4290000, episode_reward=-405.83 +/- 220.11
Episode length: 808.00 +/- 386.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 808      |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 4290000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 524     |
|    time_elapsed    | 21589   |
|    total_timesteps | 4292608 |
--------------------------------
Eval num_timesteps=4295000, episode_reward=-345.72 +/- 114.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -346         |
| time/                   |              |
|    total_timesteps      | 4295000      |
| train/                  |              |
|    approx_kl            | 0.0037586004 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.43        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 52.5         |
|    n_updates            | 5240         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 2.88         |
|    value_loss           | 20.1         |
------------------------------------------
Eval num_timesteps=4300000, episode_reward=-264.35 +/- 96.30
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 4300000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 525     |
|    time_elapsed    | 21637   |
|    total_timesteps | 4300800 |
--------------------------------
Eval num_timesteps=4305000, episode_reward=-303.52 +/- 141.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -304         |
| time/                   |              |
|    total_timesteps      | 4305000      |
| train/                  |              |
|    approx_kl            | 0.0030061984 |
|    clip_fraction        | 0.00447      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.43        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.09         |
|    n_updates            | 5250         |
|    policy_gradient_loss | -0.000876    |
|    std                  | 2.88         |
|    value_loss           | 59.7         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 526     |
|    time_elapsed    | 21678   |
|    total_timesteps | 4308992 |
--------------------------------
Eval num_timesteps=4310000, episode_reward=-467.38 +/- 147.35
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -467         |
| time/                   |              |
|    total_timesteps      | 4310000      |
| train/                  |              |
|    approx_kl            | 0.0038964762 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.42        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.144        |
|    n_updates            | 5260         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 2.87         |
|    value_loss           | 0.572        |
------------------------------------------
Eval num_timesteps=4315000, episode_reward=-322.56 +/- 108.48
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -323     |
| time/              |          |
|    total_timesteps | 4315000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 527     |
|    time_elapsed    | 21727   |
|    total_timesteps | 4317184 |
--------------------------------
Eval num_timesteps=4320000, episode_reward=-254.34 +/- 202.94
Episode length: 811.60 +/- 378.80
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 812           |
|    mean_reward          | -254          |
| time/                   |               |
|    total_timesteps      | 4320000       |
| train/                  |               |
|    approx_kl            | 0.00089242694 |
|    clip_fraction        | 0.000122      |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.42         |
|    explained_variance   | 0.983         |
|    learning_rate        | 5e-05         |
|    loss                 | 164           |
|    n_updates            | 5270          |
|    policy_gradient_loss | -0.000328     |
|    std                  | 2.87          |
|    value_loss           | 328           |
-------------------------------------------
Eval num_timesteps=4325000, episode_reward=-302.92 +/- 119.68
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 4325000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 528     |
|    time_elapsed    | 21777   |
|    total_timesteps | 4325376 |
--------------------------------
Eval num_timesteps=4330000, episode_reward=-271.98 +/- 174.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -272         |
| time/                   |              |
|    total_timesteps      | 4330000      |
| train/                  |              |
|    approx_kl            | 0.0030370797 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.42        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.36         |
|    n_updates            | 5280         |
|    policy_gradient_loss | -0.00123     |
|    std                  | 2.87         |
|    value_loss           | 14.3         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 529     |
|    time_elapsed    | 21817   |
|    total_timesteps | 4333568 |
--------------------------------
Eval num_timesteps=4335000, episode_reward=-323.93 +/- 149.52
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -324       |
| time/                   |            |
|    total_timesteps      | 4335000    |
| train/                  |            |
|    approx_kl            | 0.00302717 |
|    clip_fraction        | 0.00656    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.43      |
|    explained_variance   | 0.995      |
|    learning_rate        | 5e-05      |
|    loss                 | 0.0465     |
|    n_updates            | 5290       |
|    policy_gradient_loss | -0.000283  |
|    std                  | 2.88       |
|    value_loss           | 125        |
----------------------------------------
Eval num_timesteps=4340000, episode_reward=-336.43 +/- 177.98
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 4340000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 530     |
|    time_elapsed    | 21867   |
|    total_timesteps | 4341760 |
--------------------------------
Eval num_timesteps=4345000, episode_reward=-260.26 +/- 176.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -260         |
| time/                   |              |
|    total_timesteps      | 4345000      |
| train/                  |              |
|    approx_kl            | 0.0031825497 |
|    clip_fraction        | 0.00739      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.44        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.141        |
|    n_updates            | 5300         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 2.89         |
|    value_loss           | 24.4         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 531     |
|    time_elapsed    | 21909   |
|    total_timesteps | 4349952 |
--------------------------------
Eval num_timesteps=4350000, episode_reward=-424.38 +/- 95.02
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 4350000     |
| train/                  |             |
|    approx_kl            | 0.002244075 |
|    clip_fraction        | 0.0071      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.44       |
|    explained_variance   | 0.998       |
|    learning_rate        | 5e-05       |
|    loss                 | -0.00572    |
|    n_updates            | 5310        |
|    policy_gradient_loss | -0.000973   |
|    std                  | 2.89        |
|    value_loss           | 19.8        |
-----------------------------------------
Eval num_timesteps=4355000, episode_reward=-271.66 +/- 34.39
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 4355000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 532     |
|    time_elapsed    | 21958   |
|    total_timesteps | 4358144 |
--------------------------------
Eval num_timesteps=4360000, episode_reward=-344.84 +/- 125.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -345         |
| time/                   |              |
|    total_timesteps      | 4360000      |
| train/                  |              |
|    approx_kl            | 0.0027860561 |
|    clip_fraction        | 0.0114       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.45        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 4.5          |
|    n_updates            | 5320         |
|    policy_gradient_loss | -0.000583    |
|    std                  | 2.91         |
|    value_loss           | 238          |
------------------------------------------
Eval num_timesteps=4365000, episode_reward=-413.33 +/- 196.63
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 4365000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 533     |
|    time_elapsed    | 22009   |
|    total_timesteps | 4366336 |
--------------------------------
Eval num_timesteps=4370000, episode_reward=-383.59 +/- 134.62
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -384         |
| time/                   |              |
|    total_timesteps      | 4370000      |
| train/                  |              |
|    approx_kl            | 0.0027416213 |
|    clip_fraction        | 0.00382      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.46        |
|    explained_variance   | 0.988        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.391        |
|    n_updates            | 5330         |
|    policy_gradient_loss | -0.000807    |
|    std                  | 2.91         |
|    value_loss           | 232          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 534     |
|    time_elapsed    | 22052   |
|    total_timesteps | 4374528 |
--------------------------------
Eval num_timesteps=4375000, episode_reward=-292.10 +/- 208.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -292         |
| time/                   |              |
|    total_timesteps      | 4375000      |
| train/                  |              |
|    approx_kl            | 0.0039339857 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.46        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 80.1         |
|    n_updates            | 5340         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 2.91         |
|    value_loss           | 36.2         |
------------------------------------------
Eval num_timesteps=4380000, episode_reward=-306.94 +/- 197.99
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 4380000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 535     |
|    time_elapsed    | 22100   |
|    total_timesteps | 4382720 |
--------------------------------
Eval num_timesteps=4385000, episode_reward=-484.37 +/- 105.73
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -484        |
| time/                   |             |
|    total_timesteps      | 4385000     |
| train/                  |             |
|    approx_kl            | 0.003484208 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.47       |
|    explained_variance   | 0.995       |
|    learning_rate        | 5e-05       |
|    loss                 | 7.42        |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.00059    |
|    std                  | 2.92        |
|    value_loss           | 94          |
-----------------------------------------
Eval num_timesteps=4390000, episode_reward=-292.90 +/- 251.13
Episode length: 807.20 +/- 387.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 807      |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 4390000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 536     |
|    time_elapsed    | 22149   |
|    total_timesteps | 4390912 |
--------------------------------
Eval num_timesteps=4395000, episode_reward=-269.91 +/- 90.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -270         |
| time/                   |              |
|    total_timesteps      | 4395000      |
| train/                  |              |
|    approx_kl            | 0.0045239106 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0772       |
|    n_updates            | 5360         |
|    policy_gradient_loss | -0.00313     |
|    std                  | 2.91         |
|    value_loss           | 0.515        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 537     |
|    time_elapsed    | 22189   |
|    total_timesteps | 4399104 |
--------------------------------
Eval num_timesteps=4400000, episode_reward=-324.73 +/- 191.33
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -325         |
| time/                   |              |
|    total_timesteps      | 4400000      |
| train/                  |              |
|    approx_kl            | 0.0044972682 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.46        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 7.53         |
|    n_updates            | 5370         |
|    policy_gradient_loss | -0.000919    |
|    std                  | 2.92         |
|    value_loss           | 21.1         |
------------------------------------------
Eval num_timesteps=4405000, episode_reward=-319.30 +/- 107.59
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -319     |
| time/              |          |
|    total_timesteps | 4405000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 538     |
|    time_elapsed    | 22240   |
|    total_timesteps | 4407296 |
--------------------------------
Eval num_timesteps=4410000, episode_reward=-348.18 +/- 191.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -348         |
| time/                   |              |
|    total_timesteps      | 4410000      |
| train/                  |              |
|    approx_kl            | 0.0038766284 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.719        |
|    n_updates            | 5380         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 2.92         |
|    value_loss           | 161          |
------------------------------------------
Eval num_timesteps=4415000, episode_reward=-218.85 +/- 110.76
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 4415000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 539     |
|    time_elapsed    | 22291   |
|    total_timesteps | 4415488 |
--------------------------------
Eval num_timesteps=4420000, episode_reward=-195.79 +/- 60.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -196         |
| time/                   |              |
|    total_timesteps      | 4420000      |
| train/                  |              |
|    approx_kl            | 0.0017242725 |
|    clip_fraction        | 0.00253      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 0.994        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.67         |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.000409    |
|    std                  | 2.92         |
|    value_loss           | 53.8         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 540     |
|    time_elapsed    | 22331   |
|    total_timesteps | 4423680 |
--------------------------------
Eval num_timesteps=4425000, episode_reward=-308.80 +/- 122.90
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -309         |
| time/                   |              |
|    total_timesteps      | 4425000      |
| train/                  |              |
|    approx_kl            | 0.0032614872 |
|    clip_fraction        | 0.00908      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0103       |
|    n_updates            | 5400         |
|    policy_gradient_loss | -0.000542    |
|    std                  | 2.92         |
|    value_loss           | 0.318        |
------------------------------------------
Eval num_timesteps=4430000, episode_reward=-338.31 +/- 112.37
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -338     |
| time/              |          |
|    total_timesteps | 4430000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 541     |
|    time_elapsed    | 22383   |
|    total_timesteps | 4431872 |
--------------------------------
Eval num_timesteps=4435000, episode_reward=-366.51 +/- 102.15
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -367         |
| time/                   |              |
|    total_timesteps      | 4435000      |
| train/                  |              |
|    approx_kl            | 0.0025814162 |
|    clip_fraction        | 0.0038       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 29.2         |
|    n_updates            | 5410         |
|    policy_gradient_loss | -0.000618    |
|    std                  | 2.92         |
|    value_loss           | 101          |
------------------------------------------
Eval num_timesteps=4440000, episode_reward=-312.85 +/- 106.20
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 4440000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 542     |
|    time_elapsed    | 22433   |
|    total_timesteps | 4440064 |
--------------------------------
Eval num_timesteps=4445000, episode_reward=-313.68 +/- 138.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -314         |
| time/                   |              |
|    total_timesteps      | 4445000      |
| train/                  |              |
|    approx_kl            | 0.0019708967 |
|    clip_fraction        | 0.00248      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.47        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.64         |
|    n_updates            | 5420         |
|    policy_gradient_loss | -0.00083     |
|    std                  | 2.93         |
|    value_loss           | 265          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 543     |
|    time_elapsed    | 22474   |
|    total_timesteps | 4448256 |
--------------------------------
Eval num_timesteps=4450000, episode_reward=-170.13 +/- 74.83
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -170         |
| time/                   |              |
|    total_timesteps      | 4450000      |
| train/                  |              |
|    approx_kl            | 0.0040014475 |
|    clip_fraction        | 0.0144       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.49        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 20.2         |
|    n_updates            | 5430         |
|    policy_gradient_loss | -0.000928    |
|    std                  | 2.95         |
|    value_loss           | 71.2         |
------------------------------------------
Eval num_timesteps=4455000, episode_reward=-373.91 +/- 192.20
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 4455000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 544     |
|    time_elapsed    | 22524   |
|    total_timesteps | 4456448 |
--------------------------------
Eval num_timesteps=4460000, episode_reward=-441.45 +/- 152.39
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -441         |
| time/                   |              |
|    total_timesteps      | 4460000      |
| train/                  |              |
|    approx_kl            | 0.0020822468 |
|    clip_fraction        | 0.00402      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.5         |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 13.1         |
|    n_updates            | 5440         |
|    policy_gradient_loss | -0.000982    |
|    std                  | 2.95         |
|    value_loss           | 154          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 545     |
|    time_elapsed    | 22565   |
|    total_timesteps | 4464640 |
--------------------------------
Eval num_timesteps=4465000, episode_reward=-403.57 +/- 160.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -404         |
| time/                   |              |
|    total_timesteps      | 4465000      |
| train/                  |              |
|    approx_kl            | 0.0034722788 |
|    clip_fraction        | 0.012        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.51        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.00493     |
|    n_updates            | 5450         |
|    policy_gradient_loss | -0.000446    |
|    std                  | 2.96         |
|    value_loss           | 94.3         |
------------------------------------------
Eval num_timesteps=4470000, episode_reward=-248.22 +/- 150.36
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 4470000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 546     |
|    time_elapsed    | 22615   |
|    total_timesteps | 4472832 |
--------------------------------
Eval num_timesteps=4475000, episode_reward=-343.31 +/- 131.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -343         |
| time/                   |              |
|    total_timesteps      | 4475000      |
| train/                  |              |
|    approx_kl            | 0.0027713059 |
|    clip_fraction        | 0.0104       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.51        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.418        |
|    n_updates            | 5460         |
|    policy_gradient_loss | -0.000892    |
|    std                  | 2.96         |
|    value_loss           | 85.2         |
------------------------------------------
Eval num_timesteps=4480000, episode_reward=-274.62 +/- 164.63
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 4480000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 547     |
|    time_elapsed    | 22665   |
|    total_timesteps | 4481024 |
--------------------------------
Eval num_timesteps=4485000, episode_reward=-274.17 +/- 92.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -274         |
| time/                   |              |
|    total_timesteps      | 4485000      |
| train/                  |              |
|    approx_kl            | 0.0032515237 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.51        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.227        |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.0012      |
|    std                  | 2.97         |
|    value_loss           | 146          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 548     |
|    time_elapsed    | 22707   |
|    total_timesteps | 4489216 |
--------------------------------
Eval num_timesteps=4490000, episode_reward=-467.98 +/- 118.37
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -468        |
| time/                   |             |
|    total_timesteps      | 4490000     |
| train/                  |             |
|    approx_kl            | 0.002101373 |
|    clip_fraction        | 0.00295     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.52       |
|    explained_variance   | 0.992       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.464       |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.00043    |
|    std                  | 2.97        |
|    value_loss           | 184         |
-----------------------------------------
Eval num_timesteps=4495000, episode_reward=-255.38 +/- 110.29
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 4495000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 549     |
|    time_elapsed    | 22758   |
|    total_timesteps | 4497408 |
--------------------------------
Eval num_timesteps=4500000, episode_reward=-233.21 +/- 55.26
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -233         |
| time/                   |              |
|    total_timesteps      | 4500000      |
| train/                  |              |
|    approx_kl            | 0.0025443048 |
|    clip_fraction        | 0.00649      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.52        |
|    explained_variance   | 0.989        |
|    learning_rate        | 5e-05        |
|    loss                 | 121          |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 2.97         |
|    value_loss           | 230          |
------------------------------------------
Eval num_timesteps=4505000, episode_reward=-355.55 +/- 158.48
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -356     |
| time/              |          |
|    total_timesteps | 4505000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 550     |
|    time_elapsed    | 22808   |
|    total_timesteps | 4505600 |
--------------------------------
Eval num_timesteps=4510000, episode_reward=-478.65 +/- 164.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -479         |
| time/                   |              |
|    total_timesteps      | 4510000      |
| train/                  |              |
|    approx_kl            | 0.0019342324 |
|    clip_fraction        | 0.00148      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.52        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 285          |
|    n_updates            | 5500         |
|    policy_gradient_loss | -8.03e-05    |
|    std                  | 2.97         |
|    value_loss           | 123          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 551     |
|    time_elapsed    | 22850   |
|    total_timesteps | 4513792 |
--------------------------------
Eval num_timesteps=4515000, episode_reward=-308.09 +/- 154.47
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -308         |
| time/                   |              |
|    total_timesteps      | 4515000      |
| train/                  |              |
|    approx_kl            | 0.0037609017 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.53        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.0272      |
|    n_updates            | 5510         |
|    policy_gradient_loss | -0.00091     |
|    std                  | 2.98         |
|    value_loss           | 32.5         |
------------------------------------------
Eval num_timesteps=4520000, episode_reward=-270.16 +/- 115.11
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 4520000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 552     |
|    time_elapsed    | 22899   |
|    total_timesteps | 4521984 |
--------------------------------
Eval num_timesteps=4525000, episode_reward=-336.63 +/- 121.90
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -337        |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.002265683 |
|    clip_fraction        | 0.00465     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.53       |
|    explained_variance   | 0.991       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.204       |
|    n_updates            | 5520        |
|    policy_gradient_loss | -0.000316   |
|    std                  | 2.99        |
|    value_loss           | 160         |
-----------------------------------------
Eval num_timesteps=4530000, episode_reward=-340.45 +/- 149.76
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 4530000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 553     |
|    time_elapsed    | 22949   |
|    total_timesteps | 4530176 |
--------------------------------
Eval num_timesteps=4535000, episode_reward=-356.98 +/- 159.37
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -357         |
| time/                   |              |
|    total_timesteps      | 4535000      |
| train/                  |              |
|    approx_kl            | 0.0037909294 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.54        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 59.9         |
|    n_updates            | 5530         |
|    policy_gradient_loss | -0.00071     |
|    std                  | 3            |
|    value_loss           | 40.7         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 554     |
|    time_elapsed    | 22991   |
|    total_timesteps | 4538368 |
--------------------------------
Eval num_timesteps=4540000, episode_reward=-359.41 +/- 189.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -359         |
| time/                   |              |
|    total_timesteps      | 4540000      |
| train/                  |              |
|    approx_kl            | 0.0047545508 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.55        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | -0.0317      |
|    n_updates            | 5540         |
|    policy_gradient_loss | -0.00163     |
|    std                  | 3.01         |
|    value_loss           | 31.3         |
------------------------------------------
Eval num_timesteps=4545000, episode_reward=-362.89 +/- 198.63
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -363     |
| time/              |          |
|    total_timesteps | 4545000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 555     |
|    time_elapsed    | 23040   |
|    total_timesteps | 4546560 |
--------------------------------
Eval num_timesteps=4550000, episode_reward=-226.03 +/- 198.98
Episode length: 620.40 +/- 466.14
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 620          |
|    mean_reward          | -226         |
| time/                   |              |
|    total_timesteps      | 4550000      |
| train/                  |              |
|    approx_kl            | 0.0038865767 |
|    clip_fraction        | 0.0082       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.56        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 4.24         |
|    n_updates            | 5550         |
|    policy_gradient_loss | -0.000625    |
|    std                  | 3.01         |
|    value_loss           | 55.5         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 556     |
|    time_elapsed    | 23080   |
|    total_timesteps | 4554752 |
--------------------------------
Eval num_timesteps=4555000, episode_reward=-160.73 +/- 56.56
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -161        |
| time/                   |             |
|    total_timesteps      | 4555000     |
| train/                  |             |
|    approx_kl            | 0.004221403 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.57       |
|    explained_variance   | 0.961       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.581       |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.00174    |
|    std                  | 3.02        |
|    value_loss           | 63.7        |
-----------------------------------------
Eval num_timesteps=4560000, episode_reward=-353.77 +/- 188.32
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 4560000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 557     |
|    time_elapsed    | 23130   |
|    total_timesteps | 4562944 |
--------------------------------
Eval num_timesteps=4565000, episode_reward=-286.76 +/- 165.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -287         |
| time/                   |              |
|    total_timesteps      | 4565000      |
| train/                  |              |
|    approx_kl            | 0.0013930397 |
|    clip_fraction        | 0.000598     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.57        |
|    explained_variance   | 0.987        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.292        |
|    n_updates            | 5570         |
|    policy_gradient_loss | -0.000254    |
|    std                  | 3.02         |
|    value_loss           | 210          |
------------------------------------------
Eval num_timesteps=4570000, episode_reward=-407.74 +/- 99.47
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -408     |
| time/              |          |
|    total_timesteps | 4570000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 558     |
|    time_elapsed    | 23180   |
|    total_timesteps | 4571136 |
--------------------------------
Eval num_timesteps=4575000, episode_reward=-295.19 +/- 126.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -295         |
| time/                   |              |
|    total_timesteps      | 4575000      |
| train/                  |              |
|    approx_kl            | 0.0035383229 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.58        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.321        |
|    n_updates            | 5580         |
|    policy_gradient_loss | -0.00161     |
|    std                  | 3.04         |
|    value_loss           | 150          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 559     |
|    time_elapsed    | 23223   |
|    total_timesteps | 4579328 |
--------------------------------
Eval num_timesteps=4580000, episode_reward=-342.13 +/- 145.42
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -342        |
| time/                   |             |
|    total_timesteps      | 4580000     |
| train/                  |             |
|    approx_kl            | 0.004162628 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.6        |
|    explained_variance   | 0.994       |
|    learning_rate        | 5e-05       |
|    loss                 | 81.8        |
|    n_updates            | 5590        |
|    policy_gradient_loss | -0.000825   |
|    std                  | 3.05        |
|    value_loss           | 135         |
-----------------------------------------
Eval num_timesteps=4585000, episode_reward=-260.85 +/- 121.11
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 4585000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 560     |
|    time_elapsed    | 23273   |
|    total_timesteps | 4587520 |
--------------------------------
Eval num_timesteps=4590000, episode_reward=-343.96 +/- 100.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -344         |
| time/                   |              |
|    total_timesteps      | 4590000      |
| train/                  |              |
|    approx_kl            | 0.0023360793 |
|    clip_fraction        | 0.00302      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.6         |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.501        |
|    n_updates            | 5600         |
|    policy_gradient_loss | -0.000427    |
|    std                  | 3.05         |
|    value_loss           | 42.7         |
------------------------------------------
Eval num_timesteps=4595000, episode_reward=-307.19 +/- 133.57
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 4595000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 561     |
|    time_elapsed    | 23323   |
|    total_timesteps | 4595712 |
--------------------------------
Eval num_timesteps=4600000, episode_reward=-351.19 +/- 193.84
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -351        |
| time/                   |             |
|    total_timesteps      | 4600000     |
| train/                  |             |
|    approx_kl            | 0.003528996 |
|    clip_fraction        | 0.0101      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.6        |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 1.97        |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.0013     |
|    std                  | 3.05        |
|    value_loss           | 44.9        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 562     |
|    time_elapsed    | 23365   |
|    total_timesteps | 4603904 |
--------------------------------
Eval num_timesteps=4605000, episode_reward=-285.62 +/- 176.12
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 4605000      |
| train/                  |              |
|    approx_kl            | 0.0034073594 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.61        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.724        |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.000922    |
|    std                  | 3.07         |
|    value_loss           | 36.9         |
------------------------------------------
Eval num_timesteps=4610000, episode_reward=-267.63 +/- 74.63
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 4610000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 563     |
|    time_elapsed    | 23413   |
|    total_timesteps | 4612096 |
--------------------------------
Eval num_timesteps=4615000, episode_reward=-243.02 +/- 87.77
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -243         |
| time/                   |              |
|    total_timesteps      | 4615000      |
| train/                  |              |
|    approx_kl            | 0.0037530723 |
|    clip_fraction        | 0.0195       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.62        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.72         |
|    n_updates            | 5630         |
|    policy_gradient_loss | -0.0017      |
|    std                  | 3.07         |
|    value_loss           | 16.6         |
------------------------------------------
Eval num_timesteps=4620000, episode_reward=-228.83 +/- 134.99
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 4620000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 564     |
|    time_elapsed    | 23465   |
|    total_timesteps | 4620288 |
--------------------------------
Eval num_timesteps=4625000, episode_reward=-330.53 +/- 160.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -331         |
| time/                   |              |
|    total_timesteps      | 4625000      |
| train/                  |              |
|    approx_kl            | 0.0024125415 |
|    clip_fraction        | 0.00188      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.62        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0567       |
|    n_updates            | 5640         |
|    policy_gradient_loss | -0.000756    |
|    std                  | 3.07         |
|    value_loss           | 47           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 565     |
|    time_elapsed    | 23506   |
|    total_timesteps | 4628480 |
--------------------------------
Eval num_timesteps=4630000, episode_reward=-329.85 +/- 91.86
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -330         |
| time/                   |              |
|    total_timesteps      | 4630000      |
| train/                  |              |
|    approx_kl            | 0.0027164912 |
|    clip_fraction        | 0.00814      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 8.3          |
|    n_updates            | 5650         |
|    policy_gradient_loss | -0.000672    |
|    std                  | 3.08         |
|    value_loss           | 132          |
------------------------------------------
Eval num_timesteps=4635000, episode_reward=-355.94 +/- 257.60
Episode length: 813.40 +/- 375.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 813      |
|    mean_reward     | -356     |
| time/              |          |
|    total_timesteps | 4635000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 566     |
|    time_elapsed    | 23555   |
|    total_timesteps | 4636672 |
--------------------------------
Eval num_timesteps=4640000, episode_reward=-231.22 +/- 103.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -231         |
| time/                   |              |
|    total_timesteps      | 4640000      |
| train/                  |              |
|    approx_kl            | 0.0014766486 |
|    clip_fraction        | 0.000439     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 501          |
|    n_updates            | 5660         |
|    policy_gradient_loss | -1.91e-05    |
|    std                  | 3.08         |
|    value_loss           | 205          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 567     |
|    time_elapsed    | 23598   |
|    total_timesteps | 4644864 |
--------------------------------
Eval num_timesteps=4645000, episode_reward=-435.15 +/- 88.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -435         |
| time/                   |              |
|    total_timesteps      | 4645000      |
| train/                  |              |
|    approx_kl            | 0.0028731385 |
|    clip_fraction        | 0.0139       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 13.7         |
|    n_updates            | 5670         |
|    policy_gradient_loss | -0.000789    |
|    std                  | 3.08         |
|    value_loss           | 16.8         |
------------------------------------------
Eval num_timesteps=4650000, episode_reward=-365.89 +/- 76.13
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -366     |
| time/              |          |
|    total_timesteps | 4650000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 568     |
|    time_elapsed    | 23647   |
|    total_timesteps | 4653056 |
--------------------------------
Eval num_timesteps=4655000, episode_reward=-319.67 +/- 108.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -320        |
| time/                   |             |
|    total_timesteps      | 4655000     |
| train/                  |             |
|    approx_kl            | 0.005313224 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.63       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | -0.0428     |
|    n_updates            | 5680        |
|    policy_gradient_loss | -0.00371    |
|    std                  | 3.08        |
|    value_loss           | 3.17        |
-----------------------------------------
Eval num_timesteps=4660000, episode_reward=-227.17 +/- 128.29
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 4660000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 569     |
|    time_elapsed    | 23698   |
|    total_timesteps | 4661248 |
--------------------------------
Eval num_timesteps=4665000, episode_reward=-341.53 +/- 173.65
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -342        |
| time/                   |             |
|    total_timesteps      | 4665000     |
| train/                  |             |
|    approx_kl            | 0.002990372 |
|    clip_fraction        | 0.0128      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.63       |
|    explained_variance   | 1           |
|    learning_rate        | 5e-05       |
|    loss                 | 38          |
|    n_updates            | 5690        |
|    policy_gradient_loss | -0.00165    |
|    std                  | 3.09        |
|    value_loss           | 9.09        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 570     |
|    time_elapsed    | 23740   |
|    total_timesteps | 4669440 |
--------------------------------
Eval num_timesteps=4670000, episode_reward=-384.84 +/- 171.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -385         |
| time/                   |              |
|    total_timesteps      | 4670000      |
| train/                  |              |
|    approx_kl            | 0.0025958046 |
|    clip_fraction        | 0.0077       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 82.2         |
|    n_updates            | 5700         |
|    policy_gradient_loss | -0.000829    |
|    std                  | 3.08         |
|    value_loss           | 37.6         |
------------------------------------------
Eval num_timesteps=4675000, episode_reward=-293.32 +/- 192.80
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 4675000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 571     |
|    time_elapsed    | 23789   |
|    total_timesteps | 4677632 |
--------------------------------
Eval num_timesteps=4680000, episode_reward=-400.38 +/- 92.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -400         |
| time/                   |              |
|    total_timesteps      | 4680000      |
| train/                  |              |
|    approx_kl            | 0.0016305643 |
|    clip_fraction        | 0.000403     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 64.7         |
|    n_updates            | 5710         |
|    policy_gradient_loss | -0.000831    |
|    std                  | 3.08         |
|    value_loss           | 38.4         |
------------------------------------------
Eval num_timesteps=4685000, episode_reward=-307.62 +/- 175.57
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 4685000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 572     |
|    time_elapsed    | 23840   |
|    total_timesteps | 4685824 |
--------------------------------
Eval num_timesteps=4690000, episode_reward=-581.05 +/- 51.57
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -581         |
| time/                   |              |
|    total_timesteps      | 4690000      |
| train/                  |              |
|    approx_kl            | 0.0031166226 |
|    clip_fraction        | 0.0066       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.63        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 78.4         |
|    n_updates            | 5720         |
|    policy_gradient_loss | -0.00111     |
|    std                  | 3.08         |
|    value_loss           | 266          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 573     |
|    time_elapsed    | 23882   |
|    total_timesteps | 4694016 |
--------------------------------
Eval num_timesteps=4695000, episode_reward=-263.63 +/- 98.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -264         |
| time/                   |              |
|    total_timesteps      | 4695000      |
| train/                  |              |
|    approx_kl            | 0.0024010513 |
|    clip_fraction        | 0.00864      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.64        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0653       |
|    n_updates            | 5730         |
|    policy_gradient_loss | -0.000788    |
|    std                  | 3.09         |
|    value_loss           | 61.1         |
------------------------------------------
Eval num_timesteps=4700000, episode_reward=-427.70 +/- 136.07
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 4700000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 574     |
|    time_elapsed    | 23934   |
|    total_timesteps | 4702208 |
--------------------------------
Eval num_timesteps=4705000, episode_reward=-406.90 +/- 160.14
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 4705000     |
| train/                  |             |
|    approx_kl            | 0.002972035 |
|    clip_fraction        | 0.00295     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.64       |
|    explained_variance   | 0.985       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.951       |
|    n_updates            | 5740        |
|    policy_gradient_loss | -0.00115    |
|    std                  | 3.1         |
|    value_loss           | 254         |
-----------------------------------------
Eval num_timesteps=4710000, episode_reward=-361.08 +/- 172.21
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -361     |
| time/              |          |
|    total_timesteps | 4710000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 575     |
|    time_elapsed    | 23985   |
|    total_timesteps | 4710400 |
--------------------------------
Eval num_timesteps=4715000, episode_reward=-318.91 +/- 133.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 4715000      |
| train/                  |              |
|    approx_kl            | 0.0035398016 |
|    clip_fraction        | 0.00785      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.65        |
|    explained_variance   | 0.992        |
|    learning_rate        | 5e-05        |
|    loss                 | 14.8         |
|    n_updates            | 5750         |
|    policy_gradient_loss | -0.000801    |
|    std                  | 3.1          |
|    value_loss           | 178          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 576     |
|    time_elapsed    | 24026   |
|    total_timesteps | 4718592 |
--------------------------------
Eval num_timesteps=4720000, episode_reward=-315.46 +/- 177.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -315         |
| time/                   |              |
|    total_timesteps      | 4720000      |
| train/                  |              |
|    approx_kl            | 0.0035180924 |
|    clip_fraction        | 0.015        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.412        |
|    n_updates            | 5760         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 3.12         |
|    value_loss           | 1.21         |
------------------------------------------
Eval num_timesteps=4725000, episode_reward=-293.23 +/- 77.03
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 4725000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 577     |
|    time_elapsed    | 24079   |
|    total_timesteps | 4726784 |
--------------------------------
Eval num_timesteps=4730000, episode_reward=-404.94 +/- 166.25
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -405          |
| time/                   |               |
|    total_timesteps      | 4730000       |
| train/                  |               |
|    approx_kl            | 0.00041414695 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.66         |
|    explained_variance   | 0.991         |
|    learning_rate        | 5e-05         |
|    loss                 | 41.1          |
|    n_updates            | 5770          |
|    policy_gradient_loss | -0.000297     |
|    std                  | 3.11          |
|    value_loss           | 205           |
-------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 578     |
|    time_elapsed    | 24122   |
|    total_timesteps | 4734976 |
--------------------------------
Eval num_timesteps=4735000, episode_reward=-288.13 +/- 212.15
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -288         |
| time/                   |              |
|    total_timesteps      | 4735000      |
| train/                  |              |
|    approx_kl            | 0.0041269064 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 1            |
|    learning_rate        | 5e-05        |
|    loss                 | 0.278        |
|    n_updates            | 5780         |
|    policy_gradient_loss | -0.00192     |
|    std                  | 3.11         |
|    value_loss           | 0.665        |
------------------------------------------
Eval num_timesteps=4740000, episode_reward=-414.87 +/- 159.16
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 4740000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 579     |
|    time_elapsed    | 24175   |
|    total_timesteps | 4743168 |
--------------------------------
Eval num_timesteps=4745000, episode_reward=-245.89 +/- 74.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -246         |
| time/                   |              |
|    total_timesteps      | 4745000      |
| train/                  |              |
|    approx_kl            | 0.0023921835 |
|    clip_fraction        | 0.00502      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 3.65         |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.000583    |
|    std                  | 3.11         |
|    value_loss           | 70.8         |
------------------------------------------
Eval num_timesteps=4750000, episode_reward=-333.33 +/- 112.38
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 4750000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 580     |
|    time_elapsed    | 24228   |
|    total_timesteps | 4751360 |
--------------------------------
Eval num_timesteps=4755000, episode_reward=-333.54 +/- 188.29
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -334       |
| time/                   |            |
|    total_timesteps      | 4755000    |
| train/                  |            |
|    approx_kl            | 0.00325078 |
|    clip_fraction        | 0.00831    |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.66      |
|    explained_variance   | 0.992      |
|    learning_rate        | 5e-05      |
|    loss                 | 81.6       |
|    n_updates            | 5800       |
|    policy_gradient_loss | -0.00127   |
|    std                  | 3.11       |
|    value_loss           | 215        |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 581     |
|    time_elapsed    | 24272   |
|    total_timesteps | 4759552 |
--------------------------------
Eval num_timesteps=4760000, episode_reward=-375.15 +/- 140.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -375         |
| time/                   |              |
|    total_timesteps      | 4760000      |
| train/                  |              |
|    approx_kl            | 0.0031663522 |
|    clip_fraction        | 0.00879      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.66        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 80.3         |
|    n_updates            | 5810         |
|    policy_gradient_loss | -0.00065     |
|    std                  | 3.12         |
|    value_loss           | 65.8         |
------------------------------------------
Eval num_timesteps=4765000, episode_reward=-287.85 +/- 86.63
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 4765000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 582     |
|    time_elapsed    | 24325   |
|    total_timesteps | 4767744 |
--------------------------------
Eval num_timesteps=4770000, episode_reward=-323.84 +/- 102.80
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -324          |
| time/                   |               |
|    total_timesteps      | 4770000       |
| train/                  |               |
|    approx_kl            | 0.00030177212 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -7.67         |
|    explained_variance   | 0.986         |
|    learning_rate        | 5e-05         |
|    loss                 | 13.5          |
|    n_updates            | 5820          |
|    policy_gradient_loss | -2.37e-05     |
|    std                  | 3.12          |
|    value_loss           | 247           |
-------------------------------------------
Eval num_timesteps=4775000, episode_reward=-323.43 +/- 110.95
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -323     |
| time/              |          |
|    total_timesteps | 4775000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 583     |
|    time_elapsed    | 24378   |
|    total_timesteps | 4775936 |
--------------------------------
Eval num_timesteps=4780000, episode_reward=-275.00 +/- 58.06
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -275         |
| time/                   |              |
|    total_timesteps      | 4780000      |
| train/                  |              |
|    approx_kl            | 0.0028515914 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.67        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 75.2         |
|    n_updates            | 5830         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 3.12         |
|    value_loss           | 246          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 584     |
|    time_elapsed    | 24423   |
|    total_timesteps | 4784128 |
--------------------------------
Eval num_timesteps=4785000, episode_reward=-292.93 +/- 89.34
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -293         |
| time/                   |              |
|    total_timesteps      | 4785000      |
| train/                  |              |
|    approx_kl            | 0.0029274921 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.68        |
|    explained_variance   | 0.99         |
|    learning_rate        | 5e-05        |
|    loss                 | 2.46         |
|    n_updates            | 5840         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 3.14         |
|    value_loss           | 201          |
------------------------------------------
Eval num_timesteps=4790000, episode_reward=-297.02 +/- 194.73
Episode length: 810.20 +/- 381.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 810      |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 4790000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 585     |
|    time_elapsed    | 24474   |
|    total_timesteps | 4792320 |
--------------------------------
Eval num_timesteps=4795000, episode_reward=-423.79 +/- 112.14
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -424         |
| time/                   |              |
|    total_timesteps      | 4795000      |
| train/                  |              |
|    approx_kl            | 0.0009767896 |
|    clip_fraction        | 7.32e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 0.995        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.274        |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.000219    |
|    std                  | 3.14         |
|    value_loss           | 106          |
------------------------------------------
Eval num_timesteps=4800000, episode_reward=-286.68 +/- 161.37
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 4800000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 586     |
|    time_elapsed    | 24528   |
|    total_timesteps | 4800512 |
--------------------------------
Eval num_timesteps=4805000, episode_reward=-332.96 +/- 143.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -333         |
| time/                   |              |
|    total_timesteps      | 4805000      |
| train/                  |              |
|    approx_kl            | 0.0024263451 |
|    clip_fraction        | 0.00334      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.857        |
|    n_updates            | 5860         |
|    policy_gradient_loss | -0.000261    |
|    std                  | 3.15         |
|    value_loss           | 38.5         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 587     |
|    time_elapsed    | 24573   |
|    total_timesteps | 4808704 |
--------------------------------
Eval num_timesteps=4810000, episode_reward=-396.30 +/- 158.11
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -396         |
| time/                   |              |
|    total_timesteps      | 4810000      |
| train/                  |              |
|    approx_kl            | 0.0033703772 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.7         |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 39.1         |
|    n_updates            | 5870         |
|    policy_gradient_loss | -0.000792    |
|    std                  | 3.15         |
|    value_loss           | 104          |
------------------------------------------
Eval num_timesteps=4815000, episode_reward=-294.23 +/- 109.32
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 4815000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 588     |
|    time_elapsed    | 24627   |
|    total_timesteps | 4816896 |
--------------------------------
Eval num_timesteps=4820000, episode_reward=-249.61 +/- 103.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -250         |
| time/                   |              |
|    total_timesteps      | 4820000      |
| train/                  |              |
|    approx_kl            | 0.0029777256 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.69        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.37         |
|    n_updates            | 5880         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 3.15         |
|    value_loss           | 114          |
------------------------------------------
Eval num_timesteps=4825000, episode_reward=-242.30 +/- 98.24
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 4825000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 589     |
|    time_elapsed    | 24682   |
|    total_timesteps | 4825088 |
--------------------------------
Eval num_timesteps=4830000, episode_reward=-289.47 +/- 137.85
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -289        |
| time/                   |             |
|    total_timesteps      | 4830000     |
| train/                  |             |
|    approx_kl            | 0.002010942 |
|    clip_fraction        | 0.00399     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.69       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 11.8        |
|    n_updates            | 5890        |
|    policy_gradient_loss | -0.00105    |
|    std                  | 3.15        |
|    value_loss           | 78          |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 590     |
|    time_elapsed    | 24726   |
|    total_timesteps | 4833280 |
--------------------------------
Eval num_timesteps=4835000, episode_reward=-254.45 +/- 86.43
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -254       |
| time/                   |            |
|    total_timesteps      | 4835000    |
| train/                  |            |
|    approx_kl            | 0.00416567 |
|    clip_fraction        | 0.016      |
|    clip_range           | 0.2        |
|    entropy_loss         | -7.7       |
|    explained_variance   | 0.993      |
|    learning_rate        | 5e-05      |
|    loss                 | 131        |
|    n_updates            | 5900       |
|    policy_gradient_loss | -0.000723  |
|    std                  | 3.15       |
|    value_loss           | 111        |
----------------------------------------
Eval num_timesteps=4840000, episode_reward=-431.52 +/- 155.14
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 4840000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 591     |
|    time_elapsed    | 24780   |
|    total_timesteps | 4841472 |
--------------------------------
Eval num_timesteps=4845000, episode_reward=-320.33 +/- 87.99
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -320        |
| time/                   |             |
|    total_timesteps      | 4845000     |
| train/                  |             |
|    approx_kl            | 0.003822063 |
|    clip_fraction        | 0.0191      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.7        |
|    explained_variance   | 0.997       |
|    learning_rate        | 5e-05       |
|    loss                 | 4.29        |
|    n_updates            | 5910        |
|    policy_gradient_loss | -0.00138    |
|    std                  | 3.16        |
|    value_loss           | 49.9        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 592     |
|    time_elapsed    | 24823   |
|    total_timesteps | 4849664 |
--------------------------------
Eval num_timesteps=4850000, episode_reward=-280.23 +/- 185.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -280         |
| time/                   |              |
|    total_timesteps      | 4850000      |
| train/                  |              |
|    approx_kl            | 0.0046123196 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.71        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.104        |
|    n_updates            | 5920         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 3.17         |
|    value_loss           | 170          |
------------------------------------------
Eval num_timesteps=4855000, episode_reward=-411.85 +/- 188.18
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 4855000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 593     |
|    time_elapsed    | 24877   |
|    total_timesteps | 4857856 |
--------------------------------
Eval num_timesteps=4860000, episode_reward=-249.00 +/- 78.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -249        |
| time/                   |             |
|    total_timesteps      | 4860000     |
| train/                  |             |
|    approx_kl            | 0.002478741 |
|    clip_fraction        | 0.00767     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.305       |
|    n_updates            | 5930        |
|    policy_gradient_loss | -0.000979   |
|    std                  | 3.17        |
|    value_loss           | 153         |
-----------------------------------------
Eval num_timesteps=4865000, episode_reward=-257.28 +/- 118.29
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 4865000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 594     |
|    time_elapsed    | 24932   |
|    total_timesteps | 4866048 |
--------------------------------
Eval num_timesteps=4870000, episode_reward=-245.35 +/- 55.79
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -245        |
| time/                   |             |
|    total_timesteps      | 4870000     |
| train/                  |             |
|    approx_kl            | 0.002235442 |
|    clip_fraction        | 0.0152      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.72       |
|    explained_variance   | 0.989       |
|    learning_rate        | 5e-05       |
|    loss                 | 93.2        |
|    n_updates            | 5940        |
|    policy_gradient_loss | -0.00151    |
|    std                  | 3.18        |
|    value_loss           | 278         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 595     |
|    time_elapsed    | 24976   |
|    total_timesteps | 4874240 |
--------------------------------
Eval num_timesteps=4875000, episode_reward=-279.88 +/- 32.84
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -280         |
| time/                   |              |
|    total_timesteps      | 4875000      |
| train/                  |              |
|    approx_kl            | 0.0041633463 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.72        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 7.8          |
|    n_updates            | 5950         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 3.18         |
|    value_loss           | 55.4         |
------------------------------------------
Eval num_timesteps=4880000, episode_reward=-280.65 +/- 139.35
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -281     |
| time/              |          |
|    total_timesteps | 4880000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 596     |
|    time_elapsed    | 25031   |
|    total_timesteps | 4882432 |
--------------------------------
Eval num_timesteps=4885000, episode_reward=-252.82 +/- 169.32
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -253        |
| time/                   |             |
|    total_timesteps      | 4885000     |
| train/                  |             |
|    approx_kl            | 0.004876421 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.73       |
|    explained_variance   | 0.993       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.128       |
|    n_updates            | 5960        |
|    policy_gradient_loss | -0.00322    |
|    std                  | 3.19        |
|    value_loss           | 87.3        |
-----------------------------------------
Eval num_timesteps=4890000, episode_reward=-220.10 +/- 85.18
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 4890000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 597     |
|    time_elapsed    | 25084   |
|    total_timesteps | 4890624 |
--------------------------------
Eval num_timesteps=4895000, episode_reward=-207.43 +/- 150.33
Episode length: 801.60 +/- 398.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 802          |
|    mean_reward          | -207         |
| time/                   |              |
|    total_timesteps      | 4895000      |
| train/                  |              |
|    approx_kl            | 0.0032362542 |
|    clip_fraction        | 0.0134       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.22         |
|    n_updates            | 5970         |
|    policy_gradient_loss | -0.000703    |
|    std                  | 3.19         |
|    value_loss           | 25.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 598     |
|    time_elapsed    | 25126   |
|    total_timesteps | 4898816 |
--------------------------------
Eval num_timesteps=4900000, episode_reward=-235.25 +/- 175.75
Episode length: 810.80 +/- 380.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -235         |
| time/                   |              |
|    total_timesteps      | 4900000      |
| train/                  |              |
|    approx_kl            | 0.0033317949 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.73        |
|    explained_variance   | 0.996        |
|    learning_rate        | 5e-05        |
|    loss                 | 13.8         |
|    n_updates            | 5980         |
|    policy_gradient_loss | -0.000954    |
|    std                  | 3.19         |
|    value_loss           | 112          |
------------------------------------------
Eval num_timesteps=4905000, episode_reward=-326.37 +/- 121.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -326     |
| time/              |          |
|    total_timesteps | 4905000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 599     |
|    time_elapsed    | 25180   |
|    total_timesteps | 4907008 |
--------------------------------
Eval num_timesteps=4910000, episode_reward=-444.22 +/- 126.10
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -444         |
| time/                   |              |
|    total_timesteps      | 4910000      |
| train/                  |              |
|    approx_kl            | 0.0027896129 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 39.3         |
|    n_updates            | 5990         |
|    policy_gradient_loss | -0.000228    |
|    std                  | 3.19         |
|    value_loss           | 76.4         |
------------------------------------------
Eval num_timesteps=4915000, episode_reward=-476.47 +/- 104.13
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 4915000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 600     |
|    time_elapsed    | 25233   |
|    total_timesteps | 4915200 |
--------------------------------
Eval num_timesteps=4920000, episode_reward=-337.36 +/- 138.60
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -337         |
| time/                   |              |
|    total_timesteps      | 4920000      |
| train/                  |              |
|    approx_kl            | 0.0039587463 |
|    clip_fraction        | 0.0178       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0.993        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0288       |
|    n_updates            | 6000         |
|    policy_gradient_loss | -0.00243     |
|    std                  | 3.19         |
|    value_loss           | 97.5         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 601     |
|    time_elapsed    | 25277   |
|    total_timesteps | 4923392 |
--------------------------------
Eval num_timesteps=4925000, episode_reward=-378.56 +/- 203.50
Episode length: 806.80 +/- 388.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 807          |
|    mean_reward          | -379         |
| time/                   |              |
|    total_timesteps      | 4925000      |
| train/                  |              |
|    approx_kl            | 0.0039620562 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.74        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.0389       |
|    n_updates            | 6010         |
|    policy_gradient_loss | -0.00187     |
|    std                  | 3.2          |
|    value_loss           | 28.6         |
------------------------------------------
Eval num_timesteps=4930000, episode_reward=-310.05 +/- 132.33
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 4930000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 602     |
|    time_elapsed    | 25328   |
|    total_timesteps | 4931584 |
--------------------------------
Eval num_timesteps=4935000, episode_reward=-500.28 +/- 112.83
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -500        |
| time/                   |             |
|    total_timesteps      | 4935000     |
| train/                  |             |
|    approx_kl            | 0.002954013 |
|    clip_fraction        | 0.00457     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.75       |
|    explained_variance   | 0.988       |
|    learning_rate        | 5e-05       |
|    loss                 | 7.47        |
|    n_updates            | 6020        |
|    policy_gradient_loss | -0.000719   |
|    std                  | 3.21        |
|    value_loss           | 219         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 603     |
|    time_elapsed    | 25372   |
|    total_timesteps | 4939776 |
--------------------------------
Eval num_timesteps=4940000, episode_reward=-337.26 +/- 243.86
Episode length: 808.80 +/- 384.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 809          |
|    mean_reward          | -337         |
| time/                   |              |
|    total_timesteps      | 4940000      |
| train/                  |              |
|    approx_kl            | 0.0041867606 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 0.999        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.555        |
|    n_updates            | 6030         |
|    policy_gradient_loss | -0.00135     |
|    std                  | 3.21         |
|    value_loss           | 28           |
------------------------------------------
Eval num_timesteps=4945000, episode_reward=-330.84 +/- 100.97
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -331     |
| time/              |          |
|    total_timesteps | 4945000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 604     |
|    time_elapsed    | 25424   |
|    total_timesteps | 4947968 |
--------------------------------
Eval num_timesteps=4950000, episode_reward=-313.12 +/- 153.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -313         |
| time/                   |              |
|    total_timesteps      | 4950000      |
| train/                  |              |
|    approx_kl            | 0.0016394511 |
|    clip_fraction        | 0.00033      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 0.983        |
|    learning_rate        | 5e-05        |
|    loss                 | 25.5         |
|    n_updates            | 6040         |
|    policy_gradient_loss | -0.000589    |
|    std                  | 3.22         |
|    value_loss           | 308          |
------------------------------------------
Eval num_timesteps=4955000, episode_reward=-324.63 +/- 141.25
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -325     |
| time/              |          |
|    total_timesteps | 4955000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 605     |
|    time_elapsed    | 25477   |
|    total_timesteps | 4956160 |
--------------------------------
Eval num_timesteps=4960000, episode_reward=-270.98 +/- 186.97
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -271         |
| time/                   |              |
|    total_timesteps      | 4960000      |
| train/                  |              |
|    approx_kl            | 0.0013357631 |
|    clip_fraction        | 0.00166      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 0.991        |
|    learning_rate        | 5e-05        |
|    loss                 | 2.66         |
|    n_updates            | 6050         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 3.21         |
|    value_loss           | 169          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 606     |
|    time_elapsed    | 25523   |
|    total_timesteps | 4964352 |
--------------------------------
Eval num_timesteps=4965000, episode_reward=-273.64 +/- 156.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -274         |
| time/                   |              |
|    total_timesteps      | 4965000      |
| train/                  |              |
|    approx_kl            | 0.0025795046 |
|    clip_fraction        | 0.00988      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 1.2          |
|    n_updates            | 6060         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 3.22         |
|    value_loss           | 54.1         |
------------------------------------------
Eval num_timesteps=4970000, episode_reward=-346.72 +/- 86.45
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 4970000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 607     |
|    time_elapsed    | 25576   |
|    total_timesteps | 4972544 |
--------------------------------
Eval num_timesteps=4975000, episode_reward=-311.14 +/- 206.59
Episode length: 809.00 +/- 384.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 809         |
|    mean_reward          | -311        |
| time/                   |             |
|    total_timesteps      | 4975000     |
| train/                  |             |
|    approx_kl            | 0.004207774 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.77       |
|    explained_variance   | 0.996       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.162       |
|    n_updates            | 6070        |
|    policy_gradient_loss | -0.00166    |
|    std                  | 3.23        |
|    value_loss           | 72.4        |
-----------------------------------------
Eval num_timesteps=4980000, episode_reward=-201.84 +/- 83.92
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 4980000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 608     |
|    time_elapsed    | 25629   |
|    total_timesteps | 4980736 |
--------------------------------
Eval num_timesteps=4985000, episode_reward=-319.30 +/- 59.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 4985000      |
| train/                  |              |
|    approx_kl            | 0.0036636603 |
|    clip_fraction        | 0.0174       |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.77        |
|    explained_variance   | 0.998        |
|    learning_rate        | 5e-05        |
|    loss                 | 0.475        |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 3.23         |
|    value_loss           | 46.8         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 609     |
|    time_elapsed    | 25672   |
|    total_timesteps | 4988928 |
--------------------------------
Eval num_timesteps=4990000, episode_reward=-369.91 +/- 162.77
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -370         |
| time/                   |              |
|    total_timesteps      | 4990000      |
| train/                  |              |
|    approx_kl            | 0.0037074909 |
|    clip_fraction        | 0.00702      |
|    clip_range           | 0.2          |
|    entropy_loss         | -7.76        |
|    explained_variance   | 0.997        |
|    learning_rate        | 5e-05        |
|    loss                 | 69.7         |
|    n_updates            | 6090         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 3.21         |
|    value_loss           | 26.9         |
------------------------------------------
Eval num_timesteps=4995000, episode_reward=-271.78 +/- 130.46
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 4995000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 610     |
|    time_elapsed    | 25727   |
|    total_timesteps | 4997120 |
--------------------------------
Eval num_timesteps=5000000, episode_reward=-269.02 +/- 165.95
Episode length: 813.60 +/- 374.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | -269        |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.002252453 |
|    clip_fraction        | 0.00112     |
|    clip_range           | 0.2         |
|    entropy_loss         | -7.76       |
|    explained_variance   | 0.999       |
|    learning_rate        | 5e-05       |
|    loss                 | 0.164       |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.000646   |
|    std                  | 3.22        |
|    value_loss           | 16          |
-----------------------------------------
Eval num_timesteps=5005000, episode_reward=-312.37 +/- 182.03
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 5005000  |
---------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 611     |
|    time_elapsed    | 25779   |
|    total_timesteps | 5005312 |
--------------------------------
Training completed in 25793.88 seconds (~7.16 hours).
Model saved successfully!
