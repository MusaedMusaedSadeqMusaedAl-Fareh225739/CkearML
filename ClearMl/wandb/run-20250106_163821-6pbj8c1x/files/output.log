Using cpu device
-----------------------------
| time/              |      |
|    fps             | 1322 |
|    iterations      | 1    |
|    time_elapsed    | 3    |
|    total_timesteps | 4096 |
-----------------------------
C:\Users\jimal\anaconda3\envs\Y2B\lib\site-packages\stable_baselines3\common\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-508.65 +/- 265.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -509         |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0021003438 |
|    clip_fraction        | 0.0011       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | -0.00313     |
|    learning_rate        | 0.0001       |
|    loss                 | 8.76         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00191     |
|    std                  | 1            |
|    value_loss           | 69.2         |
------------------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 669  |
|    iterations      | 2    |
|    time_elapsed    | 12   |
|    total_timesteps | 8192 |
-----------------------------
Eval num_timesteps=10000, episode_reward=-517.99 +/- 167.64
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -518         |
| time/                   |              |
|    total_timesteps      | 10000        |
| train/                  |              |
|    approx_kl            | 0.0024572632 |
|    clip_fraction        | 0.00244      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.427        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.99         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00103     |
|    std                  | 0.994        |
|    value_loss           | 20.7         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 574   |
|    iterations      | 3     |
|    time_elapsed    | 21    |
|    total_timesteps | 12288 |
------------------------------
Eval num_timesteps=15000, episode_reward=-543.37 +/- 149.55
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -543         |
| time/                   |              |
|    total_timesteps      | 15000        |
| train/                  |              |
|    approx_kl            | 0.0001418096 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.306        |
|    learning_rate        | 0.0001       |
|    loss                 | 8.6          |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.000262    |
|    std                  | 0.993        |
|    value_loss           | 51.3         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 495   |
|    iterations      | 4     |
|    time_elapsed    | 33    |
|    total_timesteps | 16384 |
------------------------------
Eval num_timesteps=20000, episode_reward=-542.33 +/- 238.24
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -542         |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0010525351 |
|    clip_fraction        | 0.000488     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.874        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.53         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.000682    |
|    std                  | 0.993        |
|    value_loss           | 27           |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 465   |
|    iterations      | 5     |
|    time_elapsed    | 44    |
|    total_timesteps | 20480 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 480          |
|    iterations           | 6            |
|    time_elapsed         | 51           |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0013977052 |
|    clip_fraction        | 0.000537     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.493        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.7          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00115     |
|    std                  | 0.993        |
|    value_loss           | 40.8         |
------------------------------------------
Eval num_timesteps=25000, episode_reward=-533.09 +/- 324.32
Episode length: 853.60 +/- 294.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 854          |
|    mean_reward          | -533         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0031633503 |
|    clip_fraction        | 0.0109       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.121        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.06         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00348     |
|    std                  | 0.987        |
|    value_loss           | 24.5         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 450   |
|    iterations      | 7     |
|    time_elapsed    | 63    |
|    total_timesteps | 28672 |
------------------------------
Eval num_timesteps=30000, episode_reward=-544.34 +/- 94.37
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -544        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.002056044 |
|    clip_fraction        | 0.00083     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | -0.042      |
|    learning_rate        | 0.0001      |
|    loss                 | 3.23        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.000745   |
|    std                  | 0.984       |
|    value_loss           | 22.5        |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 442   |
|    iterations      | 8     |
|    time_elapsed    | 74    |
|    total_timesteps | 32768 |
------------------------------
Eval num_timesteps=35000, episode_reward=-474.86 +/- 113.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -475         |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0058473498 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | -0.00702     |
|    learning_rate        | 0.0001       |
|    loss                 | 5.69         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00401     |
|    std                  | 0.996        |
|    value_loss           | 17.8         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 445   |
|    iterations      | 9     |
|    time_elapsed    | 82    |
|    total_timesteps | 36864 |
------------------------------
Eval num_timesteps=40000, episode_reward=-632.15 +/- 126.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -632         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0008497293 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | -0.00881     |
|    learning_rate        | 0.0001       |
|    loss                 | 1.14         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000594    |
|    std                  | 0.996        |
|    value_loss           | 11.6         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 443   |
|    iterations      | 10    |
|    time_elapsed    | 92    |
|    total_timesteps | 40960 |
------------------------------
Eval num_timesteps=45000, episode_reward=-471.96 +/- 197.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -472         |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0032583042 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.00545      |
|    learning_rate        | 0.0001       |
|    loss                 | 2.67         |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.00117     |
|    std                  | 0.999        |
|    value_loss           | 9.57         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 443   |
|    iterations      | 11    |
|    time_elapsed    | 101   |
|    total_timesteps | 45056 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 454         |
|    iterations           | 12          |
|    time_elapsed         | 108         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.004399734 |
|    clip_fraction        | 0.0157      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.457       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.237       |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00219    |
|    std                  | 1           |
|    value_loss           | 4.54        |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-445.73 +/- 170.82
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -446        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.004267911 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.0997      |
|    learning_rate        | 0.0001      |
|    loss                 | 2.67        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00242    |
|    std                  | 1           |
|    value_loss           | 8.86        |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 455   |
|    iterations      | 13    |
|    time_elapsed    | 116   |
|    total_timesteps | 53248 |
------------------------------
Eval num_timesteps=55000, episode_reward=-574.14 +/- 214.00
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -574         |
| time/                   |              |
|    total_timesteps      | 55000        |
| train/                  |              |
|    approx_kl            | 0.0022806067 |
|    clip_fraction        | 0.00112      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.866        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.96         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000598    |
|    std                  | 0.999        |
|    value_loss           | 5.88         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 456   |
|    iterations      | 14    |
|    time_elapsed    | 125   |
|    total_timesteps | 57344 |
------------------------------
Eval num_timesteps=60000, episode_reward=-449.94 +/- 164.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -450         |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0005432102 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.136        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.88         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000477    |
|    std                  | 0.997        |
|    value_loss           | 17.3         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 456   |
|    iterations      | 15    |
|    time_elapsed    | 134   |
|    total_timesteps | 61440 |
------------------------------
Eval num_timesteps=65000, episode_reward=-390.79 +/- 176.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -391        |
| time/                   |             |
|    total_timesteps      | 65000       |
| train/                  |             |
|    approx_kl            | 0.002278218 |
|    clip_fraction        | 0.000684    |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.16        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00108    |
|    std                  | 0.996       |
|    value_loss           | 7.99        |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 457   |
|    iterations      | 16    |
|    time_elapsed    | 143   |
|    total_timesteps | 65536 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 465          |
|    iterations           | 17           |
|    time_elapsed         | 149          |
|    total_timesteps      | 69632        |
| train/                  |              |
|    approx_kl            | 0.0002760181 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.951        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.41         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.000226    |
|    std                  | 0.996        |
|    value_loss           | 17.2         |
------------------------------------------
Eval num_timesteps=70000, episode_reward=-411.90 +/- 62.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.002771976 |
|    clip_fraction        | 0.00732     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.177       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.17        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00108    |
|    std                  | 0.999       |
|    value_loss           | 9.23        |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 461   |
|    iterations      | 18    |
|    time_elapsed    | 159   |
|    total_timesteps | 73728 |
------------------------------
Eval num_timesteps=75000, episode_reward=-517.72 +/- 42.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -518         |
| time/                   |              |
|    total_timesteps      | 75000        |
| train/                  |              |
|    approx_kl            | 0.0032418815 |
|    clip_fraction        | 0.00554      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.86         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.26         |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.00145     |
|    std                  | 0.999        |
|    value_loss           | 7.32         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 457   |
|    iterations      | 19    |
|    time_elapsed    | 169   |
|    total_timesteps | 77824 |
------------------------------
Eval num_timesteps=80000, episode_reward=-459.95 +/- 177.21
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -460       |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.00453733 |
|    clip_fraction        | 0.0165     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.232      |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00162   |
|    std                  | 0.996      |
|    value_loss           | 1.33       |
----------------------------------------
------------------------------
| time/              |       |
|    fps             | 455   |
|    iterations      | 20    |
|    time_elapsed    | 179   |
|    total_timesteps | 81920 |
------------------------------
Eval num_timesteps=85000, episode_reward=-379.61 +/- 75.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -380         |
| time/                   |              |
|    total_timesteps      | 85000        |
| train/                  |              |
|    approx_kl            | 0.0022909553 |
|    clip_fraction        | 0.000708     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.935        |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00148     |
|    std                  | 0.994        |
|    value_loss           | 3.27         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 455   |
|    iterations      | 21    |
|    time_elapsed    | 188   |
|    total_timesteps | 86016 |
------------------------------
Eval num_timesteps=90000, episode_reward=-389.26 +/- 138.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -389         |
| time/                   |              |
|    total_timesteps      | 90000        |
| train/                  |              |
|    approx_kl            | 0.0004964634 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.85         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000625    |
|    std                  | 0.994        |
|    value_loss           | 10.9         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 453   |
|    iterations      | 22    |
|    time_elapsed    | 198   |
|    total_timesteps | 90112 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 459         |
|    iterations           | 23          |
|    time_elapsed         | 204         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.004752026 |
|    clip_fraction        | 0.0213      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.12        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00409    |
|    std                  | 0.995       |
|    value_loss           | 12.9        |
-----------------------------------------
Eval num_timesteps=95000, episode_reward=-432.51 +/- 99.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -433         |
| time/                   |              |
|    total_timesteps      | 95000        |
| train/                  |              |
|    approx_kl            | 0.0031554066 |
|    clip_fraction        | 0.00508      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.69         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.00207     |
|    std                  | 0.992        |
|    value_loss           | 7.51         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 456   |
|    iterations      | 24    |
|    time_elapsed    | 215   |
|    total_timesteps | 98304 |
------------------------------
Eval num_timesteps=100000, episode_reward=-514.03 +/- 182.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -514         |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0052132485 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.08         |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00155     |
|    std                  | 0.994        |
|    value_loss           | 6            |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 452    |
|    iterations      | 25     |
|    time_elapsed    | 226    |
|    total_timesteps | 102400 |
-------------------------------
Eval num_timesteps=105000, episode_reward=-493.40 +/- 130.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -493         |
| time/                   |              |
|    total_timesteps      | 105000       |
| train/                  |              |
|    approx_kl            | 0.0023034944 |
|    clip_fraction        | 0.00581      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.83         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.04         |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.000653    |
|    std                  | 0.994        |
|    value_loss           | 2.64         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 450    |
|    iterations      | 26     |
|    time_elapsed    | 236    |
|    total_timesteps | 106496 |
-------------------------------
Eval num_timesteps=110000, episode_reward=-463.83 +/- 144.80
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -464        |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.002423575 |
|    clip_fraction        | 0.00386     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.549       |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.000992   |
|    std                  | 0.997       |
|    value_loss           | 6.46        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 449    |
|    iterations      | 27     |
|    time_elapsed    | 245    |
|    total_timesteps | 110592 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 455          |
|    iterations           | 28           |
|    time_elapsed         | 251          |
|    total_timesteps      | 114688       |
| train/                  |              |
|    approx_kl            | 0.0026378753 |
|    clip_fraction        | 0.00554      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.963        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.23         |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.000918    |
|    std                  | 0.998        |
|    value_loss           | 7.76         |
------------------------------------------
Eval num_timesteps=115000, episode_reward=-419.63 +/- 165.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -420        |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.003132008 |
|    clip_fraction        | 0.0164      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.144       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00198    |
|    std                  | 0.997       |
|    value_loss           | 1.36        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 455    |
|    iterations      | 29     |
|    time_elapsed    | 260    |
|    total_timesteps | 118784 |
-------------------------------
Eval num_timesteps=120000, episode_reward=-524.98 +/- 104.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -525         |
| time/                   |              |
|    total_timesteps      | 120000       |
| train/                  |              |
|    approx_kl            | 0.0042038243 |
|    clip_fraction        | 0.0128       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.179        |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00209     |
|    std                  | 0.992        |
|    value_loss           | 1.19         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 454    |
|    iterations      | 30     |
|    time_elapsed    | 270    |
|    total_timesteps | 122880 |
-------------------------------
Eval num_timesteps=125000, episode_reward=-444.92 +/- 152.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -445         |
| time/                   |              |
|    total_timesteps      | 125000       |
| train/                  |              |
|    approx_kl            | 0.0041669607 |
|    clip_fraction        | 0.0107       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.88         |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00153     |
|    std                  | 0.989        |
|    value_loss           | 5.45         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 452    |
|    iterations      | 31     |
|    time_elapsed    | 280    |
|    total_timesteps | 126976 |
-------------------------------
Eval num_timesteps=130000, episode_reward=-388.42 +/- 117.08
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -388        |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.001650134 |
|    clip_fraction        | 0.00137     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.148       |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00112    |
|    std                  | 0.987       |
|    value_loss           | 6.94        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 451    |
|    iterations      | 32     |
|    time_elapsed    | 290    |
|    total_timesteps | 131072 |
-------------------------------
Eval num_timesteps=135000, episode_reward=-622.80 +/- 29.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -623         |
| time/                   |              |
|    total_timesteps      | 135000       |
| train/                  |              |
|    approx_kl            | 0.0025748466 |
|    clip_fraction        | 0.00454      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.973        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.184        |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00101     |
|    std                  | 0.976        |
|    value_loss           | 0.973        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 450    |
|    iterations      | 33     |
|    time_elapsed    | 300    |
|    total_timesteps | 135168 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 454          |
|    iterations           | 34           |
|    time_elapsed         | 306          |
|    total_timesteps      | 139264       |
| train/                  |              |
|    approx_kl            | 0.0035386838 |
|    clip_fraction        | 0.00647      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.506        |
|    n_updates            | 330          |
|    policy_gradient_loss | -0.00172     |
|    std                  | 0.973        |
|    value_loss           | 3.57         |
------------------------------------------
Eval num_timesteps=140000, episode_reward=-450.23 +/- 101.18
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -450          |
| time/                   |               |
|    total_timesteps      | 140000        |
| train/                  |               |
|    approx_kl            | 0.00089974777 |
|    clip_fraction        | 0.000806      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.17         |
|    explained_variance   | 0.986         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.185         |
|    n_updates            | 340           |
|    policy_gradient_loss | -0.00057      |
|    std                  | 0.97          |
|    value_loss           | 1.73          |
-------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 454    |
|    iterations      | 35     |
|    time_elapsed    | 315    |
|    total_timesteps | 143360 |
-------------------------------
Eval num_timesteps=145000, episode_reward=-569.05 +/- 48.00
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -569          |
| time/                   |               |
|    total_timesteps      | 145000        |
| train/                  |               |
|    approx_kl            | 0.00079828047 |
|    clip_fraction        | 2.44e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.17         |
|    explained_variance   | 0.964         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.116         |
|    n_updates            | 350           |
|    policy_gradient_loss | 2.02e-05      |
|    std                  | 0.972         |
|    value_loss           | 10.3          |
-------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 454    |
|    iterations      | 36     |
|    time_elapsed    | 324    |
|    total_timesteps | 147456 |
-------------------------------
Eval num_timesteps=150000, episode_reward=-336.95 +/- 86.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -337         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0041008475 |
|    clip_fraction        | 0.00571      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.461        |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00087     |
|    std                  | 0.969        |
|    value_loss           | 3.65         |
------------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 455    |
|    iterations      | 37     |
|    time_elapsed    | 332    |
|    total_timesteps | 151552 |
-------------------------------
Eval num_timesteps=155000, episode_reward=-363.24 +/- 119.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -363        |
| time/                   |             |
|    total_timesteps      | 155000      |
| train/                  |             |
|    approx_kl            | 0.004525045 |
|    clip_fraction        | 0.0151      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0615      |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00121    |
|    std                  | 0.965       |
|    value_loss           | 1.05        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 455    |
|    iterations      | 38     |
|    time_elapsed    | 341    |
|    total_timesteps | 155648 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 460         |
|    iterations           | 39          |
|    time_elapsed         | 347         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.001984493 |
|    clip_fraction        | 0.00256     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.141       |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00136    |
|    std                  | 0.966       |
|    value_loss           | 2.24        |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=-392.22 +/- 70.12
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -392         |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0044553094 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.285        |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00369     |
|    std                  | 0.973        |
|    value_loss           | 1.12         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 460    |
|    iterations      | 40     |
|    time_elapsed    | 355    |
|    total_timesteps | 163840 |
-------------------------------
Eval num_timesteps=165000, episode_reward=-538.58 +/- 95.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -539         |
| time/                   |              |
|    total_timesteps      | 165000       |
| train/                  |              |
|    approx_kl            | 0.0052492404 |
|    clip_fraction        | 0.0308       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0421       |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.0038      |
|    std                  | 0.97         |
|    value_loss           | 0.445        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 459    |
|    iterations      | 41     |
|    time_elapsed    | 365    |
|    total_timesteps | 167936 |
-------------------------------
Eval num_timesteps=170000, episode_reward=-342.33 +/- 66.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -342        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.004005393 |
|    clip_fraction        | 0.00759     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.178       |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0022     |
|    std                  | 0.971       |
|    value_loss           | 3.12        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 458    |
|    iterations      | 42     |
|    time_elapsed    | 375    |
|    total_timesteps | 172032 |
-------------------------------
Eval num_timesteps=175000, episode_reward=-435.83 +/- 151.64
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -436        |
| time/                   |             |
|    total_timesteps      | 175000      |
| train/                  |             |
|    approx_kl            | 0.003492059 |
|    clip_fraction        | 0.00569     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.161       |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00138    |
|    std                  | 0.976       |
|    value_loss           | 4.48        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 456    |
|    iterations      | 43     |
|    time_elapsed    | 385    |
|    total_timesteps | 176128 |
-------------------------------
Eval num_timesteps=180000, episode_reward=-462.06 +/- 91.98
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -462         |
| time/                   |              |
|    total_timesteps      | 180000       |
| train/                  |              |
|    approx_kl            | 0.0036088077 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.289        |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00256     |
|    std                  | 0.977        |
|    value_loss           | 4.42         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 455    |
|    iterations      | 44     |
|    time_elapsed    | 395    |
|    total_timesteps | 180224 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 456         |
|    iterations           | 45          |
|    time_elapsed         | 403         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.002754684 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.743       |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00162    |
|    std                  | 0.976       |
|    value_loss           | 2.18        |
-----------------------------------------
Eval num_timesteps=185000, episode_reward=-329.56 +/- 60.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -330        |
| time/                   |             |
|    total_timesteps      | 185000      |
| train/                  |             |
|    approx_kl            | 0.004934307 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.225       |
|    n_updates            | 450         |
|    policy_gradient_loss | -0.00309    |
|    std                  | 0.981       |
|    value_loss           | 1.38        |
-----------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 455    |
|    iterations      | 46     |
|    time_elapsed    | 413    |
|    total_timesteps | 188416 |
-------------------------------
Eval num_timesteps=190000, episode_reward=-406.80 +/- 56.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -407         |
| time/                   |              |
|    total_timesteps      | 190000       |
| train/                  |              |
|    approx_kl            | 0.0048705265 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0996       |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.00263     |
|    std                  | 0.984        |
|    value_loss           | 0.626        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 456    |
|    iterations      | 47     |
|    time_elapsed    | 421    |
|    total_timesteps | 192512 |
-------------------------------
Eval num_timesteps=195000, episode_reward=-299.61 +/- 18.46
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -300        |
| time/                   |             |
|    total_timesteps      | 195000      |
| train/                  |             |
|    approx_kl            | 0.005561019 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0642      |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00254    |
|    std                  | 0.988       |
|    value_loss           | 0.402       |
-----------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 456    |
|    iterations      | 48     |
|    time_elapsed    | 430    |
|    total_timesteps | 196608 |
-------------------------------
Eval num_timesteps=200000, episode_reward=-432.70 +/- 100.11
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -433         |
| time/                   |              |
|    total_timesteps      | 200000       |
| train/                  |              |
|    approx_kl            | 0.0058824774 |
|    clip_fraction        | 0.0342       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.164        |
|    n_updates            | 480          |
|    policy_gradient_loss | -0.00391     |
|    std                  | 0.993        |
|    value_loss           | 0.847        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 75     |
|    iterations      | 49     |
|    time_elapsed    | 2674   |
|    total_timesteps | 200704 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 75           |
|    iterations           | 50           |
|    time_elapsed         | 2705         |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0042464333 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.586        |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.00197     |
|    std                  | 0.994        |
|    value_loss           | 3.07         |
------------------------------------------
Eval num_timesteps=205000, episode_reward=-377.94 +/- 54.22
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -378         |
| time/                   |              |
|    total_timesteps      | 205000       |
| train/                  |              |
|    approx_kl            | 0.0048322794 |
|    clip_fraction        | 0.0183       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.157        |
|    n_updates            | 500          |
|    policy_gradient_loss | -0.00279     |
|    std                  | 0.99         |
|    value_loss           | 0.549        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 76     |
|    iterations      | 51     |
|    time_elapsed    | 2747   |
|    total_timesteps | 208896 |
-------------------------------
Eval num_timesteps=210000, episode_reward=-272.33 +/- 44.08
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -272        |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.004009629 |
|    clip_fraction        | 0.0118      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0298      |
|    n_updates            | 510         |
|    policy_gradient_loss | -0.000947   |
|    std                  | 0.995       |
|    value_loss           | 0.421       |
-----------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 76     |
|    iterations      | 52     |
|    time_elapsed    | 2790   |
|    total_timesteps | 212992 |
-------------------------------
Eval num_timesteps=215000, episode_reward=-304.93 +/- 53.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -305         |
| time/                   |              |
|    total_timesteps      | 215000       |
| train/                  |              |
|    approx_kl            | 0.0042286087 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0888       |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00207     |
|    std                  | 1            |
|    value_loss           | 1.12         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 76     |
|    iterations      | 53     |
|    time_elapsed    | 2833   |
|    total_timesteps | 217088 |
-------------------------------
Eval num_timesteps=220000, episode_reward=-392.86 +/- 157.63
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -393        |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.002024503 |
|    clip_fraction        | 0.00186     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.173       |
|    n_updates            | 530         |
|    policy_gradient_loss | -0.000915   |
|    std                  | 1           |
|    value_loss           | 6.65        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 76     |
|    iterations      | 54     |
|    time_elapsed    | 2876   |
|    total_timesteps | 221184 |
-------------------------------
Eval num_timesteps=225000, episode_reward=-226.92 +/- 104.01
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -227        |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.004318982 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.524       |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00178    |
|    std                  | 0.997       |
|    value_loss           | 3.07        |
-----------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 77     |
|    iterations      | 55     |
|    time_elapsed    | 2915   |
|    total_timesteps | 225280 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 77           |
|    iterations           | 56           |
|    time_elapsed         | 2942         |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0024008949 |
|    clip_fraction        | 0.00547      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.138        |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.000862    |
|    std                  | 0.995        |
|    value_loss           | 2.48         |
------------------------------------------
Eval num_timesteps=230000, episode_reward=-377.45 +/- 119.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -377         |
| time/                   |              |
|    total_timesteps      | 230000       |
| train/                  |              |
|    approx_kl            | 0.0033907006 |
|    clip_fraction        | 0.018        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0872       |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.000889    |
|    std                  | 0.997        |
|    value_loss           | 0.507        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 78     |
|    iterations      | 57     |
|    time_elapsed    | 2980   |
|    total_timesteps | 233472 |
-------------------------------
Eval num_timesteps=235000, episode_reward=-389.03 +/- 114.33
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -389        |
| time/                   |             |
|    total_timesteps      | 235000      |
| train/                  |             |
|    approx_kl            | 0.004899812 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0415      |
|    n_updates            | 570         |
|    policy_gradient_loss | -0.00296    |
|    std                  | 1           |
|    value_loss           | 0.267       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 78     |
|    iterations      | 58     |
|    time_elapsed    | 3020   |
|    total_timesteps | 237568 |
-------------------------------
Eval num_timesteps=240000, episode_reward=-395.30 +/- 176.01
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -395        |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.004606108 |
|    clip_fraction        | 0.013       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.311       |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00203    |
|    std                  | 0.999       |
|    value_loss           | 6.16        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 78     |
|    iterations      | 59     |
|    time_elapsed    | 3067   |
|    total_timesteps | 241664 |
-------------------------------
Eval num_timesteps=245000, episode_reward=-327.87 +/- 193.87
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -328         |
| time/                   |              |
|    total_timesteps      | 245000       |
| train/                  |              |
|    approx_kl            | 0.0037373733 |
|    clip_fraction        | 0.00847      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.11         |
|    n_updates            | 590          |
|    policy_gradient_loss | -0.00149     |
|    std                  | 0.995        |
|    value_loss           | 0.479        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 78     |
|    iterations      | 60     |
|    time_elapsed    | 3111   |
|    total_timesteps | 245760 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 79           |
|    iterations           | 61           |
|    time_elapsed         | 3142         |
|    total_timesteps      | 249856       |
| train/                  |              |
|    approx_kl            | 0.0037662676 |
|    clip_fraction        | 0.0165       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.196        |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.00258     |
|    std                  | 0.994        |
|    value_loss           | 1.11         |
------------------------------------------
Eval num_timesteps=250000, episode_reward=-277.59 +/- 113.52
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -278        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.005600118 |
|    clip_fraction        | 0.0377      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0884      |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00431    |
|    std                  | 0.994       |
|    value_loss           | 0.208       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 79     |
|    iterations      | 62     |
|    time_elapsed    | 3187   |
|    total_timesteps | 253952 |
-------------------------------
Eval num_timesteps=255000, episode_reward=-323.58 +/- 265.48
Episode length: 851.20 +/- 299.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 851          |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 255000       |
| train/                  |              |
|    approx_kl            | 0.0054144496 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.0001       |
|    loss                 | 16.3         |
|    n_updates            | 620          |
|    policy_gradient_loss | -0.00341     |
|    std                  | 0.995        |
|    value_loss           | 3.43         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 79     |
|    iterations      | 63     |
|    time_elapsed    | 3228   |
|    total_timesteps | 258048 |
-------------------------------
Eval num_timesteps=260000, episode_reward=-388.78 +/- 75.93
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -389         |
| time/                   |              |
|    total_timesteps      | 260000       |
| train/                  |              |
|    approx_kl            | 0.0027708157 |
|    clip_fraction        | 0.00347      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.144        |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.000828    |
|    std                  | 0.994        |
|    value_loss           | 8.81         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 80     |
|    iterations      | 64     |
|    time_elapsed    | 3270   |
|    total_timesteps | 262144 |
-------------------------------
Eval num_timesteps=265000, episode_reward=-239.26 +/- 50.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -239         |
| time/                   |              |
|    total_timesteps      | 265000       |
| train/                  |              |
|    approx_kl            | 0.0034249302 |
|    clip_fraction        | 0.0141       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.947        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.25         |
|    n_updates            | 640          |
|    policy_gradient_loss | -0.00214     |
|    std                  | 0.991        |
|    value_loss           | 9.24         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 80     |
|    iterations      | 65     |
|    time_elapsed    | 3314   |
|    total_timesteps | 266240 |
-------------------------------
Eval num_timesteps=270000, episode_reward=-400.56 +/- 131.89
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -401        |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.006168974 |
|    clip_fraction        | 0.0444      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0568      |
|    n_updates            | 650         |
|    policy_gradient_loss | -0.00267    |
|    std                  | 0.993       |
|    value_loss           | 0.475       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 80     |
|    iterations      | 66     |
|    time_elapsed    | 3361   |
|    total_timesteps | 270336 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 80          |
|    iterations           | 67          |
|    time_elapsed         | 3392        |
|    total_timesteps      | 274432      |
| train/                  |             |
|    approx_kl            | 0.004529019 |
|    clip_fraction        | 0.0206      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.718       |
|    n_updates            | 660         |
|    policy_gradient_loss | -0.0025     |
|    std                  | 0.993       |
|    value_loss           | 4.85        |
-----------------------------------------
Eval num_timesteps=275000, episode_reward=-292.29 +/- 78.20
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -292        |
| time/                   |             |
|    total_timesteps      | 275000      |
| train/                  |             |
|    approx_kl            | 0.005130543 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0164      |
|    n_updates            | 670         |
|    policy_gradient_loss | -0.00283    |
|    std                  | 0.994       |
|    value_loss           | 0.184       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 81     |
|    iterations      | 68     |
|    time_elapsed    | 3438   |
|    total_timesteps | 278528 |
-------------------------------
Eval num_timesteps=280000, episode_reward=-326.66 +/- 114.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -327         |
| time/                   |              |
|    total_timesteps      | 280000       |
| train/                  |              |
|    approx_kl            | 0.0045105917 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.131        |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00238     |
|    std                  | 0.992        |
|    value_loss           | 0.553        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 81     |
|    iterations      | 69     |
|    time_elapsed    | 3481   |
|    total_timesteps | 282624 |
-------------------------------
Eval num_timesteps=285000, episode_reward=-363.62 +/- 178.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -364         |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0047521424 |
|    clip_fraction        | 0.0242       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.091        |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.994        |
|    value_loss           | 0.402        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 81     |
|    iterations      | 70     |
|    time_elapsed    | 3522   |
|    total_timesteps | 286720 |
-------------------------------
Eval num_timesteps=290000, episode_reward=-333.18 +/- 142.84
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -333        |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.004184028 |
|    clip_fraction        | 0.0134      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0961      |
|    n_updates            | 700         |
|    policy_gradient_loss | -0.00164    |
|    std                  | 0.995       |
|    value_loss           | 2.33        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 81     |
|    iterations      | 71     |
|    time_elapsed    | 3560   |
|    total_timesteps | 290816 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 82           |
|    iterations           | 72           |
|    time_elapsed         | 3592         |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.0038744952 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0458       |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00206     |
|    std                  | 1            |
|    value_loss           | 0.536        |
------------------------------------------
Eval num_timesteps=295000, episode_reward=-389.42 +/- 184.03
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -389        |
| time/                   |             |
|    total_timesteps      | 295000      |
| train/                  |             |
|    approx_kl            | 0.003988629 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0254      |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.00142    |
|    std                  | 1           |
|    value_loss           | 0.372       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 73     |
|    time_elapsed    | 3636   |
|    total_timesteps | 299008 |
-------------------------------
Eval num_timesteps=300000, episode_reward=-375.74 +/- 116.07
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -376         |
| time/                   |              |
|    total_timesteps      | 300000       |
| train/                  |              |
|    approx_kl            | 0.0053934245 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.116        |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00336     |
|    std                  | 0.997        |
|    value_loss           | 0.529        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 74     |
|    time_elapsed    | 3685   |
|    total_timesteps | 303104 |
-------------------------------
Eval num_timesteps=305000, episode_reward=-423.55 +/- 155.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -424         |
| time/                   |              |
|    total_timesteps      | 305000       |
| train/                  |              |
|    approx_kl            | 0.0047528204 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0331       |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.00428     |
|    std                  | 0.999        |
|    value_loss           | 0.166        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 75     |
|    time_elapsed    | 3729   |
|    total_timesteps | 307200 |
-------------------------------
Eval num_timesteps=310000, episode_reward=-356.48 +/- 216.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -356         |
| time/                   |              |
|    total_timesteps      | 310000       |
| train/                  |              |
|    approx_kl            | 0.0043117665 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0841       |
|    n_updates            | 750          |
|    policy_gradient_loss | -0.00125     |
|    std                  | 1            |
|    value_loss           | 3.77         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 81     |
|    iterations      | 76     |
|    time_elapsed    | 3838   |
|    total_timesteps | 311296 |
-------------------------------
Eval num_timesteps=315000, episode_reward=-333.99 +/- 160.90
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -334        |
| time/                   |             |
|    total_timesteps      | 315000      |
| train/                  |             |
|    approx_kl            | 0.004627807 |
|    clip_fraction        | 0.00908     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.155       |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.0011     |
|    std                  | 1           |
|    value_loss           | 10.4        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 81     |
|    iterations      | 77     |
|    time_elapsed    | 3879   |
|    total_timesteps | 315392 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 78          |
|    time_elapsed         | 3907        |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.004258127 |
|    clip_fraction        | 0.0181      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 7.87        |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00168    |
|    std                  | 1           |
|    value_loss           | 3.27        |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=-282.91 +/- 171.47
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -283         |
| time/                   |              |
|    total_timesteps      | 320000       |
| train/                  |              |
|    approx_kl            | 0.0077182804 |
|    clip_fraction        | 0.0785       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0289       |
|    n_updates            | 780          |
|    policy_gradient_loss | -0.00695     |
|    std                  | 1            |
|    value_loss           | 0.112        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 81     |
|    iterations      | 79     |
|    time_elapsed    | 3952   |
|    total_timesteps | 323584 |
-------------------------------
Eval num_timesteps=325000, episode_reward=-323.00 +/- 148.02
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -323        |
| time/                   |             |
|    total_timesteps      | 325000      |
| train/                  |             |
|    approx_kl            | 0.004510066 |
|    clip_fraction        | 0.0131      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.132       |
|    n_updates            | 790         |
|    policy_gradient_loss | -0.00239    |
|    std                  | 1           |
|    value_loss           | 11.7        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 81     |
|    iterations      | 80     |
|    time_elapsed    | 3998   |
|    total_timesteps | 327680 |
-------------------------------
Eval num_timesteps=330000, episode_reward=-274.01 +/- 193.73
Episode length: 827.80 +/- 346.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 828        |
|    mean_reward          | -274       |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.00686445 |
|    clip_fraction        | 0.0374     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.27      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0001     |
|    loss                 | 2.18       |
|    n_updates            | 800        |
|    policy_gradient_loss | -0.00508   |
|    std                  | 1          |
|    value_loss           | 4.69       |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 81     |
|    time_elapsed    | 4041   |
|    total_timesteps | 331776 |
-------------------------------
Eval num_timesteps=335000, episode_reward=-209.89 +/- 68.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -210         |
| time/                   |              |
|    total_timesteps      | 335000       |
| train/                  |              |
|    approx_kl            | 0.0048524523 |
|    clip_fraction        | 0.0277       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.311        |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.00294     |
|    std                  | 1            |
|    value_loss           | 2.58         |
------------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 82     |
|    time_elapsed    | 4088   |
|    total_timesteps | 335872 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 82           |
|    iterations           | 83           |
|    time_elapsed         | 4118         |
|    total_timesteps      | 339968       |
| train/                  |              |
|    approx_kl            | 0.0061024725 |
|    clip_fraction        | 0.0406       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0171       |
|    n_updates            | 820          |
|    policy_gradient_loss | -0.00409     |
|    std                  | 1.01         |
|    value_loss           | 0.537        |
------------------------------------------
Eval num_timesteps=340000, episode_reward=-146.44 +/- 126.12
Episode length: 645.40 +/- 435.91
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 645          |
|    mean_reward          | -146         |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0054321853 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.97         |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00155     |
|    std                  | 1.01         |
|    value_loss           | 7.67         |
------------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 84     |
|    time_elapsed    | 4158   |
|    total_timesteps | 344064 |
-------------------------------
Eval num_timesteps=345000, episode_reward=-306.14 +/- 117.00
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -306        |
| time/                   |             |
|    total_timesteps      | 345000      |
| train/                  |             |
|    approx_kl            | 0.003038832 |
|    clip_fraction        | 0.00447     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.637       |
|    n_updates            | 840         |
|    policy_gradient_loss | -0.000708   |
|    std                  | 1.01        |
|    value_loss           | 2.45        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 85     |
|    time_elapsed    | 4202   |
|    total_timesteps | 348160 |
-------------------------------
Eval num_timesteps=350000, episode_reward=-478.45 +/- 116.80
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -478        |
| time/                   |             |
|    total_timesteps      | 350000      |
| train/                  |             |
|    approx_kl            | 0.005247196 |
|    clip_fraction        | 0.0313      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0787      |
|    n_updates            | 850         |
|    policy_gradient_loss | -0.0028     |
|    std                  | 1.01        |
|    value_loss           | 0.507       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 86     |
|    time_elapsed    | 4245   |
|    total_timesteps | 352256 |
-------------------------------
Eval num_timesteps=355000, episode_reward=-315.71 +/- 118.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -316         |
| time/                   |              |
|    total_timesteps      | 355000       |
| train/                  |              |
|    approx_kl            | 0.0046017263 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.1          |
|    n_updates            | 860          |
|    policy_gradient_loss | -0.00226     |
|    std                  | 1.01         |
|    value_loss           | 7.13         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 83     |
|    iterations      | 87     |
|    time_elapsed    | 4289   |
|    total_timesteps | 356352 |
-------------------------------
Eval num_timesteps=360000, episode_reward=-294.61 +/- 67.15
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -295         |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0058907196 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.256        |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00377     |
|    std                  | 1            |
|    value_loss           | 0.987        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 83     |
|    iterations      | 88     |
|    time_elapsed    | 4333   |
|    total_timesteps | 360448 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 89          |
|    time_elapsed         | 4361        |
|    total_timesteps      | 364544      |
| train/                  |             |
|    approx_kl            | 0.004141828 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.976       |
|    learning_rate        | 0.0001      |
|    loss                 | 30.4        |
|    n_updates            | 880         |
|    policy_gradient_loss | -0.00279    |
|    std                  | 1.01        |
|    value_loss           | 7.32        |
-----------------------------------------
Eval num_timesteps=365000, episode_reward=-322.53 +/- 209.98
Episode length: 826.80 +/- 348.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 827          |
|    mean_reward          | -323         |
| time/                   |              |
|    total_timesteps      | 365000       |
| train/                  |              |
|    approx_kl            | 0.0037328168 |
|    clip_fraction        | 0.0231       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0235       |
|    n_updates            | 890          |
|    policy_gradient_loss | -0.00123     |
|    std                  | 1.01         |
|    value_loss           | 0.123        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 83     |
|    iterations      | 90     |
|    time_elapsed    | 4402   |
|    total_timesteps | 368640 |
-------------------------------
Eval num_timesteps=370000, episode_reward=-399.10 +/- 144.91
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -399         |
| time/                   |              |
|    total_timesteps      | 370000       |
| train/                  |              |
|    approx_kl            | 0.0033790371 |
|    clip_fraction        | 0.00959      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.59         |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00175     |
|    std                  | 1.01         |
|    value_loss           | 7.83         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 83     |
|    iterations      | 91     |
|    time_elapsed    | 4449   |
|    total_timesteps | 372736 |
-------------------------------
Eval num_timesteps=375000, episode_reward=-335.93 +/- 96.00
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -336         |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0054740235 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.98         |
|    n_updates            | 910          |
|    policy_gradient_loss | -0.00249     |
|    std                  | 1.01         |
|    value_loss           | 3.48         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 83     |
|    iterations      | 92     |
|    time_elapsed    | 4494   |
|    total_timesteps | 376832 |
-------------------------------
Eval num_timesteps=380000, episode_reward=-205.25 +/- 101.80
Episode length: 818.20 +/- 365.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 818          |
|    mean_reward          | -205         |
| time/                   |              |
|    total_timesteps      | 380000       |
| train/                  |              |
|    approx_kl            | 0.0015347025 |
|    clip_fraction        | 0.00808      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0228       |
|    n_updates            | 920          |
|    policy_gradient_loss | -0.000977    |
|    std                  | 1.01         |
|    value_loss           | 1.39         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 84     |
|    iterations      | 93     |
|    time_elapsed    | 4530   |
|    total_timesteps | 380928 |
-------------------------------
Eval num_timesteps=385000, episode_reward=-246.68 +/- 126.88
Episode length: 820.60 +/- 360.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 821          |
|    mean_reward          | -247         |
| time/                   |              |
|    total_timesteps      | 385000       |
| train/                  |              |
|    approx_kl            | 0.0048391586 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.031        |
|    n_updates            | 930          |
|    policy_gradient_loss | -0.00236     |
|    std                  | 1.01         |
|    value_loss           | 0.443        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 84     |
|    iterations      | 94     |
|    time_elapsed    | 4568   |
|    total_timesteps | 385024 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 95          |
|    time_elapsed         | 4598        |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.006043222 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0249      |
|    n_updates            | 940         |
|    policy_gradient_loss | -0.0047     |
|    std                  | 1.01        |
|    value_loss           | 0.146       |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=-398.42 +/- 70.59
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -398         |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 0.0027521048 |
|    clip_fraction        | 0.0222       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0469       |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00162     |
|    std                  | 1.01         |
|    value_loss           | 0.271        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 84     |
|    iterations      | 96     |
|    time_elapsed    | 4641   |
|    total_timesteps | 393216 |
-------------------------------
Eval num_timesteps=395000, episode_reward=-409.72 +/- 168.01
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -410         |
| time/                   |              |
|    total_timesteps      | 395000       |
| train/                  |              |
|    approx_kl            | 0.0024987143 |
|    clip_fraction        | 0.00242      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.291        |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00143     |
|    std                  | 1.01         |
|    value_loss           | 2.53         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 84     |
|    iterations      | 97     |
|    time_elapsed    | 4685   |
|    total_timesteps | 397312 |
-------------------------------
Eval num_timesteps=400000, episode_reward=-449.96 +/- 135.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -450         |
| time/                   |              |
|    total_timesteps      | 400000       |
| train/                  |              |
|    approx_kl            | 0.0057195835 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0927       |
|    n_updates            | 970          |
|    policy_gradient_loss | -0.00391     |
|    std                  | 1.01         |
|    value_loss           | 2.07         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 84     |
|    iterations      | 98     |
|    time_elapsed    | 4728   |
|    total_timesteps | 401408 |
-------------------------------
Eval num_timesteps=405000, episode_reward=-350.34 +/- 138.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -350         |
| time/                   |              |
|    total_timesteps      | 405000       |
| train/                  |              |
|    approx_kl            | 0.0048054764 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.24         |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00321     |
|    std                  | 1.01         |
|    value_loss           | 8.21         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 84     |
|    iterations      | 99     |
|    time_elapsed    | 4771   |
|    total_timesteps | 405504 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 100         |
|    time_elapsed         | 4800        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.004450845 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.229       |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00171    |
|    std                  | 1.01        |
|    value_loss           | 5.9         |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=-193.96 +/- 104.55
Episode length: 811.60 +/- 378.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | -194         |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0054310146 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.194        |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 1.01         |
|    value_loss           | 5.06         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 85     |
|    iterations      | 101    |
|    time_elapsed    | 4842   |
|    total_timesteps | 413696 |
-------------------------------
Eval num_timesteps=415000, episode_reward=-444.21 +/- 166.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -444         |
| time/                   |              |
|    total_timesteps      | 415000       |
| train/                  |              |
|    approx_kl            | 0.0045737475 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0391       |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.00314     |
|    std                  | 1.01         |
|    value_loss           | 0.236        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 85     |
|    iterations      | 102    |
|    time_elapsed    | 4885   |
|    total_timesteps | 417792 |
-------------------------------
Eval num_timesteps=420000, episode_reward=-294.46 +/- 199.51
Episode length: 811.20 +/- 379.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -294         |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0039912686 |
|    clip_fraction        | 0.00884      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 32.6         |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.000845    |
|    std                  | 1.01         |
|    value_loss           | 13.5         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 85     |
|    iterations      | 103    |
|    time_elapsed    | 4924   |
|    total_timesteps | 421888 |
-------------------------------
Eval num_timesteps=425000, episode_reward=-264.61 +/- 114.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -265         |
| time/                   |              |
|    total_timesteps      | 425000       |
| train/                  |              |
|    approx_kl            | 0.0034105731 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.33         |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 1.02         |
|    value_loss           | 2.9          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 85     |
|    iterations      | 104    |
|    time_elapsed    | 4966   |
|    total_timesteps | 425984 |
-------------------------------
Eval num_timesteps=430000, episode_reward=-309.56 +/- 147.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -310         |
| time/                   |              |
|    total_timesteps      | 430000       |
| train/                  |              |
|    approx_kl            | 0.0051576197 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0535       |
|    n_updates            | 1040         |
|    policy_gradient_loss | -0.00221     |
|    std                  | 1.01         |
|    value_loss           | 0.188        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 85     |
|    iterations      | 105    |
|    time_elapsed    | 5008   |
|    total_timesteps | 430080 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 86           |
|    iterations           | 106          |
|    time_elapsed         | 5033         |
|    total_timesteps      | 434176       |
| train/                  |              |
|    approx_kl            | 0.0024233484 |
|    clip_fraction        | 0.00444      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.85         |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.000359    |
|    std                  | 1.01         |
|    value_loss           | 4.21         |
------------------------------------------
Eval num_timesteps=435000, episode_reward=-390.80 +/- 171.11
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -391        |
| time/                   |             |
|    total_timesteps      | 435000      |
| train/                  |             |
|    approx_kl            | 0.004160584 |
|    clip_fraction        | 0.0221      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0973      |
|    n_updates            | 1060        |
|    policy_gradient_loss | -0.00169    |
|    std                  | 1.01        |
|    value_loss           | 0.237       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 57     |
|    iterations      | 107    |
|    time_elapsed    | 7556   |
|    total_timesteps | 438272 |
-------------------------------
Eval num_timesteps=440000, episode_reward=-342.91 +/- 145.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -343         |
| time/                   |              |
|    total_timesteps      | 440000       |
| train/                  |              |
|    approx_kl            | 0.0058714114 |
|    clip_fraction        | 0.0481       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00892      |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00435     |
|    std                  | 1.01         |
|    value_loss           | 0.363        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 58     |
|    iterations      | 108    |
|    time_elapsed    | 7577   |
|    total_timesteps | 442368 |
-------------------------------
Eval num_timesteps=445000, episode_reward=-469.57 +/- 138.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -470         |
| time/                   |              |
|    total_timesteps      | 445000       |
| train/                  |              |
|    approx_kl            | 0.0013154581 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.182        |
|    n_updates            | 1080         |
|    policy_gradient_loss | -6.37e-06    |
|    std                  | 1.01         |
|    value_loss           | 6.29         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 58     |
|    iterations      | 109    |
|    time_elapsed    | 7599   |
|    total_timesteps | 446464 |
-------------------------------
Eval num_timesteps=450000, episode_reward=-256.22 +/- 150.20
Episode length: 821.60 +/- 358.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 822          |
|    mean_reward          | -256         |
| time/                   |              |
|    total_timesteps      | 450000       |
| train/                  |              |
|    approx_kl            | 0.0034215632 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.14         |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00224     |
|    std                  | 1.01         |
|    value_loss           | 11.5         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 59     |
|    iterations      | 110    |
|    time_elapsed    | 7616   |
|    total_timesteps | 450560 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 59          |
|    iterations           | 111         |
|    time_elapsed         | 7622        |
|    total_timesteps      | 454656      |
| train/                  |             |
|    approx_kl            | 0.004758955 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.29       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0469      |
|    n_updates            | 1100        |
|    policy_gradient_loss | -0.00265    |
|    std                  | 1.01        |
|    value_loss           | 0.175       |
-----------------------------------------
Eval num_timesteps=455000, episode_reward=-293.31 +/- 187.56
Episode length: 851.60 +/- 298.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 852         |
|    mean_reward          | -293        |
| time/                   |             |
|    total_timesteps      | 455000      |
| train/                  |             |
|    approx_kl            | 0.004819941 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0309      |
|    n_updates            | 1110        |
|    policy_gradient_loss | -0.00186    |
|    std                  | 1.01        |
|    value_loss           | 0.161       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 60     |
|    iterations      | 112    |
|    time_elapsed    | 7631   |
|    total_timesteps | 458752 |
-------------------------------
Eval num_timesteps=460000, episode_reward=-343.16 +/- 100.22
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -343        |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.004907561 |
|    clip_fraction        | 0.0298      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0259      |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00163    |
|    std                  | 1           |
|    value_loss           | 0.253       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 60     |
|    iterations      | 113    |
|    time_elapsed    | 7642   |
|    total_timesteps | 462848 |
-------------------------------
Eval num_timesteps=465000, episode_reward=-209.63 +/- 42.63
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -210        |
| time/                   |             |
|    total_timesteps      | 465000      |
| train/                  |             |
|    approx_kl            | 0.002129401 |
|    clip_fraction        | 0.00327     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.9         |
|    n_updates            | 1130        |
|    policy_gradient_loss | -0.00102    |
|    std                  | 1           |
|    value_loss           | 3.63        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 61     |
|    iterations      | 114    |
|    time_elapsed    | 7651   |
|    total_timesteps | 466944 |
-------------------------------
Eval num_timesteps=470000, episode_reward=-290.96 +/- 152.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -291         |
| time/                   |              |
|    total_timesteps      | 470000       |
| train/                  |              |
|    approx_kl            | 0.0020592185 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0517       |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.000735    |
|    std                  | 1            |
|    value_loss           | 2.7          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 61     |
|    iterations      | 115    |
|    time_elapsed    | 7660   |
|    total_timesteps | 471040 |
-------------------------------
Eval num_timesteps=475000, episode_reward=-289.17 +/- 45.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -289        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.004396952 |
|    clip_fraction        | 0.0184      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0609      |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00084    |
|    std                  | 0.998       |
|    value_loss           | 0.679       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 61     |
|    iterations      | 116    |
|    time_elapsed    | 7670   |
|    total_timesteps | 475136 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 62           |
|    iterations           | 117          |
|    time_elapsed         | 7677         |
|    total_timesteps      | 479232       |
| train/                  |              |
|    approx_kl            | 0.0044044442 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.108        |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00222     |
|    std                  | 0.995        |
|    value_loss           | 1.21         |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-322.04 +/- 111.06
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -322         |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0045823143 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.981        |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00229     |
|    std                  | 0.994        |
|    value_loss           | 4.9          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 62     |
|    iterations      | 118    |
|    time_elapsed    | 7690   |
|    total_timesteps | 483328 |
-------------------------------
Eval num_timesteps=485000, episode_reward=-392.59 +/- 41.55
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -393        |
| time/                   |             |
|    total_timesteps      | 485000      |
| train/                  |             |
|    approx_kl            | 0.004138261 |
|    clip_fraction        | 0.0206      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.47        |
|    n_updates            | 1180        |
|    policy_gradient_loss | -0.000366   |
|    std                  | 0.995       |
|    value_loss           | 0.793       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 63     |
|    iterations      | 119    |
|    time_elapsed    | 7701   |
|    total_timesteps | 487424 |
-------------------------------
Eval num_timesteps=490000, episode_reward=-252.58 +/- 183.50
Episode length: 822.40 +/- 357.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 822        |
|    mean_reward          | -253       |
| time/                   |            |
|    total_timesteps      | 490000     |
| train/                  |            |
|    approx_kl            | 0.00423622 |
|    clip_fraction        | 0.0124     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.24      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0612     |
|    n_updates            | 1190       |
|    policy_gradient_loss | -0.00186   |
|    std                  | 0.997      |
|    value_loss           | 2.37       |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 63     |
|    iterations      | 120    |
|    time_elapsed    | 7713   |
|    total_timesteps | 491520 |
-------------------------------
Eval num_timesteps=495000, episode_reward=-268.71 +/- 39.54
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -269         |
| time/                   |              |
|    total_timesteps      | 495000       |
| train/                  |              |
|    approx_kl            | 0.0021620253 |
|    clip_fraction        | 0.0116       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0339       |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.000575    |
|    std                  | 0.999        |
|    value_loss           | 0.595        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 64     |
|    iterations      | 121    |
|    time_elapsed    | 7726   |
|    total_timesteps | 495616 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 64           |
|    iterations           | 122          |
|    time_elapsed         | 7734         |
|    total_timesteps      | 499712       |
| train/                  |              |
|    approx_kl            | 0.0046725897 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.406        |
|    n_updates            | 1210         |
|    policy_gradient_loss | -0.00133     |
|    std                  | 0.995        |
|    value_loss           | 11.4         |
------------------------------------------
Eval num_timesteps=500000, episode_reward=-379.49 +/- 150.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -379        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.005285005 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.034       |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00232    |
|    std                  | 1           |
|    value_loss           | 0.445       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 65     |
|    iterations      | 123    |
|    time_elapsed    | 7746   |
|    total_timesteps | 503808 |
-------------------------------
Eval num_timesteps=505000, episode_reward=-350.65 +/- 197.01
Episode length: 803.20 +/- 395.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 803         |
|    mean_reward          | -351        |
| time/                   |             |
|    total_timesteps      | 505000      |
| train/                  |             |
|    approx_kl            | 0.006249247 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.186       |
|    n_updates            | 1230        |
|    policy_gradient_loss | -0.00305    |
|    std                  | 0.998       |
|    value_loss           | 12.8        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 65     |
|    iterations      | 124    |
|    time_elapsed    | 7758   |
|    total_timesteps | 507904 |
-------------------------------
Eval num_timesteps=510000, episode_reward=-400.01 +/- 172.14
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -400         |
| time/                   |              |
|    total_timesteps      | 510000       |
| train/                  |              |
|    approx_kl            | 0.0029084678 |
|    clip_fraction        | 0.00676      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0139       |
|    n_updates            | 1240         |
|    policy_gradient_loss | -0.000232    |
|    std                  | 0.997        |
|    value_loss           | 0.744        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 65     |
|    iterations      | 125    |
|    time_elapsed    | 7770   |
|    total_timesteps | 512000 |
-------------------------------
Eval num_timesteps=515000, episode_reward=-261.52 +/- 89.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -262         |
| time/                   |              |
|    total_timesteps      | 515000       |
| train/                  |              |
|    approx_kl            | 0.0053221816 |
|    clip_fraction        | 0.0319       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0609       |
|    n_updates            | 1250         |
|    policy_gradient_loss | -0.00219     |
|    std                  | 0.997        |
|    value_loss           | 0.163        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 66     |
|    iterations      | 126    |
|    time_elapsed    | 7784   |
|    total_timesteps | 516096 |
-------------------------------
Eval num_timesteps=520000, episode_reward=-393.14 +/- 195.46
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -393       |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.00508947 |
|    clip_fraction        | 0.0232     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0001     |
|    loss                 | 4.38       |
|    n_updates            | 1260       |
|    policy_gradient_loss | -0.00227   |
|    std                  | 0.997      |
|    value_loss           | 10.1       |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 66     |
|    iterations      | 127    |
|    time_elapsed    | 7797   |
|    total_timesteps | 520192 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 67           |
|    iterations           | 128          |
|    time_elapsed         | 7803         |
|    total_timesteps      | 524288       |
| train/                  |              |
|    approx_kl            | 0.0041176346 |
|    clip_fraction        | 0.0123       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.734        |
|    n_updates            | 1270         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 0.996        |
|    value_loss           | 3.9          |
------------------------------------------
Eval num_timesteps=525000, episode_reward=-401.63 +/- 191.22
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -402         |
| time/                   |              |
|    total_timesteps      | 525000       |
| train/                  |              |
|    approx_kl            | 0.0051759696 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0344       |
|    n_updates            | 1280         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 0.993        |
|    value_loss           | 0.172        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 67     |
|    iterations      | 129    |
|    time_elapsed    | 7812   |
|    total_timesteps | 528384 |
-------------------------------
Eval num_timesteps=530000, episode_reward=-231.38 +/- 151.82
Episode length: 819.20 +/- 363.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 819         |
|    mean_reward          | -231        |
| time/                   |             |
|    total_timesteps      | 530000      |
| train/                  |             |
|    approx_kl            | 0.003821082 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0686      |
|    n_updates            | 1290        |
|    policy_gradient_loss | -0.00217    |
|    std                  | 0.993       |
|    value_loss           | 0.704       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 68     |
|    iterations      | 130    |
|    time_elapsed    | 7821   |
|    total_timesteps | 532480 |
-------------------------------
Eval num_timesteps=535000, episode_reward=-283.85 +/- 117.00
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -284         |
| time/                   |              |
|    total_timesteps      | 535000       |
| train/                  |              |
|    approx_kl            | 0.0051591834 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.904        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0819       |
|    n_updates            | 1300         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 0.995        |
|    value_loss           | 0.356        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 68     |
|    iterations      | 131    |
|    time_elapsed    | 7830   |
|    total_timesteps | 536576 |
-------------------------------
Eval num_timesteps=540000, episode_reward=-458.37 +/- 154.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -458         |
| time/                   |              |
|    total_timesteps      | 540000       |
| train/                  |              |
|    approx_kl            | 0.0046481113 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.139        |
|    n_updates            | 1310         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 0.993        |
|    value_loss           | 0.289        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 68     |
|    iterations      | 132    |
|    time_elapsed    | 7840   |
|    total_timesteps | 540672 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 69           |
|    iterations           | 133          |
|    time_elapsed         | 7848         |
|    total_timesteps      | 544768       |
| train/                  |              |
|    approx_kl            | 0.0039138226 |
|    clip_fraction        | 0.0147       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.139        |
|    n_updates            | 1320         |
|    policy_gradient_loss | -0.000362    |
|    std                  | 0.993        |
|    value_loss           | 14.1         |
------------------------------------------
Eval num_timesteps=545000, episode_reward=-258.23 +/- 121.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -258        |
| time/                   |             |
|    total_timesteps      | 545000      |
| train/                  |             |
|    approx_kl            | 0.006308261 |
|    clip_fraction        | 0.039       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00442     |
|    n_updates            | 1330        |
|    policy_gradient_loss | -0.00269    |
|    std                  | 0.998       |
|    value_loss           | 0.183       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 69     |
|    iterations      | 134    |
|    time_elapsed    | 7858   |
|    total_timesteps | 548864 |
-------------------------------
Eval num_timesteps=550000, episode_reward=-339.62 +/- 166.53
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -340         |
| time/                   |              |
|    total_timesteps      | 550000       |
| train/                  |              |
|    approx_kl            | 0.0031358155 |
|    clip_fraction        | 0.0115       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.399        |
|    n_updates            | 1340         |
|    policy_gradient_loss | -0.00122     |
|    std                  | 0.997        |
|    value_loss           | 7.78         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 70     |
|    iterations      | 135    |
|    time_elapsed    | 7868   |
|    total_timesteps | 552960 |
-------------------------------
Eval num_timesteps=555000, episode_reward=-385.01 +/- 70.12
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -385         |
| time/                   |              |
|    total_timesteps      | 555000       |
| train/                  |              |
|    approx_kl            | 0.0041278386 |
|    clip_fraction        | 0.017        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0753       |
|    n_updates            | 1350         |
|    policy_gradient_loss | -0.00101     |
|    std                  | 0.989        |
|    value_loss           | 1.56         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 70     |
|    iterations      | 136    |
|    time_elapsed    | 7878   |
|    total_timesteps | 557056 |
-------------------------------
Eval num_timesteps=560000, episode_reward=-246.58 +/- 119.44
Episode length: 815.80 +/- 370.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 816         |
|    mean_reward          | -247        |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.002209616 |
|    clip_fraction        | 0.00344     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.14        |
|    n_updates            | 1360        |
|    policy_gradient_loss | -0.000772   |
|    std                  | 0.988       |
|    value_loss           | 4.21        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 71     |
|    iterations      | 137    |
|    time_elapsed    | 7889   |
|    total_timesteps | 561152 |
-------------------------------
Eval num_timesteps=565000, episode_reward=-215.85 +/- 132.56
Episode length: 814.60 +/- 372.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -216         |
| time/                   |              |
|    total_timesteps      | 565000       |
| train/                  |              |
|    approx_kl            | 0.0034222887 |
|    clip_fraction        | 0.0092       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.218        |
|    n_updates            | 1370         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 0.987        |
|    value_loss           | 10.8         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 71     |
|    iterations      | 138    |
|    time_elapsed    | 7899   |
|    total_timesteps | 565248 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 71          |
|    iterations           | 139         |
|    time_elapsed         | 7908        |
|    total_timesteps      | 569344      |
| train/                  |             |
|    approx_kl            | 0.005331959 |
|    clip_fraction        | 0.0462      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0464      |
|    n_updates            | 1380        |
|    policy_gradient_loss | -0.00422    |
|    std                  | 0.985       |
|    value_loss           | 0.492       |
-----------------------------------------
Eval num_timesteps=570000, episode_reward=-279.31 +/- 33.40
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -279         |
| time/                   |              |
|    total_timesteps      | 570000       |
| train/                  |              |
|    approx_kl            | 0.0044544693 |
|    clip_fraction        | 0.0323       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.04         |
|    n_updates            | 1390         |
|    policy_gradient_loss | -0.00225     |
|    std                  | 0.987        |
|    value_loss           | 3.54         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 72     |
|    iterations      | 140    |
|    time_elapsed    | 7919   |
|    total_timesteps | 573440 |
-------------------------------
Eval num_timesteps=575000, episode_reward=-418.71 +/- 109.14
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -419        |
| time/                   |             |
|    total_timesteps      | 575000      |
| train/                  |             |
|    approx_kl            | 0.005287698 |
|    clip_fraction        | 0.0265      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 35.5        |
|    n_updates            | 1400        |
|    policy_gradient_loss | -0.00329    |
|    std                  | 0.984       |
|    value_loss           | 10.2        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 72     |
|    iterations      | 141    |
|    time_elapsed    | 7929   |
|    total_timesteps | 577536 |
-------------------------------
Eval num_timesteps=580000, episode_reward=-451.66 +/- 149.87
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -452         |
| time/                   |              |
|    total_timesteps      | 580000       |
| train/                  |              |
|    approx_kl            | 0.0056223357 |
|    clip_fraction        | 0.0203       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.261        |
|    n_updates            | 1410         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 0.984        |
|    value_loss           | 4.16         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 73     |
|    iterations      | 142    |
|    time_elapsed    | 7938   |
|    total_timesteps | 581632 |
-------------------------------
Eval num_timesteps=585000, episode_reward=-382.23 +/- 68.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -382        |
| time/                   |             |
|    total_timesteps      | 585000      |
| train/                  |             |
|    approx_kl            | 0.004636243 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.25        |
|    n_updates            | 1420        |
|    policy_gradient_loss | -0.00226    |
|    std                  | 0.986       |
|    value_loss           | 10.7        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 73     |
|    iterations      | 143    |
|    time_elapsed    | 7948   |
|    total_timesteps | 585728 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 74           |
|    iterations           | 144          |
|    time_elapsed         | 7956         |
|    total_timesteps      | 589824       |
| train/                  |              |
|    approx_kl            | 0.0047967914 |
|    clip_fraction        | 0.0431       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 12           |
|    n_updates            | 1430         |
|    policy_gradient_loss | -0.00366     |
|    std                  | 0.988        |
|    value_loss           | 9.58         |
------------------------------------------
Eval num_timesteps=590000, episode_reward=-404.05 +/- 136.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -404         |
| time/                   |              |
|    total_timesteps      | 590000       |
| train/                  |              |
|    approx_kl            | 0.0047936435 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0473       |
|    n_updates            | 1440         |
|    policy_gradient_loss | -0.0016      |
|    std                  | 0.985        |
|    value_loss           | 0.326        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 74     |
|    iterations      | 145    |
|    time_elapsed    | 7966   |
|    total_timesteps | 593920 |
-------------------------------
Eval num_timesteps=595000, episode_reward=-433.58 +/- 208.17
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -434        |
| time/                   |             |
|    total_timesteps      | 595000      |
| train/                  |             |
|    approx_kl            | 0.004973823 |
|    clip_fraction        | 0.0309      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0457      |
|    n_updates            | 1450        |
|    policy_gradient_loss | -0.00138    |
|    std                  | 0.989       |
|    value_loss           | 0.272       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 74     |
|    iterations      | 146    |
|    time_elapsed    | 7976   |
|    total_timesteps | 598016 |
-------------------------------
Eval num_timesteps=600000, episode_reward=-353.70 +/- 176.99
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -354        |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.004527546 |
|    clip_fraction        | 0.0273      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.168       |
|    n_updates            | 1460        |
|    policy_gradient_loss | -0.00184    |
|    std                  | 0.983       |
|    value_loss           | 9.44        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 75     |
|    iterations      | 147    |
|    time_elapsed    | 7986   |
|    total_timesteps | 602112 |
-------------------------------
Eval num_timesteps=605000, episode_reward=-433.06 +/- 104.63
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -433        |
| time/                   |             |
|    total_timesteps      | 605000      |
| train/                  |             |
|    approx_kl            | 0.006523337 |
|    clip_fraction        | 0.0562      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.056       |
|    n_updates            | 1470        |
|    policy_gradient_loss | -0.00303    |
|    std                  | 0.979       |
|    value_loss           | 0.229       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 75     |
|    iterations      | 148    |
|    time_elapsed    | 7995   |
|    total_timesteps | 606208 |
-------------------------------
Eval num_timesteps=610000, episode_reward=-268.37 +/- 133.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -268         |
| time/                   |              |
|    total_timesteps      | 610000       |
| train/                  |              |
|    approx_kl            | 0.0028810957 |
|    clip_fraction        | 0.00457      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 12.7         |
|    n_updates            | 1480         |
|    policy_gradient_loss | -0.000274    |
|    std                  | 0.979        |
|    value_loss           | 3.81         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 76     |
|    iterations      | 149    |
|    time_elapsed    | 8004   |
|    total_timesteps | 610304 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 76           |
|    iterations           | 150          |
|    time_elapsed         | 8011         |
|    total_timesteps      | 614400       |
| train/                  |              |
|    approx_kl            | 0.0036442908 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.015        |
|    n_updates            | 1490         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 0.98         |
|    value_loss           | 0.184        |
------------------------------------------
Eval num_timesteps=615000, episode_reward=-351.22 +/- 147.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -351         |
| time/                   |              |
|    total_timesteps      | 615000       |
| train/                  |              |
|    approx_kl            | 0.0047344966 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.024        |
|    n_updates            | 1500         |
|    policy_gradient_loss | -0.00264     |
|    std                  | 0.979        |
|    value_loss           | 0.154        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 77     |
|    iterations      | 151    |
|    time_elapsed    | 8021   |
|    total_timesteps | 618496 |
-------------------------------
Eval num_timesteps=620000, episode_reward=-293.81 +/- 169.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -294        |
| time/                   |             |
|    total_timesteps      | 620000      |
| train/                  |             |
|    approx_kl            | 0.006862536 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0209      |
|    n_updates            | 1510        |
|    policy_gradient_loss | -0.00432    |
|    std                  | 0.989       |
|    value_loss           | 0.175       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 77     |
|    iterations      | 152    |
|    time_elapsed    | 8030   |
|    total_timesteps | 622592 |
-------------------------------
Eval num_timesteps=625000, episode_reward=-444.18 +/- 88.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -444         |
| time/                   |              |
|    total_timesteps      | 625000       |
| train/                  |              |
|    approx_kl            | 0.0042325966 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0243       |
|    n_updates            | 1520         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 0.988        |
|    value_loss           | 3.31         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 77     |
|    iterations      | 153    |
|    time_elapsed    | 8041   |
|    total_timesteps | 626688 |
-------------------------------
Eval num_timesteps=630000, episode_reward=-250.10 +/- 128.57
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -250       |
| time/                   |            |
|    total_timesteps      | 630000     |
| train/                  |            |
|    approx_kl            | 0.00598039 |
|    clip_fraction        | 0.0361     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.23      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.157      |
|    n_updates            | 1530       |
|    policy_gradient_loss | -0.00226   |
|    std                  | 0.993      |
|    value_loss           | 0.445      |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 78     |
|    iterations      | 154    |
|    time_elapsed    | 8049   |
|    total_timesteps | 630784 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 78           |
|    iterations           | 155          |
|    time_elapsed         | 8056         |
|    total_timesteps      | 634880       |
| train/                  |              |
|    approx_kl            | 0.0043780827 |
|    clip_fraction        | 0.00823      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 15.1         |
|    n_updates            | 1540         |
|    policy_gradient_loss | -0.00202     |
|    std                  | 0.994        |
|    value_loss           | 12.1         |
------------------------------------------
Eval num_timesteps=635000, episode_reward=-288.52 +/- 82.32
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -289        |
| time/                   |             |
|    total_timesteps      | 635000      |
| train/                  |             |
|    approx_kl            | 0.003625333 |
|    clip_fraction        | 0.0236      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0128      |
|    n_updates            | 1550        |
|    policy_gradient_loss | -0.00196    |
|    std                  | 0.999       |
|    value_loss           | 0.0735      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 79     |
|    iterations      | 156    |
|    time_elapsed    | 8064   |
|    total_timesteps | 638976 |
-------------------------------
Eval num_timesteps=640000, episode_reward=-442.36 +/- 219.97
Episode length: 813.60 +/- 374.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -442         |
| time/                   |              |
|    total_timesteps      | 640000       |
| train/                  |              |
|    approx_kl            | 0.0064303167 |
|    clip_fraction        | 0.0409       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.154        |
|    n_updates            | 1560         |
|    policy_gradient_loss | -0.00349     |
|    std                  | 0.994        |
|    value_loss           | 2.83         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 79     |
|    iterations      | 157    |
|    time_elapsed    | 8073   |
|    total_timesteps | 643072 |
-------------------------------
Eval num_timesteps=645000, episode_reward=-268.79 +/- 71.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -269         |
| time/                   |              |
|    total_timesteps      | 645000       |
| train/                  |              |
|    approx_kl            | 0.0055808895 |
|    clip_fraction        | 0.0471       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.476        |
|    n_updates            | 1570         |
|    policy_gradient_loss | -0.00396     |
|    std                  | 0.999        |
|    value_loss           | 3.01         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 80     |
|    iterations      | 158    |
|    time_elapsed    | 8082   |
|    total_timesteps | 647168 |
-------------------------------
Eval num_timesteps=650000, episode_reward=-426.90 +/- 128.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -427         |
| time/                   |              |
|    total_timesteps      | 650000       |
| train/                  |              |
|    approx_kl            | 0.0031277244 |
|    clip_fraction        | 0.00977      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0285       |
|    n_updates            | 1580         |
|    policy_gradient_loss | -0.0016      |
|    std                  | 0.995        |
|    value_loss           | 2.84         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 80     |
|    iterations      | 159    |
|    time_elapsed    | 8091   |
|    total_timesteps | 651264 |
-------------------------------
Eval num_timesteps=655000, episode_reward=-312.70 +/- 152.92
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -313        |
| time/                   |             |
|    total_timesteps      | 655000      |
| train/                  |             |
|    approx_kl            | 0.005426759 |
|    clip_fraction        | 0.0452      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.429       |
|    n_updates            | 1590        |
|    policy_gradient_loss | -0.00547    |
|    std                  | 1           |
|    value_loss           | 0.599       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 80     |
|    iterations      | 160    |
|    time_elapsed    | 8100   |
|    total_timesteps | 655360 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 81           |
|    iterations           | 161          |
|    time_elapsed         | 8107         |
|    total_timesteps      | 659456       |
| train/                  |              |
|    approx_kl            | 0.0033969088 |
|    clip_fraction        | 0.00476      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.17         |
|    n_updates            | 1600         |
|    policy_gradient_loss | -0.00127     |
|    std                  | 1            |
|    value_loss           | 7.12         |
------------------------------------------
Eval num_timesteps=660000, episode_reward=-230.45 +/- 93.49
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -230         |
| time/                   |              |
|    total_timesteps      | 660000       |
| train/                  |              |
|    approx_kl            | 0.0047330703 |
|    clip_fraction        | 0.0379       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0127       |
|    n_updates            | 1610         |
|    policy_gradient_loss | -0.00328     |
|    std                  | 0.996        |
|    value_loss           | 0.11         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 81     |
|    iterations      | 162    |
|    time_elapsed    | 8117   |
|    total_timesteps | 663552 |
-------------------------------
Eval num_timesteps=665000, episode_reward=-416.16 +/- 192.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -416         |
| time/                   |              |
|    total_timesteps      | 665000       |
| train/                  |              |
|    approx_kl            | 0.0049486645 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0225       |
|    n_updates            | 1620         |
|    policy_gradient_loss | -0.00294     |
|    std                  | 1            |
|    value_loss           | 0.19         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 163    |
|    time_elapsed    | 8127   |
|    total_timesteps | 667648 |
-------------------------------
Eval num_timesteps=670000, episode_reward=-306.92 +/- 132.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -307         |
| time/                   |              |
|    total_timesteps      | 670000       |
| train/                  |              |
|    approx_kl            | 0.0049307803 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0572       |
|    n_updates            | 1630         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 0.996        |
|    value_loss           | 0.207        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 164    |
|    time_elapsed    | 8139   |
|    total_timesteps | 671744 |
-------------------------------
Eval num_timesteps=675000, episode_reward=-288.17 +/- 42.02
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -288        |
| time/                   |             |
|    total_timesteps      | 675000      |
| train/                  |             |
|    approx_kl            | 0.002723835 |
|    clip_fraction        | 0.00649     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 46.8        |
|    n_updates            | 1640        |
|    policy_gradient_loss | -0.00285    |
|    std                  | 0.996       |
|    value_loss           | 14.1        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 82     |
|    iterations      | 165    |
|    time_elapsed    | 8151   |
|    total_timesteps | 675840 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 166         |
|    time_elapsed         | 8159        |
|    total_timesteps      | 679936      |
| train/                  |             |
|    approx_kl            | 0.004569978 |
|    clip_fraction        | 0.0185      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.153       |
|    n_updates            | 1650        |
|    policy_gradient_loss | -0.000602   |
|    std                  | 0.998       |
|    value_loss           | 1.66        |
-----------------------------------------
Eval num_timesteps=680000, episode_reward=-372.98 +/- 139.77
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -373        |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.004378765 |
|    clip_fraction        | 0.0208      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0223      |
|    n_updates            | 1660        |
|    policy_gradient_loss | -0.00257    |
|    std                  | 0.992       |
|    value_loss           | 0.136       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 83     |
|    iterations      | 167    |
|    time_elapsed    | 8171   |
|    total_timesteps | 684032 |
-------------------------------
Eval num_timesteps=685000, episode_reward=-358.31 +/- 116.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -358         |
| time/                   |              |
|    total_timesteps      | 685000       |
| train/                  |              |
|    approx_kl            | 0.0048942193 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | -8.11e-05    |
|    n_updates            | 1670         |
|    policy_gradient_loss | -0.00215     |
|    std                  | 0.991        |
|    value_loss           | 0.639        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 84     |
|    iterations      | 168    |
|    time_elapsed    | 8184   |
|    total_timesteps | 688128 |
-------------------------------
Eval num_timesteps=690000, episode_reward=-328.44 +/- 214.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -328         |
| time/                   |              |
|    total_timesteps      | 690000       |
| train/                  |              |
|    approx_kl            | 0.0050617885 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00635     |
|    n_updates            | 1680         |
|    policy_gradient_loss | -0.00322     |
|    std                  | 0.984        |
|    value_loss           | 0.173        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 84     |
|    iterations      | 169    |
|    time_elapsed    | 8196   |
|    total_timesteps | 692224 |
-------------------------------
Eval num_timesteps=695000, episode_reward=-394.43 +/- 120.35
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -394         |
| time/                   |              |
|    total_timesteps      | 695000       |
| train/                  |              |
|    approx_kl            | 0.0041028718 |
|    clip_fraction        | 0.0156       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.95         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.506        |
|    n_updates            | 1690         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 0.987        |
|    value_loss           | 0.335        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 84     |
|    iterations      | 170    |
|    time_elapsed    | 8207   |
|    total_timesteps | 696320 |
-------------------------------
Eval num_timesteps=700000, episode_reward=-366.04 +/- 105.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -366         |
| time/                   |              |
|    total_timesteps      | 700000       |
| train/                  |              |
|    approx_kl            | 0.0047270367 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0423       |
|    n_updates            | 1700         |
|    policy_gradient_loss | -0.00352     |
|    std                  | 0.983        |
|    value_loss           | 0.968        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 85     |
|    iterations      | 171    |
|    time_elapsed    | 8216   |
|    total_timesteps | 700416 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 172         |
|    time_elapsed         | 8223        |
|    total_timesteps      | 704512      |
| train/                  |             |
|    approx_kl            | 0.004311016 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0893      |
|    n_updates            | 1710        |
|    policy_gradient_loss | -0.00119    |
|    std                  | 0.984       |
|    value_loss           | 0.723       |
-----------------------------------------
Eval num_timesteps=705000, episode_reward=-332.44 +/- 204.46
Episode length: 818.00 +/- 366.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 818          |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 705000       |
| train/                  |              |
|    approx_kl            | 0.0054839994 |
|    clip_fraction        | 0.0398       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0227       |
|    n_updates            | 1720         |
|    policy_gradient_loss | -0.00406     |
|    std                  | 0.982        |
|    value_loss           | 0.0689       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 86     |
|    iterations      | 173    |
|    time_elapsed    | 8231   |
|    total_timesteps | 708608 |
-------------------------------
Eval num_timesteps=710000, episode_reward=-299.97 +/- 74.25
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -300      |
| time/                   |           |
|    total_timesteps      | 710000    |
| train/                  |           |
|    approx_kl            | 0.0052218 |
|    clip_fraction        | 0.0331    |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.2      |
|    explained_variance   | 0.996     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.02      |
|    n_updates            | 1730      |
|    policy_gradient_loss | -0.00263  |
|    std                  | 0.983     |
|    value_loss           | 0.775     |
---------------------------------------
-------------------------------
| time/              |        |
|    fps             | 86     |
|    iterations      | 174    |
|    time_elapsed    | 8240   |
|    total_timesteps | 712704 |
-------------------------------
Eval num_timesteps=715000, episode_reward=-365.49 +/- 98.90
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 715000       |
| train/                  |              |
|    approx_kl            | 0.0042800717 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.552        |
|    n_updates            | 1740         |
|    policy_gradient_loss | -0.0022      |
|    std                  | 0.982        |
|    value_loss           | 7.25         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 86     |
|    iterations      | 175    |
|    time_elapsed    | 8249   |
|    total_timesteps | 716800 |
-------------------------------
Eval num_timesteps=720000, episode_reward=-382.71 +/- 114.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -383         |
| time/                   |              |
|    total_timesteps      | 720000       |
| train/                  |              |
|    approx_kl            | 0.0041504093 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.318        |
|    n_updates            | 1750         |
|    policy_gradient_loss | -0.00224     |
|    std                  | 0.979        |
|    value_loss           | 7.08         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 87     |
|    iterations      | 176    |
|    time_elapsed    | 8258   |
|    total_timesteps | 720896 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 177         |
|    time_elapsed         | 8265        |
|    total_timesteps      | 724992      |
| train/                  |             |
|    approx_kl            | 0.005067288 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.49        |
|    n_updates            | 1760        |
|    policy_gradient_loss | -0.00203    |
|    std                  | 0.981       |
|    value_loss           | 4.5         |
-----------------------------------------
Eval num_timesteps=725000, episode_reward=-337.34 +/- 78.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -337         |
| time/                   |              |
|    total_timesteps      | 725000       |
| train/                  |              |
|    approx_kl            | 0.0047004167 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0132       |
|    n_updates            | 1770         |
|    policy_gradient_loss | -0.00245     |
|    std                  | 0.99         |
|    value_loss           | 0.0551       |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 88     |
|    iterations      | 178    |
|    time_elapsed    | 8274   |
|    total_timesteps | 729088 |
-------------------------------
Eval num_timesteps=730000, episode_reward=-289.98 +/- 177.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -290        |
| time/                   |             |
|    total_timesteps      | 730000      |
| train/                  |             |
|    approx_kl            | 0.007882222 |
|    clip_fraction        | 0.0669      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 11.2        |
|    n_updates            | 1780        |
|    policy_gradient_loss | -0.00438    |
|    std                  | 0.987       |
|    value_loss           | 5.32        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 88     |
|    iterations      | 179    |
|    time_elapsed    | 8284   |
|    total_timesteps | 733184 |
-------------------------------
Eval num_timesteps=735000, episode_reward=-323.34 +/- 221.65
Episode length: 814.00 +/- 374.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | -323        |
| time/                   |             |
|    total_timesteps      | 735000      |
| train/                  |             |
|    approx_kl            | 0.003509187 |
|    clip_fraction        | 0.00864     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.963       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0384      |
|    n_updates            | 1790        |
|    policy_gradient_loss | -0.000965   |
|    std                  | 0.985       |
|    value_loss           | 0.867       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 88     |
|    iterations      | 180    |
|    time_elapsed    | 8294   |
|    total_timesteps | 737280 |
-------------------------------
Eval num_timesteps=740000, episode_reward=-307.66 +/- 70.55
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -308         |
| time/                   |              |
|    total_timesteps      | 740000       |
| train/                  |              |
|    approx_kl            | 0.0033653178 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0841       |
|    n_updates            | 1800         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 0.988        |
|    value_loss           | 5.59         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 89     |
|    iterations      | 181    |
|    time_elapsed    | 8304   |
|    total_timesteps | 741376 |
-------------------------------
Eval num_timesteps=745000, episode_reward=-309.94 +/- 112.75
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -310        |
| time/                   |             |
|    total_timesteps      | 745000      |
| train/                  |             |
|    approx_kl            | 0.006513579 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.136       |
|    n_updates            | 1810        |
|    policy_gradient_loss | -0.00253    |
|    std                  | 0.985       |
|    value_loss           | 7.39        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 89     |
|    iterations      | 182    |
|    time_elapsed    | 8314   |
|    total_timesteps | 745472 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 90           |
|    iterations           | 183          |
|    time_elapsed         | 8321         |
|    total_timesteps      | 749568       |
| train/                  |              |
|    approx_kl            | 0.0039734594 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.837        |
|    n_updates            | 1820         |
|    policy_gradient_loss | -0.00211     |
|    std                  | 0.984        |
|    value_loss           | 5.52         |
------------------------------------------
Eval num_timesteps=750000, episode_reward=-380.69 +/- 93.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -381         |
| time/                   |              |
|    total_timesteps      | 750000       |
| train/                  |              |
|    approx_kl            | 0.0066697136 |
|    clip_fraction        | 0.0621       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0184       |
|    n_updates            | 1830         |
|    policy_gradient_loss | -0.005       |
|    std                  | 0.99         |
|    value_loss           | 0.24         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 90     |
|    iterations      | 184    |
|    time_elapsed    | 8331   |
|    total_timesteps | 753664 |
-------------------------------
Eval num_timesteps=755000, episode_reward=-231.25 +/- 169.86
Episode length: 805.80 +/- 390.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 806          |
|    mean_reward          | -231         |
| time/                   |              |
|    total_timesteps      | 755000       |
| train/                  |              |
|    approx_kl            | 0.0050263535 |
|    clip_fraction        | 0.0246       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.06         |
|    n_updates            | 1840         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 0.986        |
|    value_loss           | 1.07         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 90     |
|    iterations      | 185    |
|    time_elapsed    | 8341   |
|    total_timesteps | 757760 |
-------------------------------
Eval num_timesteps=760000, episode_reward=-228.20 +/- 112.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -228         |
| time/                   |              |
|    total_timesteps      | 760000       |
| train/                  |              |
|    approx_kl            | 0.0046029715 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0191       |
|    n_updates            | 1850         |
|    policy_gradient_loss | 0.000274     |
|    std                  | 0.989        |
|    value_loss           | 0.166        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 91     |
|    iterations      | 186    |
|    time_elapsed    | 8351   |
|    total_timesteps | 761856 |
-------------------------------
Eval num_timesteps=765000, episode_reward=-507.33 +/- 141.80
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -507        |
| time/                   |             |
|    total_timesteps      | 765000      |
| train/                  |             |
|    approx_kl            | 0.004357405 |
|    clip_fraction        | 0.02        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.543       |
|    n_updates            | 1860        |
|    policy_gradient_loss | -0.00146    |
|    std                  | 0.985       |
|    value_loss           | 1.64        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 91     |
|    iterations      | 187    |
|    time_elapsed    | 8360   |
|    total_timesteps | 765952 |
-------------------------------
Eval num_timesteps=770000, episode_reward=-179.43 +/- 129.38
Episode length: 812.20 +/- 377.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | -179         |
| time/                   |              |
|    total_timesteps      | 770000       |
| train/                  |              |
|    approx_kl            | 0.0036598567 |
|    clip_fraction        | 0.00813      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.61         |
|    n_updates            | 1870         |
|    policy_gradient_loss | -0.000582    |
|    std                  | 0.985        |
|    value_loss           | 16.4         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 92     |
|    iterations      | 188    |
|    time_elapsed    | 8370   |
|    total_timesteps | 770048 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 92           |
|    iterations           | 189          |
|    time_elapsed         | 8377         |
|    total_timesteps      | 774144       |
| train/                  |              |
|    approx_kl            | 0.0027852969 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0274       |
|    n_updates            | 1880         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 0.983        |
|    value_loss           | 0.46         |
------------------------------------------
Eval num_timesteps=775000, episode_reward=-350.37 +/- 60.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -350         |
| time/                   |              |
|    total_timesteps      | 775000       |
| train/                  |              |
|    approx_kl            | 0.0051796366 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0914       |
|    n_updates            | 1890         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 0.982        |
|    value_loss           | 1.68         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 92     |
|    iterations      | 190    |
|    time_elapsed    | 8387   |
|    total_timesteps | 778240 |
-------------------------------
Eval num_timesteps=780000, episode_reward=-302.52 +/- 231.87
Episode length: 810.80 +/- 380.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -303         |
| time/                   |              |
|    total_timesteps      | 780000       |
| train/                  |              |
|    approx_kl            | 0.0063624806 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 4            |
|    n_updates            | 1900         |
|    policy_gradient_loss | -0.00371     |
|    std                  | 0.982        |
|    value_loss           | 6.04         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 93     |
|    iterations      | 191    |
|    time_elapsed    | 8396   |
|    total_timesteps | 782336 |
-------------------------------
Eval num_timesteps=785000, episode_reward=-356.15 +/- 119.50
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -356        |
| time/                   |             |
|    total_timesteps      | 785000      |
| train/                  |             |
|    approx_kl            | 0.006688644 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.236       |
|    n_updates            | 1910        |
|    policy_gradient_loss | -0.00376    |
|    std                  | 0.978       |
|    value_loss           | 4.05        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 93     |
|    iterations      | 192    |
|    time_elapsed    | 8406   |
|    total_timesteps | 786432 |
-------------------------------
Eval num_timesteps=790000, episode_reward=-355.38 +/- 120.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -355         |
| time/                   |              |
|    total_timesteps      | 790000       |
| train/                  |              |
|    approx_kl            | 0.0058726915 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0371       |
|    n_updates            | 1920         |
|    policy_gradient_loss | -0.00163     |
|    std                  | 0.984        |
|    value_loss           | 0.404        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 93     |
|    iterations      | 193    |
|    time_elapsed    | 8417   |
|    total_timesteps | 790528 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 94           |
|    iterations           | 194          |
|    time_elapsed         | 8424         |
|    total_timesteps      | 794624       |
| train/                  |              |
|    approx_kl            | 0.0037044897 |
|    clip_fraction        | 0.0238       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0357       |
|    n_updates            | 1930         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 0.986        |
|    value_loss           | 0.349        |
------------------------------------------
Eval num_timesteps=795000, episode_reward=-249.34 +/- 75.48
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -249        |
| time/                   |             |
|    total_timesteps      | 795000      |
| train/                  |             |
|    approx_kl            | 0.004502743 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.467       |
|    n_updates            | 1940        |
|    policy_gradient_loss | -0.00111    |
|    std                  | 0.986       |
|    value_loss           | 3.21        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 94     |
|    iterations      | 195    |
|    time_elapsed    | 8434   |
|    total_timesteps | 798720 |
-------------------------------
Eval num_timesteps=800000, episode_reward=-324.32 +/- 232.34
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 800000       |
| train/                  |              |
|    approx_kl            | 0.0049241967 |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.128        |
|    n_updates            | 1950         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 0.982        |
|    value_loss           | 0.309        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 95     |
|    iterations      | 196    |
|    time_elapsed    | 8444   |
|    total_timesteps | 802816 |
-------------------------------
Eval num_timesteps=805000, episode_reward=-186.83 +/- 171.21
Episode length: 625.80 +/- 459.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 626         |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 805000      |
| train/                  |             |
|    approx_kl            | 0.002722318 |
|    clip_fraction        | 0.00649     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0734      |
|    n_updates            | 1960        |
|    policy_gradient_loss | -9.82e-05   |
|    std                  | 0.982       |
|    value_loss           | 2.06        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 95     |
|    iterations      | 197    |
|    time_elapsed    | 8454   |
|    total_timesteps | 806912 |
-------------------------------
Eval num_timesteps=810000, episode_reward=-449.04 +/- 141.24
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -449         |
| time/                   |              |
|    total_timesteps      | 810000       |
| train/                  |              |
|    approx_kl            | 0.0068591177 |
|    clip_fraction        | 0.04         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.289        |
|    n_updates            | 1970         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.981        |
|    value_loss           | 3.05         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 95     |
|    iterations      | 198    |
|    time_elapsed    | 8464   |
|    total_timesteps | 811008 |
-------------------------------
Eval num_timesteps=815000, episode_reward=-416.34 +/- 184.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -416         |
| time/                   |              |
|    total_timesteps      | 815000       |
| train/                  |              |
|    approx_kl            | 0.0052077346 |
|    clip_fraction        | 0.0472       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.194        |
|    n_updates            | 1980         |
|    policy_gradient_loss | -0.00325     |
|    std                  | 0.982        |
|    value_loss           | 0.6          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 96     |
|    iterations      | 199    |
|    time_elapsed    | 8476   |
|    total_timesteps | 815104 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 96           |
|    iterations           | 200          |
|    time_elapsed         | 8484         |
|    total_timesteps      | 819200       |
| train/                  |              |
|    approx_kl            | 0.0055984445 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0423       |
|    n_updates            | 1990         |
|    policy_gradient_loss | -0.00327     |
|    std                  | 0.987        |
|    value_loss           | 0.171        |
------------------------------------------
Eval num_timesteps=820000, episode_reward=-223.76 +/- 203.32
Episode length: 624.00 +/- 461.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 624         |
|    mean_reward          | -224        |
| time/                   |             |
|    total_timesteps      | 820000      |
| train/                  |             |
|    approx_kl            | 0.004911195 |
|    clip_fraction        | 0.0455      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00965    |
|    n_updates            | 2000        |
|    policy_gradient_loss | -0.00457    |
|    std                  | 0.99        |
|    value_loss           | 0.0669      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 96     |
|    iterations      | 201    |
|    time_elapsed    | 8493   |
|    total_timesteps | 823296 |
-------------------------------
Eval num_timesteps=825000, episode_reward=-218.68 +/- 63.80
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -219        |
| time/                   |             |
|    total_timesteps      | 825000      |
| train/                  |             |
|    approx_kl            | 0.004768094 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.686       |
|    n_updates            | 2010        |
|    policy_gradient_loss | -0.00209    |
|    std                  | 0.992       |
|    value_loss           | 11.9        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 97     |
|    iterations      | 202    |
|    time_elapsed    | 8503   |
|    total_timesteps | 827392 |
-------------------------------
Eval num_timesteps=830000, episode_reward=-335.82 +/- 60.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -336         |
| time/                   |              |
|    total_timesteps      | 830000       |
| train/                  |              |
|    approx_kl            | 0.0034745818 |
|    clip_fraction        | 0.0146       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0632       |
|    n_updates            | 2020         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 0.993        |
|    value_loss           | 7.68         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 97     |
|    iterations      | 203    |
|    time_elapsed    | 8513   |
|    total_timesteps | 831488 |
-------------------------------
Eval num_timesteps=835000, episode_reward=-332.00 +/- 102.34
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 835000       |
| train/                  |              |
|    approx_kl            | 0.0049441224 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0345       |
|    n_updates            | 2030         |
|    policy_gradient_loss | -0.00317     |
|    std                  | 0.994        |
|    value_loss           | 0.264        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 98     |
|    iterations      | 204    |
|    time_elapsed    | 8523   |
|    total_timesteps | 835584 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 98           |
|    iterations           | 205          |
|    time_elapsed         | 8530         |
|    total_timesteps      | 839680       |
| train/                  |              |
|    approx_kl            | 0.0051786806 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00908      |
|    n_updates            | 2040         |
|    policy_gradient_loss | -0.0033      |
|    std                  | 0.994        |
|    value_loss           | 1.73         |
------------------------------------------
Eval num_timesteps=840000, episode_reward=-344.25 +/- 87.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -344        |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.006306197 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00874     |
|    n_updates            | 2050        |
|    policy_gradient_loss | -0.00342    |
|    std                  | 0.991       |
|    value_loss           | 0.0901      |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 98     |
|    iterations      | 206    |
|    time_elapsed    | 8539   |
|    total_timesteps | 843776 |
-------------------------------
Eval num_timesteps=845000, episode_reward=-312.19 +/- 105.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -312         |
| time/                   |              |
|    total_timesteps      | 845000       |
| train/                  |              |
|    approx_kl            | 0.0048261825 |
|    clip_fraction        | 0.0211       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00346      |
|    n_updates            | 2060         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 0.994        |
|    value_loss           | 0.186        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 99     |
|    iterations      | 207    |
|    time_elapsed    | 8548   |
|    total_timesteps | 847872 |
-------------------------------
Eval num_timesteps=850000, episode_reward=-259.48 +/- 169.39
Episode length: 809.20 +/- 383.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 809          |
|    mean_reward          | -259         |
| time/                   |              |
|    total_timesteps      | 850000       |
| train/                  |              |
|    approx_kl            | 0.0038165392 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.366        |
|    n_updates            | 2070         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 0.996        |
|    value_loss           | 14.6         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 99     |
|    iterations      | 208    |
|    time_elapsed    | 8556   |
|    total_timesteps | 851968 |
-------------------------------
Eval num_timesteps=855000, episode_reward=-373.78 +/- 151.79
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -374        |
| time/                   |             |
|    total_timesteps      | 855000      |
| train/                  |             |
|    approx_kl            | 0.004831137 |
|    clip_fraction        | 0.0439      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0537      |
|    n_updates            | 2080        |
|    policy_gradient_loss | -0.00466    |
|    std                  | 0.998       |
|    value_loss           | 0.466       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 99     |
|    iterations      | 209    |
|    time_elapsed    | 8564   |
|    total_timesteps | 856064 |
-------------------------------
Eval num_timesteps=860000, episode_reward=-370.60 +/- 172.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -371         |
| time/                   |              |
|    total_timesteps      | 860000       |
| train/                  |              |
|    approx_kl            | 0.0068035033 |
|    clip_fraction        | 0.0551       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.2          |
|    n_updates            | 2090         |
|    policy_gradient_loss | -0.00409     |
|    std                  | 0.996        |
|    value_loss           | 3.07         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 100    |
|    iterations      | 210    |
|    time_elapsed    | 8573   |
|    total_timesteps | 860160 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 100          |
|    iterations           | 211          |
|    time_elapsed         | 8580         |
|    total_timesteps      | 864256       |
| train/                  |              |
|    approx_kl            | 0.0048364215 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.928        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0752       |
|    n_updates            | 2100         |
|    policy_gradient_loss | -0.00225     |
|    std                  | 0.997        |
|    value_loss           | 10           |
------------------------------------------
Eval num_timesteps=865000, episode_reward=-187.26 +/- 111.48
Episode length: 812.20 +/- 377.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 812         |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 865000      |
| train/                  |             |
|    approx_kl            | 0.004942173 |
|    clip_fraction        | 0.0347      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0103      |
|    n_updates            | 2110        |
|    policy_gradient_loss | -0.00282    |
|    std                  | 0.989       |
|    value_loss           | 0.18        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 101    |
|    iterations      | 212    |
|    time_elapsed    | 8589   |
|    total_timesteps | 868352 |
-------------------------------
Eval num_timesteps=870000, episode_reward=-378.89 +/- 146.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -379         |
| time/                   |              |
|    total_timesteps      | 870000       |
| train/                  |              |
|    approx_kl            | 0.0043235575 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0143       |
|    n_updates            | 2120         |
|    policy_gradient_loss | -0.00231     |
|    std                  | 0.989        |
|    value_loss           | 0.676        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 101    |
|    iterations      | 213    |
|    time_elapsed    | 8600   |
|    total_timesteps | 872448 |
-------------------------------
Eval num_timesteps=875000, episode_reward=-200.21 +/- 123.66
Episode length: 812.80 +/- 376.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -200         |
| time/                   |              |
|    total_timesteps      | 875000       |
| train/                  |              |
|    approx_kl            | 0.0053077275 |
|    clip_fraction        | 0.0289       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.111        |
|    n_updates            | 2130         |
|    policy_gradient_loss | -0.00141     |
|    std                  | 0.986        |
|    value_loss           | 9.84         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 101    |
|    iterations      | 214    |
|    time_elapsed    | 8609   |
|    total_timesteps | 876544 |
-------------------------------
Eval num_timesteps=880000, episode_reward=-384.74 +/- 210.89
Episode length: 807.80 +/- 386.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -385         |
| time/                   |              |
|    total_timesteps      | 880000       |
| train/                  |              |
|    approx_kl            | 0.0059191883 |
|    clip_fraction        | 0.0356       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.556        |
|    n_updates            | 2140         |
|    policy_gradient_loss | -0.00222     |
|    std                  | 0.978        |
|    value_loss           | 0.256        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 102    |
|    iterations      | 215    |
|    time_elapsed    | 8618   |
|    total_timesteps | 880640 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 216         |
|    time_elapsed         | 8625        |
|    total_timesteps      | 884736      |
| train/                  |             |
|    approx_kl            | 0.004887767 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.121       |
|    n_updates            | 2150        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 0.977       |
|    value_loss           | 10.7        |
-----------------------------------------
Eval num_timesteps=885000, episode_reward=-367.11 +/- 119.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -367         |
| time/                   |              |
|    total_timesteps      | 885000       |
| train/                  |              |
|    approx_kl            | 0.0054681795 |
|    clip_fraction        | 0.0458       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.017        |
|    n_updates            | 2160         |
|    policy_gradient_loss | -0.00333     |
|    std                  | 0.976        |
|    value_loss           | 0.064        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 102    |
|    iterations      | 217    |
|    time_elapsed    | 8633   |
|    total_timesteps | 888832 |
-------------------------------
Eval num_timesteps=890000, episode_reward=-285.17 +/- 151.64
Episode length: 810.80 +/- 380.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -285         |
| time/                   |              |
|    total_timesteps      | 890000       |
| train/                  |              |
|    approx_kl            | 0.0037692785 |
|    clip_fraction        | 0.0369       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0215       |
|    n_updates            | 2170         |
|    policy_gradient_loss | -0.00343     |
|    std                  | 0.973        |
|    value_loss           | 0.567        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 103    |
|    iterations      | 218    |
|    time_elapsed    | 8643   |
|    total_timesteps | 892928 |
-------------------------------
Eval num_timesteps=895000, episode_reward=-231.48 +/- 54.08
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -231        |
| time/                   |             |
|    total_timesteps      | 895000      |
| train/                  |             |
|    approx_kl            | 0.005406171 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.198       |
|    n_updates            | 2180        |
|    policy_gradient_loss | -0.000912   |
|    std                  | 0.971       |
|    value_loss           | 0.374       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 103    |
|    iterations      | 219    |
|    time_elapsed    | 8652   |
|    total_timesteps | 897024 |
-------------------------------
Eval num_timesteps=900000, episode_reward=-314.73 +/- 128.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -315         |
| time/                   |              |
|    total_timesteps      | 900000       |
| train/                  |              |
|    approx_kl            | 0.0039884406 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0476       |
|    n_updates            | 2190         |
|    policy_gradient_loss | -0.00273     |
|    std                  | 0.971        |
|    value_loss           | 0.391        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 104    |
|    iterations      | 220    |
|    time_elapsed    | 8661   |
|    total_timesteps | 901120 |
-------------------------------
Eval num_timesteps=905000, episode_reward=-302.68 +/- 191.70
Episode length: 807.40 +/- 387.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 807          |
|    mean_reward          | -303         |
| time/                   |              |
|    total_timesteps      | 905000       |
| train/                  |              |
|    approx_kl            | 0.0026366492 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.162        |
|    n_updates            | 2200         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 0.975        |
|    value_loss           | 2            |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 104    |
|    iterations      | 221    |
|    time_elapsed    | 8670   |
|    total_timesteps | 905216 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 104          |
|    iterations           | 222          |
|    time_elapsed         | 8676         |
|    total_timesteps      | 909312       |
| train/                  |              |
|    approx_kl            | 0.0044911555 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.106        |
|    n_updates            | 2210         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.976        |
|    value_loss           | 0.416        |
------------------------------------------
Eval num_timesteps=910000, episode_reward=-343.03 +/- 164.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -343         |
| time/                   |              |
|    total_timesteps      | 910000       |
| train/                  |              |
|    approx_kl            | 0.0038215218 |
|    clip_fraction        | 0.0253       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.037        |
|    n_updates            | 2220         |
|    policy_gradient_loss | -0.000717    |
|    std                  | 0.978        |
|    value_loss           | 0.11         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 105    |
|    iterations      | 223    |
|    time_elapsed    | 8685   |
|    total_timesteps | 913408 |
-------------------------------
Eval num_timesteps=915000, episode_reward=-360.27 +/- 128.15
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -360         |
| time/                   |              |
|    total_timesteps      | 915000       |
| train/                  |              |
|    approx_kl            | 0.0056335544 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00149     |
|    n_updates            | 2230         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.981        |
|    value_loss           | 0.389        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 105    |
|    iterations      | 224    |
|    time_elapsed    | 8695   |
|    total_timesteps | 917504 |
-------------------------------
Eval num_timesteps=920000, episode_reward=-371.05 +/- 101.13
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -371      |
| time/                   |           |
|    total_timesteps      | 920000    |
| train/                  |           |
|    approx_kl            | 0.0036857 |
|    clip_fraction        | 0.0155    |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.19     |
|    explained_variance   | 0.979     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.622     |
|    n_updates            | 2240      |
|    policy_gradient_loss | -0.0019   |
|    std                  | 0.981     |
|    value_loss           | 8.17      |
---------------------------------------
-------------------------------
| time/              |        |
|    fps             | 105    |
|    iterations      | 225    |
|    time_elapsed    | 8705   |
|    total_timesteps | 921600 |
-------------------------------
Eval num_timesteps=925000, episode_reward=-293.54 +/- 203.16
Episode length: 829.20 +/- 343.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 829          |
|    mean_reward          | -294         |
| time/                   |              |
|    total_timesteps      | 925000       |
| train/                  |              |
|    approx_kl            | 0.0043081227 |
|    clip_fraction        | 0.0137       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.22         |
|    n_updates            | 2250         |
|    policy_gradient_loss | -0.000591    |
|    std                  | 0.982        |
|    value_loss           | 3.15         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 106    |
|    iterations      | 226    |
|    time_elapsed    | 8719   |
|    total_timesteps | 925696 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 106          |
|    iterations           | 227          |
|    time_elapsed         | 8730         |
|    total_timesteps      | 929792       |
| train/                  |              |
|    approx_kl            | 0.0058714002 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0722       |
|    n_updates            | 2260         |
|    policy_gradient_loss | -0.00258     |
|    std                  | 0.98         |
|    value_loss           | 11.7         |
------------------------------------------
Eval num_timesteps=930000, episode_reward=-237.97 +/- 162.43
Episode length: 807.80 +/- 386.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -238         |
| time/                   |              |
|    total_timesteps      | 930000       |
| train/                  |              |
|    approx_kl            | 0.0045923586 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0425       |
|    n_updates            | 2270         |
|    policy_gradient_loss | -0.00363     |
|    std                  | 0.982        |
|    value_loss           | 0.194        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 106    |
|    iterations      | 228    |
|    time_elapsed    | 8740   |
|    total_timesteps | 933888 |
-------------------------------
Eval num_timesteps=935000, episode_reward=-319.83 +/- 125.79
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -320        |
| time/                   |             |
|    total_timesteps      | 935000      |
| train/                  |             |
|    approx_kl            | 0.004219307 |
|    clip_fraction        | 0.0326      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0781      |
|    n_updates            | 2280        |
|    policy_gradient_loss | -0.000881   |
|    std                  | 0.98        |
|    value_loss           | 0.498       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 107    |
|    iterations      | 229    |
|    time_elapsed    | 8750   |
|    total_timesteps | 937984 |
-------------------------------
Eval num_timesteps=940000, episode_reward=-332.46 +/- 104.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 940000       |
| train/                  |              |
|    approx_kl            | 0.0053784233 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.173        |
|    n_updates            | 2290         |
|    policy_gradient_loss | -0.00102     |
|    std                  | 0.979        |
|    value_loss           | 15           |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 107    |
|    iterations      | 230    |
|    time_elapsed    | 8760   |
|    total_timesteps | 942080 |
-------------------------------
Eval num_timesteps=945000, episode_reward=-300.22 +/- 40.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -300         |
| time/                   |              |
|    total_timesteps      | 945000       |
| train/                  |              |
|    approx_kl            | 0.0066483524 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.103        |
|    n_updates            | 2300         |
|    policy_gradient_loss | -0.00328     |
|    std                  | 0.977        |
|    value_loss           | 9.6          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 107    |
|    iterations      | 231    |
|    time_elapsed    | 8769   |
|    total_timesteps | 946176 |
-------------------------------
Eval num_timesteps=950000, episode_reward=-350.66 +/- 161.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -351         |
| time/                   |              |
|    total_timesteps      | 950000       |
| train/                  |              |
|    approx_kl            | 0.0056487103 |
|    clip_fraction        | 0.055        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0639       |
|    n_updates            | 2310         |
|    policy_gradient_loss | -0.00304     |
|    std                  | 0.974        |
|    value_loss           | 15.8         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 108    |
|    iterations      | 232    |
|    time_elapsed    | 8778   |
|    total_timesteps | 950272 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 108          |
|    iterations           | 233          |
|    time_elapsed         | 8785         |
|    total_timesteps      | 954368       |
| train/                  |              |
|    approx_kl            | 0.0048584584 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0226       |
|    n_updates            | 2320         |
|    policy_gradient_loss | -0.00271     |
|    std                  | 0.967        |
|    value_loss           | 0.264        |
------------------------------------------
Eval num_timesteps=955000, episode_reward=-364.85 +/- 155.34
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 955000       |
| train/                  |              |
|    approx_kl            | 0.0045297425 |
|    clip_fraction        | 0.0353       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0459       |
|    n_updates            | 2330         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 0.97         |
|    value_loss           | 0.124        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 108    |
|    iterations      | 234    |
|    time_elapsed    | 8794   |
|    total_timesteps | 958464 |
-------------------------------
Eval num_timesteps=960000, episode_reward=-320.93 +/- 156.95
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -321       |
| time/                   |            |
|    total_timesteps      | 960000     |
| train/                  |            |
|    approx_kl            | 0.00509175 |
|    clip_fraction        | 0.0453     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.16      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0493     |
|    n_updates            | 2340       |
|    policy_gradient_loss | -0.00375   |
|    std                  | 0.975      |
|    value_loss           | 0.175      |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 109    |
|    iterations      | 235    |
|    time_elapsed    | 8803   |
|    total_timesteps | 962560 |
-------------------------------
Eval num_timesteps=965000, episode_reward=-363.26 +/- 105.32
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -363        |
| time/                   |             |
|    total_timesteps      | 965000      |
| train/                  |             |
|    approx_kl            | 0.003429504 |
|    clip_fraction        | 0.0114      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.862       |
|    n_updates            | 2350        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 0.975       |
|    value_loss           | 8.15        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 109    |
|    iterations      | 236    |
|    time_elapsed    | 8813   |
|    total_timesteps | 966656 |
-------------------------------
Eval num_timesteps=970000, episode_reward=-283.17 +/- 180.39
Episode length: 816.00 +/- 370.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 816          |
|    mean_reward          | -283         |
| time/                   |              |
|    total_timesteps      | 970000       |
| train/                  |              |
|    approx_kl            | 0.0077021085 |
|    clip_fraction        | 0.0488       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 24.9         |
|    n_updates            | 2360         |
|    policy_gradient_loss | -0.00265     |
|    std                  | 0.98         |
|    value_loss           | 13.7         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 110    |
|    iterations      | 237    |
|    time_elapsed    | 8822   |
|    total_timesteps | 970752 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 110          |
|    iterations           | 238          |
|    time_elapsed         | 8828         |
|    total_timesteps      | 974848       |
| train/                  |              |
|    approx_kl            | 0.0038672283 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.12         |
|    n_updates            | 2370         |
|    policy_gradient_loss | -0.00312     |
|    std                  | 0.973        |
|    value_loss           | 6.28         |
------------------------------------------
Eval num_timesteps=975000, episode_reward=-234.67 +/- 89.57
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -235         |
| time/                   |              |
|    total_timesteps      | 975000       |
| train/                  |              |
|    approx_kl            | 0.0058082268 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.000702     |
|    n_updates            | 2380         |
|    policy_gradient_loss | -0.00295     |
|    std                  | 0.98         |
|    value_loss           | 0.187        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 110    |
|    iterations      | 239    |
|    time_elapsed    | 8837   |
|    total_timesteps | 978944 |
-------------------------------
Eval num_timesteps=980000, episode_reward=-398.71 +/- 142.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -399         |
| time/                   |              |
|    total_timesteps      | 980000       |
| train/                  |              |
|    approx_kl            | 0.0033657695 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.203        |
|    n_updates            | 2390         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 0.976        |
|    value_loss           | 3.4          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 111    |
|    iterations      | 240    |
|    time_elapsed    | 8848   |
|    total_timesteps | 983040 |
-------------------------------
Eval num_timesteps=985000, episode_reward=-235.51 +/- 117.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -236        |
| time/                   |             |
|    total_timesteps      | 985000      |
| train/                  |             |
|    approx_kl            | 0.004388953 |
|    clip_fraction        | 0.02        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0239      |
|    n_updates            | 2400        |
|    policy_gradient_loss | -0.000992   |
|    std                  | 0.976       |
|    value_loss           | 0.826       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 111    |
|    iterations      | 241    |
|    time_elapsed    | 8859   |
|    total_timesteps | 987136 |
-------------------------------
Eval num_timesteps=990000, episode_reward=-233.23 +/- 176.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -233         |
| time/                   |              |
|    total_timesteps      | 990000       |
| train/                  |              |
|    approx_kl            | 0.0042642504 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0353       |
|    n_updates            | 2410         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 0.969        |
|    value_loss           | 0.654        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 111    |
|    iterations      | 242    |
|    time_elapsed    | 8869   |
|    total_timesteps | 991232 |
-------------------------------
Eval num_timesteps=995000, episode_reward=-269.73 +/- 70.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -270         |
| time/                   |              |
|    total_timesteps      | 995000       |
| train/                  |              |
|    approx_kl            | 0.0043171467 |
|    clip_fraction        | 0.0166       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.108        |
|    n_updates            | 2420         |
|    policy_gradient_loss | -0.00226     |
|    std                  | 0.969        |
|    value_loss           | 1.5          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 112    |
|    iterations      | 243    |
|    time_elapsed    | 8879   |
|    total_timesteps | 995328 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 244         |
|    time_elapsed         | 8886        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.005611429 |
|    clip_fraction        | 0.0331      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.193       |
|    n_updates            | 2430        |
|    policy_gradient_loss | -0.00234    |
|    std                  | 0.97        |
|    value_loss           | 8.49        |
-----------------------------------------
Eval num_timesteps=1000000, episode_reward=-297.12 +/- 131.73
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -297        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.005033886 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0174      |
|    n_updates            | 2440        |
|    policy_gradient_loss | -0.0032     |
|    std                  | 0.972       |
|    value_loss           | 0.184       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 112     |
|    iterations      | 245     |
|    time_elapsed    | 8895    |
|    total_timesteps | 1003520 |
--------------------------------
Eval num_timesteps=1005000, episode_reward=-269.83 +/- 101.65
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -270        |
| time/                   |             |
|    total_timesteps      | 1005000     |
| train/                  |             |
|    approx_kl            | 0.003059422 |
|    clip_fraction        | 0.0107      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 26.9        |
|    n_updates            | 2450        |
|    policy_gradient_loss | -0.00069    |
|    std                  | 0.972       |
|    value_loss           | 10.5        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 113     |
|    iterations      | 246     |
|    time_elapsed    | 8905    |
|    total_timesteps | 1007616 |
--------------------------------
Eval num_timesteps=1010000, episode_reward=-324.27 +/- 33.58
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 1010000      |
| train/                  |              |
|    approx_kl            | 0.0049359226 |
|    clip_fraction        | 0.0482       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.683        |
|    n_updates            | 2460         |
|    policy_gradient_loss | -0.00374     |
|    std                  | 0.977        |
|    value_loss           | 0.596        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 113     |
|    iterations      | 247     |
|    time_elapsed    | 8914    |
|    total_timesteps | 1011712 |
--------------------------------
Eval num_timesteps=1015000, episode_reward=-290.42 +/- 145.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -290         |
| time/                   |              |
|    total_timesteps      | 1015000      |
| train/                  |              |
|    approx_kl            | 0.0052139396 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.18         |
|    n_updates            | 2470         |
|    policy_gradient_loss | -0.00292     |
|    std                  | 0.977        |
|    value_loss           | 6.34         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 113     |
|    iterations      | 248     |
|    time_elapsed    | 8926    |
|    total_timesteps | 1015808 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 114          |
|    iterations           | 249          |
|    time_elapsed         | 8932         |
|    total_timesteps      | 1019904      |
| train/                  |              |
|    approx_kl            | 0.0043732375 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0001       |
|    loss                 | 16.8         |
|    n_updates            | 2480         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.977        |
|    value_loss           | 11.4         |
------------------------------------------
Eval num_timesteps=1020000, episode_reward=-339.69 +/- 95.28
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -340        |
| time/                   |             |
|    total_timesteps      | 1020000     |
| train/                  |             |
|    approx_kl            | 0.004461944 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.066       |
|    n_updates            | 2490        |
|    policy_gradient_loss | -0.00175    |
|    std                  | 0.972       |
|    value_loss           | 0.118       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 114     |
|    iterations      | 250     |
|    time_elapsed    | 8941    |
|    total_timesteps | 1024000 |
--------------------------------
Eval num_timesteps=1025000, episode_reward=-234.51 +/- 127.35
Episode length: 814.40 +/- 373.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -235         |
| time/                   |              |
|    total_timesteps      | 1025000      |
| train/                  |              |
|    approx_kl            | 0.0059091877 |
|    clip_fraction        | 0.0496       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0352       |
|    n_updates            | 2500         |
|    policy_gradient_loss | -0.00457     |
|    std                  | 0.971        |
|    value_loss           | 0.562        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 114     |
|    iterations      | 251     |
|    time_elapsed    | 8951    |
|    total_timesteps | 1028096 |
--------------------------------
Eval num_timesteps=1030000, episode_reward=-307.77 +/- 120.61
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -308       |
| time/                   |            |
|    total_timesteps      | 1030000    |
| train/                  |            |
|    approx_kl            | 0.00436572 |
|    clip_fraction        | 0.022      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.13      |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.638      |
|    n_updates            | 2510       |
|    policy_gradient_loss | -0.000978  |
|    std                  | 0.965      |
|    value_loss           | 0.445      |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 115     |
|    iterations      | 252     |
|    time_elapsed    | 8961    |
|    total_timesteps | 1032192 |
--------------------------------
Eval num_timesteps=1035000, episode_reward=-171.05 +/- 135.66
Episode length: 655.80 +/- 425.30
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 656          |
|    mean_reward          | -171         |
| time/                   |              |
|    total_timesteps      | 1035000      |
| train/                  |              |
|    approx_kl            | 0.0016776365 |
|    clip_fraction        | 0.00679      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.12        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0701       |
|    n_updates            | 2520         |
|    policy_gradient_loss | -0.000767    |
|    std                  | 0.965        |
|    value_loss           | 5.47         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 115     |
|    iterations      | 253     |
|    time_elapsed    | 8970    |
|    total_timesteps | 1036288 |
--------------------------------
Eval num_timesteps=1040000, episode_reward=-260.75 +/- 141.66
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -261        |
| time/                   |             |
|    total_timesteps      | 1040000     |
| train/                  |             |
|    approx_kl            | 0.004481077 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0557      |
|    n_updates            | 2530        |
|    policy_gradient_loss | -0.00295    |
|    std                  | 0.975       |
|    value_loss           | 0.126       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 115     |
|    iterations      | 254     |
|    time_elapsed    | 8980    |
|    total_timesteps | 1040384 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 116          |
|    iterations           | 255          |
|    time_elapsed         | 8986         |
|    total_timesteps      | 1044480      |
| train/                  |              |
|    approx_kl            | 0.0047032614 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.027        |
|    n_updates            | 2540         |
|    policy_gradient_loss | -0.00127     |
|    std                  | 0.979        |
|    value_loss           | 0.371        |
------------------------------------------
Eval num_timesteps=1045000, episode_reward=-351.17 +/- 115.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -351         |
| time/                   |              |
|    total_timesteps      | 1045000      |
| train/                  |              |
|    approx_kl            | 0.0045493105 |
|    clip_fraction        | 0.0351       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0249       |
|    n_updates            | 2550         |
|    policy_gradient_loss | -0.00321     |
|    std                  | 0.982        |
|    value_loss           | 0.148        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 116     |
|    iterations      | 256     |
|    time_elapsed    | 8996    |
|    total_timesteps | 1048576 |
--------------------------------
Eval num_timesteps=1050000, episode_reward=-247.68 +/- 72.75
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -248        |
| time/                   |             |
|    total_timesteps      | 1050000     |
| train/                  |             |
|    approx_kl            | 0.004503008 |
|    clip_fraction        | 0.0155      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0918      |
|    n_updates            | 2560        |
|    policy_gradient_loss | -0.000595   |
|    std                  | 0.984       |
|    value_loss           | 4.72        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 116     |
|    iterations      | 257     |
|    time_elapsed    | 9007    |
|    total_timesteps | 1052672 |
--------------------------------
Eval num_timesteps=1055000, episode_reward=-476.72 +/- 99.16
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -477        |
| time/                   |             |
|    total_timesteps      | 1055000     |
| train/                  |             |
|    approx_kl            | 0.004564329 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.215       |
|    n_updates            | 2570        |
|    policy_gradient_loss | -0.00198    |
|    std                  | 0.982       |
|    value_loss           | 5.07        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 117     |
|    iterations      | 258     |
|    time_elapsed    | 9017    |
|    total_timesteps | 1056768 |
--------------------------------
Eval num_timesteps=1060000, episode_reward=-266.58 +/- 69.36
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -267        |
| time/                   |             |
|    total_timesteps      | 1060000     |
| train/                  |             |
|    approx_kl            | 0.005050104 |
|    clip_fraction        | 0.0228      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0221      |
|    n_updates            | 2580        |
|    policy_gradient_loss | -0.00218    |
|    std                  | 0.979       |
|    value_loss           | 1.61        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 117     |
|    iterations      | 259     |
|    time_elapsed    | 9027    |
|    total_timesteps | 1060864 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 117          |
|    iterations           | 260          |
|    time_elapsed         | 9033         |
|    total_timesteps      | 1064960      |
| train/                  |              |
|    approx_kl            | 0.0046675354 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0158       |
|    n_updates            | 2590         |
|    policy_gradient_loss | -0.00232     |
|    std                  | 0.982        |
|    value_loss           | 0.37         |
------------------------------------------
Eval num_timesteps=1065000, episode_reward=-339.19 +/- 146.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -339         |
| time/                   |              |
|    total_timesteps      | 1065000      |
| train/                  |              |
|    approx_kl            | 0.0044515543 |
|    clip_fraction        | 0.0235       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.000586     |
|    n_updates            | 2600         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 0.98         |
|    value_loss           | 0.137        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 118     |
|    iterations      | 261     |
|    time_elapsed    | 9043    |
|    total_timesteps | 1069056 |
--------------------------------
Eval num_timesteps=1070000, episode_reward=-287.23 +/- 106.41
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -287        |
| time/                   |             |
|    total_timesteps      | 1070000     |
| train/                  |             |
|    approx_kl            | 0.004105886 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0298      |
|    n_updates            | 2610        |
|    policy_gradient_loss | -0.00214    |
|    std                  | 0.978       |
|    value_loss           | 2.82        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 118     |
|    iterations      | 262     |
|    time_elapsed    | 9051    |
|    total_timesteps | 1073152 |
--------------------------------
Eval num_timesteps=1075000, episode_reward=-244.75 +/- 159.49
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -245        |
| time/                   |             |
|    total_timesteps      | 1075000     |
| train/                  |             |
|    approx_kl            | 0.006579279 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.06        |
|    n_updates            | 2620        |
|    policy_gradient_loss | -0.0024     |
|    std                  | 0.978       |
|    value_loss           | 9.3         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 118     |
|    iterations      | 263     |
|    time_elapsed    | 9062    |
|    total_timesteps | 1077248 |
--------------------------------
Eval num_timesteps=1080000, episode_reward=-230.12 +/- 68.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -230         |
| time/                   |              |
|    total_timesteps      | 1080000      |
| train/                  |              |
|    approx_kl            | 0.0042451536 |
|    clip_fraction        | 0.0228       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.67         |
|    n_updates            | 2630         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 0.978        |
|    value_loss           | 2.84         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 119     |
|    iterations      | 264     |
|    time_elapsed    | 9074    |
|    total_timesteps | 1081344 |
--------------------------------
Eval num_timesteps=1085000, episode_reward=-298.44 +/- 102.16
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -298        |
| time/                   |             |
|    total_timesteps      | 1085000     |
| train/                  |             |
|    approx_kl            | 0.005192929 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.43        |
|    n_updates            | 2640        |
|    policy_gradient_loss | -0.000448   |
|    std                  | 0.979       |
|    value_loss           | 6.33        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 119     |
|    iterations      | 265     |
|    time_elapsed    | 9084    |
|    total_timesteps | 1085440 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 119         |
|    iterations           | 266         |
|    time_elapsed         | 9090        |
|    total_timesteps      | 1089536     |
| train/                  |             |
|    approx_kl            | 0.006056486 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.969       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.17        |
|    n_updates            | 2650        |
|    policy_gradient_loss | -0.00396    |
|    std                  | 0.987       |
|    value_loss           | 6.9         |
-----------------------------------------
Eval num_timesteps=1090000, episode_reward=-387.82 +/- 152.25
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -388        |
| time/                   |             |
|    total_timesteps      | 1090000     |
| train/                  |             |
|    approx_kl            | 0.005406647 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0067      |
|    n_updates            | 2660        |
|    policy_gradient_loss | -0.00209    |
|    std                  | 0.983       |
|    value_loss           | 0.124       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 120     |
|    iterations      | 267     |
|    time_elapsed    | 9100    |
|    total_timesteps | 1093632 |
--------------------------------
Eval num_timesteps=1095000, episode_reward=-247.26 +/- 54.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -247         |
| time/                   |              |
|    total_timesteps      | 1095000      |
| train/                  |              |
|    approx_kl            | 0.0051927874 |
|    clip_fraction        | 0.0424       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0505       |
|    n_updates            | 2670         |
|    policy_gradient_loss | -0.00301     |
|    std                  | 0.978        |
|    value_loss           | 0.172        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 120     |
|    iterations      | 268     |
|    time_elapsed    | 9110    |
|    total_timesteps | 1097728 |
--------------------------------
Eval num_timesteps=1100000, episode_reward=-470.90 +/- 153.60
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -471        |
| time/                   |             |
|    total_timesteps      | 1100000     |
| train/                  |             |
|    approx_kl            | 0.001956462 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.16       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0001      |
|    loss                 | 6.5         |
|    n_updates            | 2680        |
|    policy_gradient_loss | -0.000973   |
|    std                  | 0.977       |
|    value_loss           | 2.35        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 120     |
|    iterations      | 269     |
|    time_elapsed    | 9122    |
|    total_timesteps | 1101824 |
--------------------------------
Eval num_timesteps=1105000, episode_reward=-374.78 +/- 128.59
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -375         |
| time/                   |              |
|    total_timesteps      | 1105000      |
| train/                  |              |
|    approx_kl            | 0.0049183536 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0792       |
|    n_updates            | 2690         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 0.974        |
|    value_loss           | 8.21         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 121     |
|    iterations      | 270     |
|    time_elapsed    | 9132    |
|    total_timesteps | 1105920 |
--------------------------------
Eval num_timesteps=1110000, episode_reward=-452.84 +/- 110.69
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -453         |
| time/                   |              |
|    total_timesteps      | 1110000      |
| train/                  |              |
|    approx_kl            | 0.0027116532 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.59         |
|    n_updates            | 2700         |
|    policy_gradient_loss | -0.000807    |
|    std                  | 0.974        |
|    value_loss           | 5.02         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 121     |
|    iterations      | 271     |
|    time_elapsed    | 9142    |
|    total_timesteps | 1110016 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 121          |
|    iterations           | 272          |
|    time_elapsed         | 9148         |
|    total_timesteps      | 1114112      |
| train/                  |              |
|    approx_kl            | 0.0051474627 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0537       |
|    n_updates            | 2710         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.976        |
|    value_loss           | 3.47         |
------------------------------------------
Eval num_timesteps=1115000, episode_reward=-250.35 +/- 185.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -250         |
| time/                   |              |
|    total_timesteps      | 1115000      |
| train/                  |              |
|    approx_kl            | 0.0028060114 |
|    clip_fraction        | 0.0127       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.028        |
|    n_updates            | 2720         |
|    policy_gradient_loss | -3.96e-05    |
|    std                  | 0.974        |
|    value_loss           | 0.0884       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 122     |
|    iterations      | 273     |
|    time_elapsed    | 9157    |
|    total_timesteps | 1118208 |
--------------------------------
Eval num_timesteps=1120000, episode_reward=-291.27 +/- 135.72
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -291         |
| time/                   |              |
|    total_timesteps      | 1120000      |
| train/                  |              |
|    approx_kl            | 0.0052438523 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0505       |
|    n_updates            | 2730         |
|    policy_gradient_loss | -0.00303     |
|    std                  | 0.977        |
|    value_loss           | 0.457        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 122     |
|    iterations      | 274     |
|    time_elapsed    | 9170    |
|    total_timesteps | 1122304 |
--------------------------------
Eval num_timesteps=1125000, episode_reward=-331.95 +/- 220.78
Episode length: 814.40 +/- 373.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 1125000      |
| train/                  |              |
|    approx_kl            | 0.0052068825 |
|    clip_fraction        | 0.0223       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.16         |
|    n_updates            | 2740         |
|    policy_gradient_loss | -0.00245     |
|    std                  | 0.976        |
|    value_loss           | 3.59         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 122     |
|    iterations      | 275     |
|    time_elapsed    | 9181    |
|    total_timesteps | 1126400 |
--------------------------------
Eval num_timesteps=1130000, episode_reward=-283.00 +/- 183.02
Episode length: 814.80 +/- 372.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -283         |
| time/                   |              |
|    total_timesteps      | 1130000      |
| train/                  |              |
|    approx_kl            | 0.0036440622 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.426        |
|    n_updates            | 2750         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 0.971        |
|    value_loss           | 3.3          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 123     |
|    iterations      | 276     |
|    time_elapsed    | 9190    |
|    total_timesteps | 1130496 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 123          |
|    iterations           | 277          |
|    time_elapsed         | 9197         |
|    total_timesteps      | 1134592      |
| train/                  |              |
|    approx_kl            | 0.0034795348 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.13         |
|    n_updates            | 2760         |
|    policy_gradient_loss | -0.000331    |
|    std                  | 0.969        |
|    value_loss           | 18           |
------------------------------------------
Eval num_timesteps=1135000, episode_reward=-239.16 +/- 130.38
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -239        |
| time/                   |             |
|    total_timesteps      | 1135000     |
| train/                  |             |
|    approx_kl            | 0.006369942 |
|    clip_fraction        | 0.0618      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0272      |
|    n_updates            | 2770        |
|    policy_gradient_loss | -0.00383    |
|    std                  | 0.957       |
|    value_loss           | 0.116       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 123     |
|    iterations      | 278     |
|    time_elapsed    | 9208    |
|    total_timesteps | 1138688 |
--------------------------------
Eval num_timesteps=1140000, episode_reward=-191.49 +/- 73.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -191         |
| time/                   |              |
|    total_timesteps      | 1140000      |
| train/                  |              |
|    approx_kl            | 0.0051311348 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.956        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.077        |
|    n_updates            | 2780         |
|    policy_gradient_loss | -0.00219     |
|    std                  | 0.958        |
|    value_loss           | 1.13         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 123     |
|    iterations      | 279     |
|    time_elapsed    | 9217    |
|    total_timesteps | 1142784 |
--------------------------------
Eval num_timesteps=1145000, episode_reward=-176.08 +/- 71.18
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -176        |
| time/                   |             |
|    total_timesteps      | 1145000     |
| train/                  |             |
|    approx_kl            | 0.005493343 |
|    clip_fraction        | 0.0334      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.5         |
|    n_updates            | 2790        |
|    policy_gradient_loss | -0.0029     |
|    std                  | 0.955       |
|    value_loss           | 2.17        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 124     |
|    iterations      | 280     |
|    time_elapsed    | 9228    |
|    total_timesteps | 1146880 |
--------------------------------
Eval num_timesteps=1150000, episode_reward=-322.37 +/- 95.11
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -322         |
| time/                   |              |
|    total_timesteps      | 1150000      |
| train/                  |              |
|    approx_kl            | 0.0058899587 |
|    clip_fraction        | 0.0465       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0206       |
|    n_updates            | 2800         |
|    policy_gradient_loss | -0.00259     |
|    std                  | 0.959        |
|    value_loss           | 1.1          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 124     |
|    iterations      | 281     |
|    time_elapsed    | 9240    |
|    total_timesteps | 1150976 |
--------------------------------
Eval num_timesteps=1155000, episode_reward=-224.81 +/- 57.97
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -225         |
| time/                   |              |
|    total_timesteps      | 1155000      |
| train/                  |              |
|    approx_kl            | 0.0049286094 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.148        |
|    n_updates            | 2810         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.961        |
|    value_loss           | 3.16         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 124     |
|    iterations      | 282     |
|    time_elapsed    | 9251    |
|    total_timesteps | 1155072 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 125          |
|    iterations           | 283          |
|    time_elapsed         | 9257         |
|    total_timesteps      | 1159168      |
| train/                  |              |
|    approx_kl            | 0.0030681938 |
|    clip_fraction        | 0.0126       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.033        |
|    n_updates            | 2820         |
|    policy_gradient_loss | -0.000902    |
|    std                  | 0.953        |
|    value_loss           | 0.248        |
------------------------------------------
Eval num_timesteps=1160000, episode_reward=-413.45 +/- 130.35
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 1160000     |
| train/                  |             |
|    approx_kl            | 0.004208802 |
|    clip_fraction        | 0.0244      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0265      |
|    n_updates            | 2830        |
|    policy_gradient_loss | -0.000884   |
|    std                  | 0.958       |
|    value_loss           | 0.15        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 125     |
|    iterations      | 284     |
|    time_elapsed    | 9266    |
|    total_timesteps | 1163264 |
--------------------------------
Eval num_timesteps=1165000, episode_reward=-287.48 +/- 191.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -287         |
| time/                   |              |
|    total_timesteps      | 1165000      |
| train/                  |              |
|    approx_kl            | 0.0048056366 |
|    clip_fraction        | 0.0361       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0386       |
|    n_updates            | 2840         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 0.955        |
|    value_loss           | 0.429        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 125     |
|    iterations      | 285     |
|    time_elapsed    | 9276    |
|    total_timesteps | 1167360 |
--------------------------------
Eval num_timesteps=1170000, episode_reward=-327.71 +/- 181.95
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -328       |
| time/                   |            |
|    total_timesteps      | 1170000    |
| train/                  |            |
|    approx_kl            | 0.00422554 |
|    clip_fraction        | 0.0203     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.09      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0264     |
|    n_updates            | 2850       |
|    policy_gradient_loss | -0.000643  |
|    std                  | 0.954      |
|    value_loss           | 1.22       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 126     |
|    iterations      | 286     |
|    time_elapsed    | 9286    |
|    total_timesteps | 1171456 |
--------------------------------
Eval num_timesteps=1175000, episode_reward=-272.52 +/- 82.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -273         |
| time/                   |              |
|    total_timesteps      | 1175000      |
| train/                  |              |
|    approx_kl            | 0.0062180543 |
|    clip_fraction        | 0.0409       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.107        |
|    n_updates            | 2860         |
|    policy_gradient_loss | -0.00413     |
|    std                  | 0.952        |
|    value_loss           | 4.97         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 126     |
|    iterations      | 287     |
|    time_elapsed    | 9297    |
|    total_timesteps | 1175552 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 126         |
|    iterations           | 288         |
|    time_elapsed         | 9305        |
|    total_timesteps      | 1179648     |
| train/                  |             |
|    approx_kl            | 0.004324979 |
|    clip_fraction        | 0.0178      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0697      |
|    n_updates            | 2870        |
|    policy_gradient_loss | -0.0013     |
|    std                  | 0.948       |
|    value_loss           | 7.65        |
-----------------------------------------
Eval num_timesteps=1180000, episode_reward=-400.47 +/- 126.87
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -400         |
| time/                   |              |
|    total_timesteps      | 1180000      |
| train/                  |              |
|    approx_kl            | 0.0048703915 |
|    clip_fraction        | 0.0439       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0545       |
|    n_updates            | 2880         |
|    policy_gradient_loss | -0.00173     |
|    std                  | 0.951        |
|    value_loss           | 0.0725       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 127     |
|    iterations      | 289     |
|    time_elapsed    | 9315    |
|    total_timesteps | 1183744 |
--------------------------------
Eval num_timesteps=1185000, episode_reward=-308.20 +/- 191.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -308         |
| time/                   |              |
|    total_timesteps      | 1185000      |
| train/                  |              |
|    approx_kl            | 0.0041499413 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.961        |
|    learning_rate        | 0.0001       |
|    loss                 | 14.1         |
|    n_updates            | 2890         |
|    policy_gradient_loss | -0.00182     |
|    std                  | 0.948        |
|    value_loss           | 6.16         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 127     |
|    iterations      | 290     |
|    time_elapsed    | 9325    |
|    total_timesteps | 1187840 |
--------------------------------
Eval num_timesteps=1190000, episode_reward=-453.08 +/- 152.23
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -453       |
| time/                   |            |
|    total_timesteps      | 1190000    |
| train/                  |            |
|    approx_kl            | 0.00701489 |
|    clip_fraction        | 0.0454     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.07      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0001     |
|    loss                 | 27         |
|    n_updates            | 2900       |
|    policy_gradient_loss | -0.00295   |
|    std                  | 0.946      |
|    value_loss           | 7.77       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 127     |
|    iterations      | 291     |
|    time_elapsed    | 9337    |
|    total_timesteps | 1191936 |
--------------------------------
Eval num_timesteps=1195000, episode_reward=-167.16 +/- 99.59
Episode length: 809.40 +/- 383.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 809          |
|    mean_reward          | -167         |
| time/                   |              |
|    total_timesteps      | 1195000      |
| train/                  |              |
|    approx_kl            | 0.0043336907 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.348        |
|    n_updates            | 2910         |
|    policy_gradient_loss | -0.00197     |
|    std                  | 0.946        |
|    value_loss           | 10           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 127     |
|    iterations      | 292     |
|    time_elapsed    | 9349    |
|    total_timesteps | 1196032 |
--------------------------------
Eval num_timesteps=1200000, episode_reward=-226.88 +/- 179.24
Episode length: 812.20 +/- 377.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | -227         |
| time/                   |              |
|    total_timesteps      | 1200000      |
| train/                  |              |
|    approx_kl            | 0.0050082738 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0396       |
|    n_updates            | 2920         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 0.944        |
|    value_loss           | 0.136        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 128     |
|    iterations      | 293     |
|    time_elapsed    | 9359    |
|    total_timesteps | 1200128 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 128         |
|    iterations           | 294         |
|    time_elapsed         | 9366        |
|    total_timesteps      | 1204224     |
| train/                  |             |
|    approx_kl            | 0.004735715 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.166       |
|    n_updates            | 2930        |
|    policy_gradient_loss | -0.00298    |
|    std                  | 0.944       |
|    value_loss           | 4.8         |
-----------------------------------------
Eval num_timesteps=1205000, episode_reward=-306.39 +/- 151.98
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -306        |
| time/                   |             |
|    total_timesteps      | 1205000     |
| train/                  |             |
|    approx_kl            | 0.006022596 |
|    clip_fraction        | 0.063       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00361    |
|    n_updates            | 2940        |
|    policy_gradient_loss | -0.00377    |
|    std                  | 0.943       |
|    value_loss           | 0.0632      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 128     |
|    iterations      | 295     |
|    time_elapsed    | 9375    |
|    total_timesteps | 1208320 |
--------------------------------
Eval num_timesteps=1210000, episode_reward=-341.01 +/- 166.81
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -341        |
| time/                   |             |
|    total_timesteps      | 1210000     |
| train/                  |             |
|    approx_kl            | 0.004883672 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.05       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.179       |
|    n_updates            | 2950        |
|    policy_gradient_loss | -0.00121    |
|    std                  | 0.941       |
|    value_loss           | 9.2         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 129     |
|    iterations      | 296     |
|    time_elapsed    | 9385    |
|    total_timesteps | 1212416 |
--------------------------------
Eval num_timesteps=1215000, episode_reward=-212.91 +/- 87.90
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -213         |
| time/                   |              |
|    total_timesteps      | 1215000      |
| train/                  |              |
|    approx_kl            | 0.0035074723 |
|    clip_fraction        | 0.00872      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.503        |
|    n_updates            | 2960         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 0.939        |
|    value_loss           | 9.5          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 129     |
|    iterations      | 297     |
|    time_elapsed    | 9395    |
|    total_timesteps | 1216512 |
--------------------------------
Eval num_timesteps=1220000, episode_reward=-269.82 +/- 121.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -270         |
| time/                   |              |
|    total_timesteps      | 1220000      |
| train/                  |              |
|    approx_kl            | 0.0045907767 |
|    clip_fraction        | 0.0534       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0855       |
|    n_updates            | 2970         |
|    policy_gradient_loss | -0.00217     |
|    std                  | 0.941        |
|    value_loss           | 0.248        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 129     |
|    iterations      | 298     |
|    time_elapsed    | 9404    |
|    total_timesteps | 1220608 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 130          |
|    iterations           | 299          |
|    time_elapsed         | 9411         |
|    total_timesteps      | 1224704      |
| train/                  |              |
|    approx_kl            | 0.0038715946 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.3          |
|    n_updates            | 2980         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 0.947        |
|    value_loss           | 1.44         |
------------------------------------------
Eval num_timesteps=1225000, episode_reward=-371.97 +/- 99.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -372         |
| time/                   |              |
|    total_timesteps      | 1225000      |
| train/                  |              |
|    approx_kl            | 0.0034158337 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0397       |
|    n_updates            | 2990         |
|    policy_gradient_loss | -0.00163     |
|    std                  | 0.936        |
|    value_loss           | 0.0646       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 130     |
|    iterations      | 300     |
|    time_elapsed    | 9421    |
|    total_timesteps | 1228800 |
--------------------------------
Eval num_timesteps=1230000, episode_reward=-268.59 +/- 215.88
Episode length: 811.40 +/- 379.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -269        |
| time/                   |             |
|    total_timesteps      | 1230000     |
| train/                  |             |
|    approx_kl            | 0.005200586 |
|    clip_fraction        | 0.0237      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.431       |
|    n_updates            | 3000        |
|    policy_gradient_loss | -0.00348    |
|    std                  | 0.937       |
|    value_loss           | 4.31        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 130     |
|    iterations      | 301     |
|    time_elapsed    | 9436    |
|    total_timesteps | 1232896 |
--------------------------------
Eval num_timesteps=1235000, episode_reward=-272.90 +/- 106.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -273         |
| time/                   |              |
|    total_timesteps      | 1235000      |
| train/                  |              |
|    approx_kl            | 0.0036682964 |
|    clip_fraction        | 0.0168       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.86         |
|    n_updates            | 3010         |
|    policy_gradient_loss | -0.000483    |
|    std                  | 0.938        |
|    value_loss           | 2.47         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 130     |
|    iterations      | 302     |
|    time_elapsed    | 9451    |
|    total_timesteps | 1236992 |
--------------------------------
Eval num_timesteps=1240000, episode_reward=-389.17 +/- 136.06
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -389        |
| time/                   |             |
|    total_timesteps      | 1240000     |
| train/                  |             |
|    approx_kl            | 0.005586054 |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.33        |
|    n_updates            | 3020        |
|    policy_gradient_loss | -0.00435    |
|    std                  | 0.932       |
|    value_loss           | 2.32        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 131     |
|    iterations      | 303     |
|    time_elapsed    | 9464    |
|    total_timesteps | 1241088 |
--------------------------------
Eval num_timesteps=1245000, episode_reward=-189.98 +/- 60.16
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -190         |
| time/                   |              |
|    total_timesteps      | 1245000      |
| train/                  |              |
|    approx_kl            | 0.0031260885 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 8.04         |
|    n_updates            | 3030         |
|    policy_gradient_loss | -0.00149     |
|    std                  | 0.936        |
|    value_loss           | 4.02         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 131     |
|    iterations      | 304     |
|    time_elapsed    | 9476    |
|    total_timesteps | 1245184 |
--------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 131        |
|    iterations           | 305        |
|    time_elapsed         | 9485       |
|    total_timesteps      | 1249280    |
| train/                  |            |
|    approx_kl            | 0.00636491 |
|    clip_fraction        | 0.0413     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.03      |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.139      |
|    n_updates            | 3040       |
|    policy_gradient_loss | -0.00194   |
|    std                  | 0.931      |
|    value_loss           | 17.1       |
----------------------------------------
Eval num_timesteps=1250000, episode_reward=-220.42 +/- 86.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -220         |
| time/                   |              |
|    total_timesteps      | 1250000      |
| train/                  |              |
|    approx_kl            | 0.0052055153 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0106       |
|    n_updates            | 3050         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 0.935        |
|    value_loss           | 0.139        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 131     |
|    iterations      | 306     |
|    time_elapsed    | 9496    |
|    total_timesteps | 1253376 |
--------------------------------
Eval num_timesteps=1255000, episode_reward=-401.13 +/- 154.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -401        |
| time/                   |             |
|    total_timesteps      | 1255000     |
| train/                  |             |
|    approx_kl            | 0.004053903 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.03       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.08        |
|    n_updates            | 3060        |
|    policy_gradient_loss | -0.00277    |
|    std                  | 0.935       |
|    value_loss           | 7.01        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 132     |
|    iterations      | 307     |
|    time_elapsed    | 9508    |
|    total_timesteps | 1257472 |
--------------------------------
Eval num_timesteps=1260000, episode_reward=-258.50 +/- 54.32
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -259        |
| time/                   |             |
|    total_timesteps      | 1260000     |
| train/                  |             |
|    approx_kl            | 0.005497775 |
|    clip_fraction        | 0.0342      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.279       |
|    n_updates            | 3070        |
|    policy_gradient_loss | -0.00206    |
|    std                  | 0.937       |
|    value_loss           | 1.47        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 132     |
|    iterations      | 308     |
|    time_elapsed    | 9519    |
|    total_timesteps | 1261568 |
--------------------------------
Eval num_timesteps=1265000, episode_reward=-274.42 +/- 152.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -274         |
| time/                   |              |
|    total_timesteps      | 1265000      |
| train/                  |              |
|    approx_kl            | 0.0045870896 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0694       |
|    n_updates            | 3080         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 0.938        |
|    value_loss           | 3.98         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 132     |
|    iterations      | 309     |
|    time_elapsed    | 9530    |
|    total_timesteps | 1265664 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 133         |
|    iterations           | 310         |
|    time_elapsed         | 9537        |
|    total_timesteps      | 1269760     |
| train/                  |             |
|    approx_kl            | 0.005312548 |
|    clip_fraction        | 0.0395      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.04       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0836      |
|    n_updates            | 3090        |
|    policy_gradient_loss | -0.00156    |
|    std                  | 0.936       |
|    value_loss           | 4.84        |
-----------------------------------------
Eval num_timesteps=1270000, episode_reward=-297.78 +/- 254.92
Episode length: 813.60 +/- 374.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -298         |
| time/                   |              |
|    total_timesteps      | 1270000      |
| train/                  |              |
|    approx_kl            | 0.0031101564 |
|    clip_fraction        | 0.0359       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0661       |
|    n_updates            | 3100         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 0.937        |
|    value_loss           | 0.935        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 133     |
|    iterations      | 311     |
|    time_elapsed    | 9546    |
|    total_timesteps | 1273856 |
--------------------------------
Eval num_timesteps=1275000, episode_reward=-428.11 +/- 235.44
Episode length: 831.20 +/- 339.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 831          |
|    mean_reward          | -428         |
| time/                   |              |
|    total_timesteps      | 1275000      |
| train/                  |              |
|    approx_kl            | 0.0049897046 |
|    clip_fraction        | 0.0416       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.05        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.687        |
|    n_updates            | 3110         |
|    policy_gradient_loss | -0.00323     |
|    std                  | 0.944        |
|    value_loss           | 0.374        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 133     |
|    iterations      | 312     |
|    time_elapsed    | 9555    |
|    total_timesteps | 1277952 |
--------------------------------
Eval num_timesteps=1280000, episode_reward=-295.96 +/- 134.51
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -296        |
| time/                   |             |
|    total_timesteps      | 1280000     |
| train/                  |             |
|    approx_kl            | 0.004578252 |
|    clip_fraction        | 0.0296      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.07       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 8.62        |
|    n_updates            | 3120        |
|    policy_gradient_loss | -0.00146    |
|    std                  | 0.944       |
|    value_loss           | 6.12        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 134     |
|    iterations      | 313     |
|    time_elapsed    | 9565    |
|    total_timesteps | 1282048 |
--------------------------------
Eval num_timesteps=1285000, episode_reward=-197.83 +/- 105.62
Episode length: 813.40 +/- 375.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -198         |
| time/                   |              |
|    total_timesteps      | 1285000      |
| train/                  |              |
|    approx_kl            | 0.0044269366 |
|    clip_fraction        | 0.0264       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.116        |
|    n_updates            | 3130         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.943        |
|    value_loss           | 5.81         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 134     |
|    iterations      | 314     |
|    time_elapsed    | 9574    |
|    total_timesteps | 1286144 |
--------------------------------
Eval num_timesteps=1290000, episode_reward=-227.64 +/- 123.97
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -228        |
| time/                   |             |
|    total_timesteps      | 1290000     |
| train/                  |             |
|    approx_kl            | 0.005540533 |
|    clip_fraction        | 0.0422      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.398       |
|    n_updates            | 3140        |
|    policy_gradient_loss | -0.00281    |
|    std                  | 0.944       |
|    value_loss           | 0.618       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 134     |
|    iterations      | 315     |
|    time_elapsed    | 9584    |
|    total_timesteps | 1290240 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 134          |
|    iterations           | 316          |
|    time_elapsed         | 9590         |
|    total_timesteps      | 1294336      |
| train/                  |              |
|    approx_kl            | 0.0046218503 |
|    clip_fraction        | 0.0323       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0661       |
|    n_updates            | 3150         |
|    policy_gradient_loss | -0.000682    |
|    std                  | 0.943        |
|    value_loss           | 11.1         |
------------------------------------------
Eval num_timesteps=1295000, episode_reward=-320.44 +/- 147.19
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -320       |
| time/                   |            |
|    total_timesteps      | 1295000    |
| train/                  |            |
|    approx_kl            | 0.00565973 |
|    clip_fraction        | 0.0336     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.06      |
|    explained_variance   | 1          |
|    learning_rate        | 0.0001     |
|    loss                 | 0.012      |
|    n_updates            | 3160       |
|    policy_gradient_loss | -0.00241   |
|    std                  | 0.944      |
|    value_loss           | 0.0602     |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 135     |
|    iterations      | 317     |
|    time_elapsed    | 9600    |
|    total_timesteps | 1298432 |
--------------------------------
Eval num_timesteps=1300000, episode_reward=-253.75 +/- 113.64
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -254         |
| time/                   |              |
|    total_timesteps      | 1300000      |
| train/                  |              |
|    approx_kl            | 0.0033132834 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.989        |
|    n_updates            | 3170         |
|    policy_gradient_loss | -0.00117     |
|    std                  | 0.94         |
|    value_loss           | 2.74         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 135     |
|    iterations      | 318     |
|    time_elapsed    | 9609    |
|    total_timesteps | 1302528 |
--------------------------------
Eval num_timesteps=1305000, episode_reward=-311.17 +/- 193.80
Episode length: 813.20 +/- 375.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -311         |
| time/                   |              |
|    total_timesteps      | 1305000      |
| train/                  |              |
|    approx_kl            | 0.0040046703 |
|    clip_fraction        | 0.0275       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.04        |
|    explained_variance   | 0.936        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.763        |
|    n_updates            | 3180         |
|    policy_gradient_loss | -0.00285     |
|    std                  | 0.936        |
|    value_loss           | 0.819        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 135     |
|    iterations      | 319     |
|    time_elapsed    | 9618    |
|    total_timesteps | 1306624 |
--------------------------------
Eval num_timesteps=1310000, episode_reward=-241.82 +/- 178.23
Episode length: 819.40 +/- 363.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 819          |
|    mean_reward          | -242         |
| time/                   |              |
|    total_timesteps      | 1310000      |
| train/                  |              |
|    approx_kl            | 0.0055202693 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.03        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.109        |
|    n_updates            | 3190         |
|    policy_gradient_loss | -0.00247     |
|    std                  | 0.934        |
|    value_loss           | 8.08         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 136     |
|    iterations      | 320     |
|    time_elapsed    | 9629    |
|    total_timesteps | 1310720 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 136         |
|    iterations           | 321         |
|    time_elapsed         | 9637        |
|    total_timesteps      | 1314816     |
| train/                  |             |
|    approx_kl            | 0.003663969 |
|    clip_fraction        | 0.0236      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0153      |
|    n_updates            | 3200        |
|    policy_gradient_loss | -0.00129    |
|    std                  | 0.925       |
|    value_loss           | 0.558       |
-----------------------------------------
Eval num_timesteps=1315000, episode_reward=-264.74 +/- 124.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -265         |
| time/                   |              |
|    total_timesteps      | 1315000      |
| train/                  |              |
|    approx_kl            | 0.0033530667 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4           |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00721      |
|    n_updates            | 3210         |
|    policy_gradient_loss | -0.0017      |
|    std                  | 0.927        |
|    value_loss           | 0.722        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 136     |
|    iterations      | 322     |
|    time_elapsed    | 9651    |
|    total_timesteps | 1318912 |
--------------------------------
Eval num_timesteps=1320000, episode_reward=-183.36 +/- 118.62
Episode length: 814.80 +/- 372.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 815         |
|    mean_reward          | -183        |
| time/                   |             |
|    total_timesteps      | 1320000     |
| train/                  |             |
|    approx_kl            | 0.005035489 |
|    clip_fraction        | 0.0398      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0234      |
|    n_updates            | 3220        |
|    policy_gradient_loss | -0.00239    |
|    std                  | 0.931       |
|    value_loss           | 0.0724      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 136     |
|    iterations      | 323     |
|    time_elapsed    | 9662    |
|    total_timesteps | 1323008 |
--------------------------------
Eval num_timesteps=1325000, episode_reward=-191.72 +/- 166.42
Episode length: 625.00 +/- 460.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 625         |
|    mean_reward          | -192        |
| time/                   |             |
|    total_timesteps      | 1325000     |
| train/                  |             |
|    approx_kl            | 0.004796083 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.145       |
|    n_updates            | 3230        |
|    policy_gradient_loss | -0.00231    |
|    std                  | 0.927       |
|    value_loss           | 2.81        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 137     |
|    iterations      | 324     |
|    time_elapsed    | 9673    |
|    total_timesteps | 1327104 |
--------------------------------
Eval num_timesteps=1330000, episode_reward=-254.97 +/- 89.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -255         |
| time/                   |              |
|    total_timesteps      | 1330000      |
| train/                  |              |
|    approx_kl            | 0.0053089946 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.01        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.675        |
|    n_updates            | 3240         |
|    policy_gradient_loss | -0.00269     |
|    std                  | 0.928        |
|    value_loss           | 1.03         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 137     |
|    iterations      | 325     |
|    time_elapsed    | 9683    |
|    total_timesteps | 1331200 |
--------------------------------
Eval num_timesteps=1335000, episode_reward=-387.83 +/- 177.78
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -388        |
| time/                   |             |
|    total_timesteps      | 1335000     |
| train/                  |             |
|    approx_kl            | 0.004515567 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.01       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0426      |
|    n_updates            | 3250        |
|    policy_gradient_loss | -0.00315    |
|    std                  | 0.928       |
|    value_loss           | 0.385       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 137     |
|    iterations      | 326     |
|    time_elapsed    | 9695    |
|    total_timesteps | 1335296 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 327         |
|    time_elapsed         | 9702        |
|    total_timesteps      | 1339392     |
| train/                  |             |
|    approx_kl            | 0.004022465 |
|    clip_fraction        | 0.0257      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.99       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0406      |
|    n_updates            | 3260        |
|    policy_gradient_loss | -0.00146    |
|    std                  | 0.919       |
|    value_loss           | 0.319       |
-----------------------------------------
Eval num_timesteps=1340000, episode_reward=-270.51 +/- 196.72
Episode length: 817.40 +/- 367.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 817          |
|    mean_reward          | -271         |
| time/                   |              |
|    total_timesteps      | 1340000      |
| train/                  |              |
|    approx_kl            | 0.0036051606 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0167       |
|    n_updates            | 3270         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 0.914        |
|    value_loss           | 0.862        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 138     |
|    iterations      | 328     |
|    time_elapsed    | 9711    |
|    total_timesteps | 1343488 |
--------------------------------
Eval num_timesteps=1345000, episode_reward=-258.15 +/- 155.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -258         |
| time/                   |              |
|    total_timesteps      | 1345000      |
| train/                  |              |
|    approx_kl            | 0.0047042985 |
|    clip_fraction        | 0.0133       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 12           |
|    n_updates            | 3280         |
|    policy_gradient_loss | -0.000762    |
|    std                  | 0.915        |
|    value_loss           | 5.76         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 138     |
|    iterations      | 329     |
|    time_elapsed    | 9722    |
|    total_timesteps | 1347584 |
--------------------------------
Eval num_timesteps=1350000, episode_reward=-292.46 +/- 137.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -292        |
| time/                   |             |
|    total_timesteps      | 1350000     |
| train/                  |             |
|    approx_kl            | 0.005157751 |
|    clip_fraction        | 0.0239      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.034       |
|    n_updates            | 3290        |
|    policy_gradient_loss | -0.00139    |
|    std                  | 0.91        |
|    value_loss           | 0.2         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 138     |
|    iterations      | 330     |
|    time_elapsed    | 9731    |
|    total_timesteps | 1351680 |
--------------------------------
Eval num_timesteps=1355000, episode_reward=-283.17 +/- 72.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -283         |
| time/                   |              |
|    total_timesteps      | 1355000      |
| train/                  |              |
|    approx_kl            | 0.0061830045 |
|    clip_fraction        | 0.0338       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0277       |
|    n_updates            | 3300         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 0.91         |
|    value_loss           | 1.68         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 139     |
|    iterations      | 331     |
|    time_elapsed    | 9740    |
|    total_timesteps | 1355776 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 332         |
|    time_elapsed         | 9746        |
|    total_timesteps      | 1359872     |
| train/                  |             |
|    approx_kl            | 0.003776323 |
|    clip_fraction        | 0.0173      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0766      |
|    n_updates            | 3310        |
|    policy_gradient_loss | -0.000511   |
|    std                  | 0.911       |
|    value_loss           | 14.3        |
-----------------------------------------
Eval num_timesteps=1360000, episode_reward=-235.02 +/- 186.78
Episode length: 817.60 +/- 366.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 818          |
|    mean_reward          | -235         |
| time/                   |              |
|    total_timesteps      | 1360000      |
| train/                  |              |
|    approx_kl            | 0.0044677034 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00797      |
|    n_updates            | 3320         |
|    policy_gradient_loss | -0.00251     |
|    std                  | 0.908        |
|    value_loss           | 0.142        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 139     |
|    iterations      | 333     |
|    time_elapsed    | 9755    |
|    total_timesteps | 1363968 |
--------------------------------
Eval num_timesteps=1365000, episode_reward=-285.75 +/- 142.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 1365000      |
| train/                  |              |
|    approx_kl            | 0.0052890936 |
|    clip_fraction        | 0.0292       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.738        |
|    n_updates            | 3330         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.906        |
|    value_loss           | 15.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 140     |
|    iterations      | 334     |
|    time_elapsed    | 9766    |
|    total_timesteps | 1368064 |
--------------------------------
Eval num_timesteps=1370000, episode_reward=-325.93 +/- 174.75
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -326        |
| time/                   |             |
|    total_timesteps      | 1370000     |
| train/                  |             |
|    approx_kl            | 0.003842218 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0843      |
|    n_updates            | 3340        |
|    policy_gradient_loss | -0.00306    |
|    std                  | 0.908       |
|    value_loss           | 0.266       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 140     |
|    iterations      | 335     |
|    time_elapsed    | 9776    |
|    total_timesteps | 1372160 |
--------------------------------
Eval num_timesteps=1375000, episode_reward=-304.15 +/- 163.14
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -304         |
| time/                   |              |
|    total_timesteps      | 1375000      |
| train/                  |              |
|    approx_kl            | 0.0043525537 |
|    clip_fraction        | 0.0177       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.29         |
|    n_updates            | 3350         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.908        |
|    value_loss           | 12.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 140     |
|    iterations      | 336     |
|    time_elapsed    | 9787    |
|    total_timesteps | 1376256 |
--------------------------------
Eval num_timesteps=1380000, episode_reward=-331.24 +/- 175.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -331         |
| time/                   |              |
|    total_timesteps      | 1380000      |
| train/                  |              |
|    approx_kl            | 0.0070684943 |
|    clip_fraction        | 0.0598       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0573       |
|    n_updates            | 3360         |
|    policy_gradient_loss | -0.00345     |
|    std                  | 0.912        |
|    value_loss           | 0.0702       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 140     |
|    iterations      | 337     |
|    time_elapsed    | 9797    |
|    total_timesteps | 1380352 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 141          |
|    iterations           | 338          |
|    time_elapsed         | 9804         |
|    total_timesteps      | 1384448      |
| train/                  |              |
|    approx_kl            | 0.0030334857 |
|    clip_fraction        | 0.0051       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.579        |
|    n_updates            | 3370         |
|    policy_gradient_loss | -0.000981    |
|    std                  | 0.91         |
|    value_loss           | 3.02         |
------------------------------------------
Eval num_timesteps=1385000, episode_reward=-272.40 +/- 127.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -272        |
| time/                   |             |
|    total_timesteps      | 1385000     |
| train/                  |             |
|    approx_kl            | 0.006126133 |
|    clip_fraction        | 0.0486      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0268      |
|    n_updates            | 3380        |
|    policy_gradient_loss | -0.00384    |
|    std                  | 0.917       |
|    value_loss           | 0.0996      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 141     |
|    iterations      | 339     |
|    time_elapsed    | 9814    |
|    total_timesteps | 1388544 |
--------------------------------
Eval num_timesteps=1390000, episode_reward=-238.98 +/- 127.53
Episode length: 820.00 +/- 362.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 820          |
|    mean_reward          | -239         |
| time/                   |              |
|    total_timesteps      | 1390000      |
| train/                  |              |
|    approx_kl            | 0.0042166403 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.96         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.147        |
|    n_updates            | 3390         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 0.918        |
|    value_loss           | 2.48         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 141     |
|    iterations      | 340     |
|    time_elapsed    | 9824    |
|    total_timesteps | 1392640 |
--------------------------------
Eval num_timesteps=1395000, episode_reward=-227.09 +/- 175.38
Episode length: 826.40 +/- 349.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 826          |
|    mean_reward          | -227         |
| time/                   |              |
|    total_timesteps      | 1395000      |
| train/                  |              |
|    approx_kl            | 0.0034310129 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.97        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.424        |
|    n_updates            | 3400         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 0.917        |
|    value_loss           | 3.34         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 142     |
|    iterations      | 341     |
|    time_elapsed    | 9833    |
|    total_timesteps | 1396736 |
--------------------------------
Eval num_timesteps=1400000, episode_reward=-303.80 +/- 116.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -304        |
| time/                   |             |
|    total_timesteps      | 1400000     |
| train/                  |             |
|    approx_kl            | 0.006064278 |
|    clip_fraction        | 0.0533      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0188      |
|    n_updates            | 3410        |
|    policy_gradient_loss | -0.00517    |
|    std                  | 0.908       |
|    value_loss           | 0.0892      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 142     |
|    iterations      | 342     |
|    time_elapsed    | 9844    |
|    total_timesteps | 1400832 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 142         |
|    iterations           | 343         |
|    time_elapsed         | 9852        |
|    total_timesteps      | 1404928     |
| train/                  |             |
|    approx_kl            | 0.004273393 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.52        |
|    n_updates            | 3420        |
|    policy_gradient_loss | -0.000711   |
|    std                  | 0.906       |
|    value_loss           | 5.56        |
-----------------------------------------
Eval num_timesteps=1405000, episode_reward=-386.73 +/- 145.30
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -387        |
| time/                   |             |
|    total_timesteps      | 1405000     |
| train/                  |             |
|    approx_kl            | 0.004211331 |
|    clip_fraction        | 0.0219      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.111       |
|    n_updates            | 3430        |
|    policy_gradient_loss | -0.00126    |
|    std                  | 0.902       |
|    value_loss           | 0.641       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 142     |
|    iterations      | 344     |
|    time_elapsed    | 9862    |
|    total_timesteps | 1409024 |
--------------------------------
Eval num_timesteps=1410000, episode_reward=-246.22 +/- 161.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -246         |
| time/                   |              |
|    total_timesteps      | 1410000      |
| train/                  |              |
|    approx_kl            | 0.0027190142 |
|    clip_fraction        | 0.0132       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 3440         |
|    policy_gradient_loss | -0.000709    |
|    std                  | 0.9          |
|    value_loss           | 4.34         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 143     |
|    iterations      | 345     |
|    time_elapsed    | 9872    |
|    total_timesteps | 1413120 |
--------------------------------
Eval num_timesteps=1415000, episode_reward=-353.75 +/- 81.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -354         |
| time/                   |              |
|    total_timesteps      | 1415000      |
| train/                  |              |
|    approx_kl            | 0.0038254322 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.11         |
|    n_updates            | 3450         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.9          |
|    value_loss           | 5.14         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 143     |
|    iterations      | 346     |
|    time_elapsed    | 9884    |
|    total_timesteps | 1417216 |
--------------------------------
Eval num_timesteps=1420000, episode_reward=-232.54 +/- 134.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -233         |
| time/                   |              |
|    total_timesteps      | 1420000      |
| train/                  |              |
|    approx_kl            | 0.0021561333 |
|    clip_fraction        | 0.0215       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.058        |
|    n_updates            | 3460         |
|    policy_gradient_loss | -0.000454    |
|    std                  | 0.899        |
|    value_loss           | 3.91         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 143     |
|    iterations      | 347     |
|    time_elapsed    | 9893    |
|    total_timesteps | 1421312 |
--------------------------------
Eval num_timesteps=1425000, episode_reward=-374.08 +/- 144.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -374         |
| time/                   |              |
|    total_timesteps      | 1425000      |
| train/                  |              |
|    approx_kl            | 0.0049006827 |
|    clip_fraction        | 0.0249       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.29         |
|    n_updates            | 3470         |
|    policy_gradient_loss | -0.00206     |
|    std                  | 0.899        |
|    value_loss           | 3.01         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 143     |
|    iterations      | 348     |
|    time_elapsed    | 9903    |
|    total_timesteps | 1425408 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 144          |
|    iterations           | 349          |
|    time_elapsed         | 9909         |
|    total_timesteps      | 1429504      |
| train/                  |              |
|    approx_kl            | 0.0053761723 |
|    clip_fraction        | 0.0524       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00938      |
|    n_updates            | 3480         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 0.898        |
|    value_loss           | 0.0998       |
------------------------------------------
Eval num_timesteps=1430000, episode_reward=-371.20 +/- 227.55
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -371        |
| time/                   |             |
|    total_timesteps      | 1430000     |
| train/                  |             |
|    approx_kl            | 0.005939343 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0237      |
|    n_updates            | 3490        |
|    policy_gradient_loss | -0.00467    |
|    std                  | 0.902       |
|    value_loss           | 0.0752      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 144     |
|    iterations      | 350     |
|    time_elapsed    | 9918    |
|    total_timesteps | 1433600 |
--------------------------------
Eval num_timesteps=1435000, episode_reward=-469.88 +/- 156.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -470         |
| time/                   |              |
|    total_timesteps      | 1435000      |
| train/                  |              |
|    approx_kl            | 0.0035893514 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0911       |
|    n_updates            | 3500         |
|    policy_gradient_loss | -0.001       |
|    std                  | 0.899        |
|    value_loss           | 5.54         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 144     |
|    iterations      | 351     |
|    time_elapsed    | 9927    |
|    total_timesteps | 1437696 |
--------------------------------
Eval num_timesteps=1440000, episode_reward=-343.46 +/- 188.86
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -343         |
| time/                   |              |
|    total_timesteps      | 1440000      |
| train/                  |              |
|    approx_kl            | 0.0047289887 |
|    clip_fraction        | 0.0234       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0621       |
|    n_updates            | 3510         |
|    policy_gradient_loss | -0.00103     |
|    std                  | 0.896        |
|    value_loss           | 4.15         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 145     |
|    iterations      | 352     |
|    time_elapsed    | 9936    |
|    total_timesteps | 1441792 |
--------------------------------
Eval num_timesteps=1445000, episode_reward=-147.05 +/- 32.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 1445000      |
| train/                  |              |
|    approx_kl            | 0.0051678284 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0306       |
|    n_updates            | 3520         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 0.899        |
|    value_loss           | 0.355        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 145     |
|    iterations      | 353     |
|    time_elapsed    | 9945    |
|    total_timesteps | 1445888 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 145         |
|    iterations           | 354         |
|    time_elapsed         | 9952        |
|    total_timesteps      | 1449984     |
| train/                  |             |
|    approx_kl            | 0.004469866 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0246      |
|    n_updates            | 3530        |
|    policy_gradient_loss | -0.00277    |
|    std                  | 0.899       |
|    value_loss           | 0.345       |
-----------------------------------------
Eval num_timesteps=1450000, episode_reward=-335.51 +/- 195.27
Episode length: 809.20 +/- 383.60
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 809        |
|    mean_reward          | -336       |
| time/                   |            |
|    total_timesteps      | 1450000    |
| train/                  |            |
|    approx_kl            | 0.00667193 |
|    clip_fraction        | 0.0588     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.91      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.00333    |
|    n_updates            | 3540       |
|    policy_gradient_loss | -0.00464   |
|    std                  | 0.9        |
|    value_loss           | 0.0664     |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 145     |
|    iterations      | 355     |
|    time_elapsed    | 9961    |
|    total_timesteps | 1454080 |
--------------------------------
Eval num_timesteps=1455000, episode_reward=-318.91 +/- 124.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 1455000      |
| train/                  |              |
|    approx_kl            | 0.0044028517 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.15         |
|    n_updates            | 3550         |
|    policy_gradient_loss | -0.00185     |
|    std                  | 0.899        |
|    value_loss           | 0.828        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 146     |
|    iterations      | 356     |
|    time_elapsed    | 9971    |
|    total_timesteps | 1458176 |
--------------------------------
Eval num_timesteps=1460000, episode_reward=-256.98 +/- 97.03
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -257        |
| time/                   |             |
|    total_timesteps      | 1460000     |
| train/                  |             |
|    approx_kl            | 0.004023945 |
|    clip_fraction        | 0.0211      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.54        |
|    n_updates            | 3560        |
|    policy_gradient_loss | -0.00144    |
|    std                  | 0.897       |
|    value_loss           | 7.34        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 146     |
|    iterations      | 357     |
|    time_elapsed    | 9982    |
|    total_timesteps | 1462272 |
--------------------------------
Eval num_timesteps=1465000, episode_reward=-308.00 +/- 93.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -308         |
| time/                   |              |
|    total_timesteps      | 1465000      |
| train/                  |              |
|    approx_kl            | 0.0037414057 |
|    clip_fraction        | 0.0319       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0615       |
|    n_updates            | 3570         |
|    policy_gradient_loss | -0.00259     |
|    std                  | 0.898        |
|    value_loss           | 11.7         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 146     |
|    iterations      | 358     |
|    time_elapsed    | 9993    |
|    total_timesteps | 1466368 |
--------------------------------
Eval num_timesteps=1470000, episode_reward=-220.22 +/- 136.74
Episode length: 813.40 +/- 375.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -220         |
| time/                   |              |
|    total_timesteps      | 1470000      |
| train/                  |              |
|    approx_kl            | 0.0049248254 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.166        |
|    n_updates            | 3580         |
|    policy_gradient_loss | -0.00183     |
|    std                  | 0.9          |
|    value_loss           | 3.41         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 146     |
|    iterations      | 359     |
|    time_elapsed    | 10003   |
|    total_timesteps | 1470464 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 147         |
|    iterations           | 360         |
|    time_elapsed         | 10009       |
|    total_timesteps      | 1474560     |
| train/                  |             |
|    approx_kl            | 0.005812834 |
|    clip_fraction        | 0.0458      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0615      |
|    n_updates            | 3590        |
|    policy_gradient_loss | -0.0033     |
|    std                  | 0.898       |
|    value_loss           | 1.25        |
-----------------------------------------
Eval num_timesteps=1475000, episode_reward=-425.41 +/- 115.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -425         |
| time/                   |              |
|    total_timesteps      | 1475000      |
| train/                  |              |
|    approx_kl            | 0.0047737826 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.048        |
|    n_updates            | 3600         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 0.89         |
|    value_loss           | 0.608        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 147     |
|    iterations      | 361     |
|    time_elapsed    | 10019   |
|    total_timesteps | 1478656 |
--------------------------------
Eval num_timesteps=1480000, episode_reward=-278.16 +/- 209.89
Episode length: 801.00 +/- 400.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 801          |
|    mean_reward          | -278         |
| time/                   |              |
|    total_timesteps      | 1480000      |
| train/                  |              |
|    approx_kl            | 0.0062025385 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.55         |
|    n_updates            | 3610         |
|    policy_gradient_loss | -0.00281     |
|    std                  | 0.888        |
|    value_loss           | 5.59         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 147     |
|    iterations      | 362     |
|    time_elapsed    | 10028   |
|    total_timesteps | 1482752 |
--------------------------------
Eval num_timesteps=1485000, episode_reward=-335.98 +/- 195.61
Episode length: 816.80 +/- 368.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 817          |
|    mean_reward          | -336         |
| time/                   |              |
|    total_timesteps      | 1485000      |
| train/                  |              |
|    approx_kl            | 0.0050408198 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0325       |
|    n_updates            | 3620         |
|    policy_gradient_loss | -0.00341     |
|    std                  | 0.891        |
|    value_loss           | 8.82         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 148     |
|    iterations      | 363     |
|    time_elapsed    | 10039   |
|    total_timesteps | 1486848 |
--------------------------------
Eval num_timesteps=1490000, episode_reward=-288.76 +/- 175.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -289         |
| time/                   |              |
|    total_timesteps      | 1490000      |
| train/                  |              |
|    approx_kl            | 0.0066019353 |
|    clip_fraction        | 0.0501       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.265        |
|    n_updates            | 3630         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 0.888        |
|    value_loss           | 2            |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 148     |
|    iterations      | 364     |
|    time_elapsed    | 10049   |
|    total_timesteps | 1490944 |
--------------------------------
Eval num_timesteps=1495000, episode_reward=-306.14 +/- 134.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -306         |
| time/                   |              |
|    total_timesteps      | 1495000      |
| train/                  |              |
|    approx_kl            | 0.0073708165 |
|    clip_fraction        | 0.0809       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00439      |
|    n_updates            | 3640         |
|    policy_gradient_loss | -0.00376     |
|    std                  | 0.89         |
|    value_loss           | 0.0682       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 148     |
|    iterations      | 365     |
|    time_elapsed    | 10060   |
|    total_timesteps | 1495040 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 148         |
|    iterations           | 366         |
|    time_elapsed         | 10068       |
|    total_timesteps      | 1499136     |
| train/                  |             |
|    approx_kl            | 0.006276736 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0839      |
|    n_updates            | 3650        |
|    policy_gradient_loss | -0.000744   |
|    std                  | 0.891       |
|    value_loss           | 12.3        |
-----------------------------------------
Eval num_timesteps=1500000, episode_reward=-237.72 +/- 132.99
Episode length: 814.60 +/- 372.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -238         |
| time/                   |              |
|    total_timesteps      | 1500000      |
| train/                  |              |
|    approx_kl            | 0.0050789937 |
|    clip_fraction        | 0.0405       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.89        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0175       |
|    n_updates            | 3660         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 0.894        |
|    value_loss           | 0.108        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 149     |
|    iterations      | 367     |
|    time_elapsed    | 10077   |
|    total_timesteps | 1503232 |
--------------------------------
Eval num_timesteps=1505000, episode_reward=-338.45 +/- 150.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -338        |
| time/                   |             |
|    total_timesteps      | 1505000     |
| train/                  |             |
|    approx_kl            | 0.004658509 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.041       |
|    n_updates            | 3670        |
|    policy_gradient_loss | -0.0013     |
|    std                  | 0.892       |
|    value_loss           | 0.578       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 149     |
|    iterations      | 368     |
|    time_elapsed    | 10089   |
|    total_timesteps | 1507328 |
--------------------------------
Eval num_timesteps=1510000, episode_reward=-484.80 +/- 119.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -485         |
| time/                   |              |
|    total_timesteps      | 1510000      |
| train/                  |              |
|    approx_kl            | 0.0020472198 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.89        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0206       |
|    n_updates            | 3680         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 0.894        |
|    value_loss           | 1.4          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 149     |
|    iterations      | 369     |
|    time_elapsed    | 10100   |
|    total_timesteps | 1511424 |
--------------------------------
Eval num_timesteps=1515000, episode_reward=-383.77 +/- 127.38
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -384        |
| time/                   |             |
|    total_timesteps      | 1515000     |
| train/                  |             |
|    approx_kl            | 0.004696285 |
|    clip_fraction        | 0.0377      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 11.5        |
|    n_updates            | 3690        |
|    policy_gradient_loss | -0.00211    |
|    std                  | 0.893       |
|    value_loss           | 5.33        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 149     |
|    iterations      | 370     |
|    time_elapsed    | 10109   |
|    total_timesteps | 1515520 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 150         |
|    iterations           | 371         |
|    time_elapsed         | 10118       |
|    total_timesteps      | 1519616     |
| train/                  |             |
|    approx_kl            | 0.006354424 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.105       |
|    n_updates            | 3700        |
|    policy_gradient_loss | -0.00251    |
|    std                  | 0.893       |
|    value_loss           | 1.87        |
-----------------------------------------
Eval num_timesteps=1520000, episode_reward=-293.38 +/- 59.57
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -293        |
| time/                   |             |
|    total_timesteps      | 1520000     |
| train/                  |             |
|    approx_kl            | 0.004461012 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0506      |
|    n_updates            | 3710        |
|    policy_gradient_loss | -0.00179    |
|    std                  | 0.894       |
|    value_loss           | 0.0994      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 150     |
|    iterations      | 372     |
|    time_elapsed    | 10127   |
|    total_timesteps | 1523712 |
--------------------------------
Eval num_timesteps=1525000, episode_reward=-335.52 +/- 122.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -336         |
| time/                   |              |
|    total_timesteps      | 1525000      |
| train/                  |              |
|    approx_kl            | 0.0037406282 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0177       |
|    n_updates            | 3720         |
|    policy_gradient_loss | -0.000742    |
|    std                  | 0.889        |
|    value_loss           | 0.136        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 150     |
|    iterations      | 373     |
|    time_elapsed    | 10139   |
|    total_timesteps | 1527808 |
--------------------------------
Eval num_timesteps=1530000, episode_reward=-284.56 +/- 70.68
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -285         |
| time/                   |              |
|    total_timesteps      | 1530000      |
| train/                  |              |
|    approx_kl            | 0.0041311528 |
|    clip_fraction        | 0.0245       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.87        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0717       |
|    n_updates            | 3730         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 0.889        |
|    value_loss           | 3.67         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 150     |
|    iterations      | 374     |
|    time_elapsed    | 10151   |
|    total_timesteps | 1531904 |
--------------------------------
Eval num_timesteps=1535000, episode_reward=-372.91 +/- 164.94
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -373        |
| time/                   |             |
|    total_timesteps      | 1535000     |
| train/                  |             |
|    approx_kl            | 0.005901221 |
|    clip_fraction        | 0.0345      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0921      |
|    n_updates            | 3740        |
|    policy_gradient_loss | -0.00133    |
|    std                  | 0.888       |
|    value_loss           | 4.77        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 151     |
|    iterations      | 375     |
|    time_elapsed    | 10164   |
|    total_timesteps | 1536000 |
--------------------------------
Eval num_timesteps=1540000, episode_reward=-416.21 +/- 174.30
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 1540000     |
| train/                  |             |
|    approx_kl            | 0.005480015 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0971      |
|    n_updates            | 3750        |
|    policy_gradient_loss | -0.00221    |
|    std                  | 0.89        |
|    value_loss           | 12          |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 151     |
|    iterations      | 376     |
|    time_elapsed    | 10175   |
|    total_timesteps | 1540096 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 377         |
|    time_elapsed         | 10182       |
|    total_timesteps      | 1544192     |
| train/                  |             |
|    approx_kl            | 0.006073367 |
|    clip_fraction        | 0.0515      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0232      |
|    n_updates            | 3760        |
|    policy_gradient_loss | -0.00498    |
|    std                  | 0.887       |
|    value_loss           | 0.141       |
-----------------------------------------
Eval num_timesteps=1545000, episode_reward=-409.04 +/- 157.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -409        |
| time/                   |             |
|    total_timesteps      | 1545000     |
| train/                  |             |
|    approx_kl            | 0.004054978 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0151      |
|    n_updates            | 3770        |
|    policy_gradient_loss | -0.000681   |
|    std                  | 0.882       |
|    value_loss           | 0.0633      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 151     |
|    iterations      | 378     |
|    time_elapsed    | 10192   |
|    total_timesteps | 1548288 |
--------------------------------
Eval num_timesteps=1550000, episode_reward=-168.03 +/- 172.77
Episode length: 814.60 +/- 372.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 815         |
|    mean_reward          | -168        |
| time/                   |             |
|    total_timesteps      | 1550000     |
| train/                  |             |
|    approx_kl            | 0.004384134 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.13        |
|    n_updates            | 3780        |
|    policy_gradient_loss | -0.00125    |
|    std                  | 0.881       |
|    value_loss           | 12.7        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 152     |
|    iterations      | 379     |
|    time_elapsed    | 10202   |
|    total_timesteps | 1552384 |
--------------------------------
Eval num_timesteps=1555000, episode_reward=-354.59 +/- 170.21
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -355        |
| time/                   |             |
|    total_timesteps      | 1555000     |
| train/                  |             |
|    approx_kl            | 0.004068774 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 17          |
|    n_updates            | 3790        |
|    policy_gradient_loss | -0.00191    |
|    std                  | 0.883       |
|    value_loss           | 9.74        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 152     |
|    iterations      | 380     |
|    time_elapsed    | 10211   |
|    total_timesteps | 1556480 |
--------------------------------
Eval num_timesteps=1560000, episode_reward=-376.81 +/- 192.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -377         |
| time/                   |              |
|    total_timesteps      | 1560000      |
| train/                  |              |
|    approx_kl            | 0.0045547597 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0389       |
|    n_updates            | 3800         |
|    policy_gradient_loss | -0.00235     |
|    std                  | 0.884        |
|    value_loss           | 0.261        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 152     |
|    iterations      | 381     |
|    time_elapsed    | 10222   |
|    total_timesteps | 1560576 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 382         |
|    time_elapsed         | 10229       |
|    total_timesteps      | 1564672     |
| train/                  |             |
|    approx_kl            | 0.004693414 |
|    clip_fraction        | 0.0384      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0126      |
|    n_updates            | 3810        |
|    policy_gradient_loss | -0.00388    |
|    std                  | 0.884       |
|    value_loss           | 0.174       |
-----------------------------------------
Eval num_timesteps=1565000, episode_reward=-182.36 +/- 190.03
Episode length: 624.20 +/- 461.50
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 624          |
|    mean_reward          | -182         |
| time/                   |              |
|    total_timesteps      | 1565000      |
| train/                  |              |
|    approx_kl            | 0.0049601663 |
|    clip_fraction        | 0.0435       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.86        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0121       |
|    n_updates            | 3820         |
|    policy_gradient_loss | -0.00288     |
|    std                  | 0.89         |
|    value_loss           | 0.201        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 153     |
|    iterations      | 383     |
|    time_elapsed    | 10238   |
|    total_timesteps | 1568768 |
--------------------------------
Eval num_timesteps=1570000, episode_reward=-359.18 +/- 258.06
Episode length: 817.00 +/- 368.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 817       |
|    mean_reward          | -359      |
| time/                   |           |
|    total_timesteps      | 1570000   |
| train/                  |           |
|    approx_kl            | 0.0047392 |
|    clip_fraction        | 0.0331    |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.87     |
|    explained_variance   | 1         |
|    learning_rate        | 0.0001    |
|    loss                 | 0.00105   |
|    n_updates            | 3830      |
|    policy_gradient_loss | -0.00153  |
|    std                  | 0.89      |
|    value_loss           | 0.063     |
---------------------------------------
--------------------------------
| time/              |         |
|    fps             | 153     |
|    iterations      | 384     |
|    time_elapsed    | 10248   |
|    total_timesteps | 1572864 |
--------------------------------
Eval num_timesteps=1575000, episode_reward=-100.79 +/- 54.42
Episode length: 811.20 +/- 379.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -101         |
| time/                   |              |
|    total_timesteps      | 1575000      |
| train/                  |              |
|    approx_kl            | 0.0034794551 |
|    clip_fraction        | 0.0171       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.87        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.413        |
|    n_updates            | 3840         |
|    policy_gradient_loss | -0.00146     |
|    std                  | 0.89         |
|    value_loss           | 2.31         |
------------------------------------------
New best mean reward!
Eval num_timesteps=1580000, episode_reward=-254.45 +/- 170.91
Episode length: 814.00 +/- 374.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | -254        |
| time/                   |             |
|    total_timesteps      | 1580000     |
| train/                  |             |
|    approx_kl            | 0.005234223 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.305       |
|    n_updates            | 3850        |
|    policy_gradient_loss | -0.00283    |
|    std                  | 0.9         |
|    value_loss           | 0.404       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 153     |
|    iterations      | 386     |
|    time_elapsed    | 10270   |
|    total_timesteps | 1581056 |
--------------------------------
Eval num_timesteps=1585000, episode_reward=-256.97 +/- 164.97
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -257        |
| time/                   |             |
|    total_timesteps      | 1585000     |
| train/                  |             |
|    approx_kl            | 0.004894359 |
|    clip_fraction        | 0.0306      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.064       |
|    n_updates            | 3860        |
|    policy_gradient_loss | -0.00322    |
|    std                  | 0.9         |
|    value_loss           | 0.739       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 154     |
|    iterations      | 387     |
|    time_elapsed    | 10281   |
|    total_timesteps | 1585152 |
--------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 388        |
|    time_elapsed         | 10290      |
|    total_timesteps      | 1589248    |
| train/                  |            |
|    approx_kl            | 0.00645304 |
|    clip_fraction        | 0.0437     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.89      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0001     |
|    loss                 | 1.2        |
|    n_updates            | 3870       |
|    policy_gradient_loss | -0.00148   |
|    std                  | 0.899      |
|    value_loss           | 0.975      |
----------------------------------------
Eval num_timesteps=1590000, episode_reward=-397.61 +/- 83.70
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -398         |
| time/                   |              |
|    total_timesteps      | 1590000      |
| train/                  |              |
|    approx_kl            | 0.0044268314 |
|    clip_fraction        | 0.0374       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00139     |
|    n_updates            | 3880         |
|    policy_gradient_loss | -0.00213     |
|    std                  | 0.909        |
|    value_loss           | 0.0421       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 154     |
|    iterations      | 389     |
|    time_elapsed    | 10299   |
|    total_timesteps | 1593344 |
--------------------------------
Eval num_timesteps=1595000, episode_reward=-288.39 +/- 153.08
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -288        |
| time/                   |             |
|    total_timesteps      | 1595000     |
| train/                  |             |
|    approx_kl            | 0.004662233 |
|    clip_fraction        | 0.0206      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.93        |
|    n_updates            | 3890        |
|    policy_gradient_loss | -0.0024     |
|    std                  | 0.911       |
|    value_loss           | 12.1        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 154     |
|    iterations      | 390     |
|    time_elapsed    | 10308   |
|    total_timesteps | 1597440 |
--------------------------------
Eval num_timesteps=1600000, episode_reward=-290.28 +/- 102.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -290         |
| time/                   |              |
|    total_timesteps      | 1600000      |
| train/                  |              |
|    approx_kl            | 0.0045652725 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 8.96         |
|    n_updates            | 3900         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 0.912        |
|    value_loss           | 4.09         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 155     |
|    iterations      | 391     |
|    time_elapsed    | 10319   |
|    total_timesteps | 1601536 |
--------------------------------
Eval num_timesteps=1605000, episode_reward=-394.94 +/- 196.64
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -395        |
| time/                   |             |
|    total_timesteps      | 1605000     |
| train/                  |             |
|    approx_kl            | 0.004516422 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.81        |
|    n_updates            | 3910        |
|    policy_gradient_loss | -0.002      |
|    std                  | 0.911       |
|    value_loss           | 3.53        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 155     |
|    iterations      | 392     |
|    time_elapsed    | 10329   |
|    total_timesteps | 1605632 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 155          |
|    iterations           | 393          |
|    time_elapsed         | 10336        |
|    total_timesteps      | 1609728      |
| train/                  |              |
|    approx_kl            | 0.0053262888 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0683       |
|    n_updates            | 3920         |
|    policy_gradient_loss | -0.0046      |
|    std                  | 0.911        |
|    value_loss           | 3.78         |
------------------------------------------
Eval num_timesteps=1610000, episode_reward=-383.82 +/- 138.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -384         |
| time/                   |              |
|    total_timesteps      | 1610000      |
| train/                  |              |
|    approx_kl            | 0.0053021885 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0544       |
|    n_updates            | 3930         |
|    policy_gradient_loss | -0.00275     |
|    std                  | 0.911        |
|    value_loss           | 0.11         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 155     |
|    iterations      | 394     |
|    time_elapsed    | 10346   |
|    total_timesteps | 1613824 |
--------------------------------
Eval num_timesteps=1615000, episode_reward=-330.78 +/- 167.16
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -331        |
| time/                   |             |
|    total_timesteps      | 1615000     |
| train/                  |             |
|    approx_kl            | 0.005661082 |
|    clip_fraction        | 0.0448      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.2         |
|    n_updates            | 3940        |
|    policy_gradient_loss | -0.00373    |
|    std                  | 0.915       |
|    value_loss           | 7.94        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 156     |
|    iterations      | 395     |
|    time_elapsed    | 10359   |
|    total_timesteps | 1617920 |
--------------------------------
Eval num_timesteps=1620000, episode_reward=-358.15 +/- 187.18
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -358        |
| time/                   |             |
|    total_timesteps      | 1620000     |
| train/                  |             |
|    approx_kl            | 0.006271962 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 13.7        |
|    n_updates            | 3950        |
|    policy_gradient_loss | -0.00409    |
|    std                  | 0.91        |
|    value_loss           | 3.01        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 156     |
|    iterations      | 396     |
|    time_elapsed    | 10372   |
|    total_timesteps | 1622016 |
--------------------------------
Eval num_timesteps=1625000, episode_reward=-376.44 +/- 181.31
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -376        |
| time/                   |             |
|    total_timesteps      | 1625000     |
| train/                  |             |
|    approx_kl            | 0.005634343 |
|    clip_fraction        | 0.0397      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0159      |
|    n_updates            | 3960        |
|    policy_gradient_loss | -0.00238    |
|    std                  | 0.907       |
|    value_loss           | 0.181       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 156     |
|    iterations      | 397     |
|    time_elapsed    | 10387   |
|    total_timesteps | 1626112 |
--------------------------------
Eval num_timesteps=1630000, episode_reward=-262.10 +/- 116.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -262         |
| time/                   |              |
|    total_timesteps      | 1630000      |
| train/                  |              |
|    approx_kl            | 0.0046737166 |
|    clip_fraction        | 0.0175       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.043        |
|    n_updates            | 3970         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.908        |
|    value_loss           | 1.66         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 156     |
|    iterations      | 398     |
|    time_elapsed    | 10397   |
|    total_timesteps | 1630208 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 157          |
|    iterations           | 399          |
|    time_elapsed         | 10403        |
|    total_timesteps      | 1634304      |
| train/                  |              |
|    approx_kl            | 0.0039030528 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0172       |
|    n_updates            | 3980         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 0.904        |
|    value_loss           | 0.225        |
------------------------------------------
Eval num_timesteps=1635000, episode_reward=-225.48 +/- 99.38
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -225        |
| time/                   |             |
|    total_timesteps      | 1635000     |
| train/                  |             |
|    approx_kl            | 0.005912126 |
|    clip_fraction        | 0.0399      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0599      |
|    n_updates            | 3990        |
|    policy_gradient_loss | -0.00156    |
|    std                  | 0.906       |
|    value_loss           | 0.0982      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 157     |
|    iterations      | 400     |
|    time_elapsed    | 10413   |
|    total_timesteps | 1638400 |
--------------------------------
Eval num_timesteps=1640000, episode_reward=-259.04 +/- 144.16
Episode length: 812.80 +/- 376.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 813         |
|    mean_reward          | -259        |
| time/                   |             |
|    total_timesteps      | 1640000     |
| train/                  |             |
|    approx_kl            | 0.004754409 |
|    clip_fraction        | 0.0232      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.93        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.685       |
|    n_updates            | 4000        |
|    policy_gradient_loss | -0.00221    |
|    std                  | 0.903       |
|    value_loss           | 1.97        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 157     |
|    iterations      | 401     |
|    time_elapsed    | 10422   |
|    total_timesteps | 1642496 |
--------------------------------
Eval num_timesteps=1645000, episode_reward=-250.33 +/- 154.47
Episode length: 814.00 +/- 374.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | -250        |
| time/                   |             |
|    total_timesteps      | 1645000     |
| train/                  |             |
|    approx_kl            | 0.004485877 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0661      |
|    n_updates            | 4010        |
|    policy_gradient_loss | -0.00317    |
|    std                  | 0.901       |
|    value_loss           | 2.62        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 157     |
|    iterations      | 402     |
|    time_elapsed    | 10432   |
|    total_timesteps | 1646592 |
--------------------------------
Eval num_timesteps=1650000, episode_reward=-251.40 +/- 123.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -251         |
| time/                   |              |
|    total_timesteps      | 1650000      |
| train/                  |              |
|    approx_kl            | 0.0036033345 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.808        |
|    n_updates            | 4020         |
|    policy_gradient_loss | -0.00236     |
|    std                  | 0.901        |
|    value_loss           | 5.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 158     |
|    iterations      | 403     |
|    time_elapsed    | 10443   |
|    total_timesteps | 1650688 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 158          |
|    iterations           | 404          |
|    time_elapsed         | 10452        |
|    total_timesteps      | 1654784      |
| train/                  |              |
|    approx_kl            | 0.0062194155 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0329       |
|    n_updates            | 4030         |
|    policy_gradient_loss | -0.00109     |
|    std                  | 0.9          |
|    value_loss           | 1.12         |
------------------------------------------
Eval num_timesteps=1655000, episode_reward=-268.78 +/- 124.33
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -269         |
| time/                   |              |
|    total_timesteps      | 1655000      |
| train/                  |              |
|    approx_kl            | 0.0072415685 |
|    clip_fraction        | 0.0707       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.89        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0206       |
|    n_updates            | 4040         |
|    policy_gradient_loss | -0.00334     |
|    std                  | 0.9          |
|    value_loss           | 0.0964       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 158     |
|    iterations      | 405     |
|    time_elapsed    | 10461   |
|    total_timesteps | 1658880 |
--------------------------------
Eval num_timesteps=1660000, episode_reward=-285.03 +/- 192.51
Episode length: 814.80 +/- 372.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -285         |
| time/                   |              |
|    total_timesteps      | 1660000      |
| train/                  |              |
|    approx_kl            | 0.0037554908 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0917       |
|    n_updates            | 4050         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 0.905        |
|    value_loss           | 0.511        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 158     |
|    iterations      | 406     |
|    time_elapsed    | 10471   |
|    total_timesteps | 1662976 |
--------------------------------
Eval num_timesteps=1665000, episode_reward=-252.93 +/- 177.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -253        |
| time/                   |             |
|    total_timesteps      | 1665000     |
| train/                  |             |
|    approx_kl            | 0.004498551 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.254       |
|    n_updates            | 4060        |
|    policy_gradient_loss | -0.00178    |
|    std                  | 0.902       |
|    value_loss           | 1.07        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 159     |
|    iterations      | 407     |
|    time_elapsed    | 10482   |
|    total_timesteps | 1667072 |
--------------------------------
Eval num_timesteps=1670000, episode_reward=-289.09 +/- 204.61
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -289        |
| time/                   |             |
|    total_timesteps      | 1670000     |
| train/                  |             |
|    approx_kl            | 0.004369306 |
|    clip_fraction        | 0.0223      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 7.78        |
|    n_updates            | 4070        |
|    policy_gradient_loss | -0.00133    |
|    std                  | 0.903       |
|    value_loss           | 1.74        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 159     |
|    iterations      | 408     |
|    time_elapsed    | 10492   |
|    total_timesteps | 1671168 |
--------------------------------
Eval num_timesteps=1675000, episode_reward=-440.31 +/- 125.51
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -440        |
| time/                   |             |
|    total_timesteps      | 1675000     |
| train/                  |             |
|    approx_kl            | 0.004686373 |
|    clip_fraction        | 0.0328      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00931     |
|    n_updates            | 4080        |
|    policy_gradient_loss | -0.000861   |
|    std                  | 0.906       |
|    value_loss           | 0.112       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 159     |
|    iterations      | 409     |
|    time_elapsed    | 10503   |
|    total_timesteps | 1675264 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 159         |
|    iterations           | 410         |
|    time_elapsed         | 10509       |
|    total_timesteps      | 1679360     |
| train/                  |             |
|    approx_kl            | 0.004785642 |
|    clip_fraction        | 0.0367      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.5         |
|    n_updates            | 4090        |
|    policy_gradient_loss | -0.001      |
|    std                  | 0.905       |
|    value_loss           | 7.85        |
-----------------------------------------
Eval num_timesteps=1680000, episode_reward=-239.87 +/- 171.32
Episode length: 808.40 +/- 385.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -240         |
| time/                   |              |
|    total_timesteps      | 1680000      |
| train/                  |              |
|    approx_kl            | 0.0040096557 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.000993    |
|    n_updates            | 4100         |
|    policy_gradient_loss | -0.00198     |
|    std                  | 0.915        |
|    value_loss           | 0.0531       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 160     |
|    iterations      | 411     |
|    time_elapsed    | 10521   |
|    total_timesteps | 1683456 |
--------------------------------
Eval num_timesteps=1685000, episode_reward=-276.61 +/- 90.97
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -277         |
| time/                   |              |
|    total_timesteps      | 1685000      |
| train/                  |              |
|    approx_kl            | 0.0045233658 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.148        |
|    n_updates            | 4110         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.915        |
|    value_loss           | 8.67         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 160     |
|    iterations      | 412     |
|    time_elapsed    | 10535   |
|    total_timesteps | 1687552 |
--------------------------------
Eval num_timesteps=1690000, episode_reward=-242.91 +/- 79.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -243         |
| time/                   |              |
|    total_timesteps      | 1690000      |
| train/                  |              |
|    approx_kl            | 0.0034460672 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.149        |
|    n_updates            | 4120         |
|    policy_gradient_loss | -0.00255     |
|    std                  | 0.915        |
|    value_loss           | 3.59         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 160     |
|    iterations      | 413     |
|    time_elapsed    | 10546   |
|    total_timesteps | 1691648 |
--------------------------------
Eval num_timesteps=1695000, episode_reward=-288.51 +/- 188.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -289         |
| time/                   |              |
|    total_timesteps      | 1695000      |
| train/                  |              |
|    approx_kl            | 0.0044047376 |
|    clip_fraction        | 0.0263       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.33         |
|    n_updates            | 4130         |
|    policy_gradient_loss | -0.00156     |
|    std                  | 0.912        |
|    value_loss           | 0.647        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 160     |
|    iterations      | 414     |
|    time_elapsed    | 10556   |
|    total_timesteps | 1695744 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 160         |
|    iterations           | 415         |
|    time_elapsed         | 10563       |
|    total_timesteps      | 1699840     |
| train/                  |             |
|    approx_kl            | 0.004932621 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0308      |
|    n_updates            | 4140        |
|    policy_gradient_loss | -0.00398    |
|    std                  | 0.909       |
|    value_loss           | 0.501       |
-----------------------------------------
Eval num_timesteps=1700000, episode_reward=-290.25 +/- 63.14
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -290        |
| time/                   |             |
|    total_timesteps      | 1700000     |
| train/                  |             |
|    approx_kl            | 0.005356416 |
|    clip_fraction        | 0.0389      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00728     |
|    n_updates            | 4150        |
|    policy_gradient_loss | -0.00268    |
|    std                  | 0.917       |
|    value_loss           | 0.137       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 161     |
|    iterations      | 416     |
|    time_elapsed    | 10575   |
|    total_timesteps | 1703936 |
--------------------------------
Eval num_timesteps=1705000, episode_reward=-259.59 +/- 150.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -260         |
| time/                   |              |
|    total_timesteps      | 1705000      |
| train/                  |              |
|    approx_kl            | 0.0040052906 |
|    clip_fraction        | 0.0315       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.334        |
|    n_updates            | 4160         |
|    policy_gradient_loss | -0.00291     |
|    std                  | 0.917        |
|    value_loss           | 0.752        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 161     |
|    iterations      | 417     |
|    time_elapsed    | 10586   |
|    total_timesteps | 1708032 |
--------------------------------
Eval num_timesteps=1710000, episode_reward=-349.10 +/- 200.35
Episode length: 812.80 +/- 376.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -349         |
| time/                   |              |
|    total_timesteps      | 1710000      |
| train/                  |              |
|    approx_kl            | 0.0040874546 |
|    clip_fraction        | 0.0259       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0966       |
|    n_updates            | 4170         |
|    policy_gradient_loss | -0.000907    |
|    std                  | 0.917        |
|    value_loss           | 5.57         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 161     |
|    iterations      | 418     |
|    time_elapsed    | 10598   |
|    total_timesteps | 1712128 |
--------------------------------
Eval num_timesteps=1715000, episode_reward=-214.73 +/- 128.18
Episode length: 814.40 +/- 373.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -215         |
| time/                   |              |
|    total_timesteps      | 1715000      |
| train/                  |              |
|    approx_kl            | 0.0055232523 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.69         |
|    n_updates            | 4180         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 0.917        |
|    value_loss           | 4.72         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 161     |
|    iterations      | 419     |
|    time_elapsed    | 10610   |
|    total_timesteps | 1716224 |
--------------------------------
Eval num_timesteps=1720000, episode_reward=-238.11 +/- 147.22
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -238        |
| time/                   |             |
|    total_timesteps      | 1720000     |
| train/                  |             |
|    approx_kl            | 0.005264423 |
|    clip_fraction        | 0.0401      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.999       |
|    n_updates            | 4190        |
|    policy_gradient_loss | -0.00244    |
|    std                  | 0.92        |
|    value_loss           | 4.63        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 161     |
|    iterations      | 420     |
|    time_elapsed    | 10620   |
|    total_timesteps | 1720320 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 162          |
|    iterations           | 421          |
|    time_elapsed         | 10626        |
|    total_timesteps      | 1724416      |
| train/                  |              |
|    approx_kl            | 0.0050039664 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.323        |
|    n_updates            | 4200         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 0.924        |
|    value_loss           | 0.257        |
------------------------------------------
Eval num_timesteps=1725000, episode_reward=-263.29 +/- 115.75
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -263        |
| time/                   |             |
|    total_timesteps      | 1725000     |
| train/                  |             |
|    approx_kl            | 0.005741899 |
|    clip_fraction        | 0.0456      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0203      |
|    n_updates            | 4210        |
|    policy_gradient_loss | -0.00227    |
|    std                  | 0.93        |
|    value_loss           | 0.0585      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 162     |
|    iterations      | 422     |
|    time_elapsed    | 10637   |
|    total_timesteps | 1728512 |
--------------------------------
Eval num_timesteps=1730000, episode_reward=-341.95 +/- 138.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -342         |
| time/                   |              |
|    total_timesteps      | 1730000      |
| train/                  |              |
|    approx_kl            | 0.0061816676 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 15.6         |
|    n_updates            | 4220         |
|    policy_gradient_loss | -0.00186     |
|    std                  | 0.93         |
|    value_loss           | 5.37         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 162     |
|    iterations      | 423     |
|    time_elapsed    | 10647   |
|    total_timesteps | 1732608 |
--------------------------------
Eval num_timesteps=1735000, episode_reward=-287.18 +/- 159.10
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -287         |
| time/                   |              |
|    total_timesteps      | 1735000      |
| train/                  |              |
|    approx_kl            | 0.0047734645 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.98        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.76         |
|    n_updates            | 4230         |
|    policy_gradient_loss | -0.00265     |
|    std                  | 0.929        |
|    value_loss           | 2.7          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 162     |
|    iterations      | 424     |
|    time_elapsed    | 10658   |
|    total_timesteps | 1736704 |
--------------------------------
Eval num_timesteps=1740000, episode_reward=-365.78 +/- 141.33
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -366        |
| time/                   |             |
|    total_timesteps      | 1740000     |
| train/                  |             |
|    approx_kl            | 0.004620248 |
|    clip_fraction        | 0.0439      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.97       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.15        |
|    n_updates            | 4240        |
|    policy_gradient_loss | -0.00261    |
|    std                  | 0.924       |
|    value_loss           | 0.167       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 163     |
|    iterations      | 425     |
|    time_elapsed    | 10669   |
|    total_timesteps | 1740800 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 163          |
|    iterations           | 426          |
|    time_elapsed         | 10676        |
|    total_timesteps      | 1744896      |
| train/                  |              |
|    approx_kl            | 0.0047533857 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.01         |
|    n_updates            | 4250         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.919        |
|    value_loss           | 3.21         |
------------------------------------------
Eval num_timesteps=1745000, episode_reward=-338.40 +/- 177.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -338         |
| time/                   |              |
|    total_timesteps      | 1745000      |
| train/                  |              |
|    approx_kl            | 0.0041200565 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00913      |
|    n_updates            | 4260         |
|    policy_gradient_loss | -0.00102     |
|    std                  | 0.917        |
|    value_loss           | 0.137        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 163     |
|    iterations      | 427     |
|    time_elapsed    | 10687   |
|    total_timesteps | 1748992 |
--------------------------------
Eval num_timesteps=1750000, episode_reward=-251.95 +/- 144.92
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -252        |
| time/                   |             |
|    total_timesteps      | 1750000     |
| train/                  |             |
|    approx_kl            | 0.005677893 |
|    clip_fraction        | 0.0398      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0152      |
|    n_updates            | 4270        |
|    policy_gradient_loss | -0.00274    |
|    std                  | 0.915       |
|    value_loss           | 0.158       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 163     |
|    iterations      | 428     |
|    time_elapsed    | 10698   |
|    total_timesteps | 1753088 |
--------------------------------
Eval num_timesteps=1755000, episode_reward=-428.08 +/- 118.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -428         |
| time/                   |              |
|    total_timesteps      | 1755000      |
| train/                  |              |
|    approx_kl            | 0.0061362344 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.015        |
|    n_updates            | 4280         |
|    policy_gradient_loss | -0.00339     |
|    std                  | 0.914        |
|    value_loss           | 4.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 164     |
|    iterations      | 429     |
|    time_elapsed    | 10710   |
|    total_timesteps | 1757184 |
--------------------------------
Eval num_timesteps=1760000, episode_reward=-334.23 +/- 105.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -334         |
| time/                   |              |
|    total_timesteps      | 1760000      |
| train/                  |              |
|    approx_kl            | 0.0067120874 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.229        |
|    n_updates            | 4290         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.914        |
|    value_loss           | 1.71         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 164     |
|    iterations      | 430     |
|    time_elapsed    | 10720   |
|    total_timesteps | 1761280 |
--------------------------------
Eval num_timesteps=1765000, episode_reward=-462.08 +/- 108.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -462        |
| time/                   |             |
|    total_timesteps      | 1765000     |
| train/                  |             |
|    approx_kl            | 0.005006009 |
|    clip_fraction        | 0.0327      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.129       |
|    n_updates            | 4300        |
|    policy_gradient_loss | -0.0031     |
|    std                  | 0.919       |
|    value_loss           | 2.2         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 164     |
|    iterations      | 431     |
|    time_elapsed    | 10729   |
|    total_timesteps | 1765376 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 164          |
|    iterations           | 432          |
|    time_elapsed         | 10735        |
|    total_timesteps      | 1769472      |
| train/                  |              |
|    approx_kl            | 0.0065315617 |
|    clip_fraction        | 0.0454       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0278       |
|    n_updates            | 4310         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 0.921        |
|    value_loss           | 0.205        |
------------------------------------------
Eval num_timesteps=1770000, episode_reward=-247.10 +/- 117.98
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -247        |
| time/                   |             |
|    total_timesteps      | 1770000     |
| train/                  |             |
|    approx_kl            | 0.004137313 |
|    clip_fraction        | 0.0355      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0726      |
|    n_updates            | 4320        |
|    policy_gradient_loss | -0.00214    |
|    std                  | 0.922       |
|    value_loss           | 0.115       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 165     |
|    iterations      | 433     |
|    time_elapsed    | 10744   |
|    total_timesteps | 1773568 |
--------------------------------
Eval num_timesteps=1775000, episode_reward=-276.96 +/- 82.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -277         |
| time/                   |              |
|    total_timesteps      | 1775000      |
| train/                  |              |
|    approx_kl            | 0.0047273836 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.346        |
|    n_updates            | 4330         |
|    policy_gradient_loss | -0.00244     |
|    std                  | 0.918        |
|    value_loss           | 1.19         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 165     |
|    iterations      | 434     |
|    time_elapsed    | 10757   |
|    total_timesteps | 1777664 |
--------------------------------
Eval num_timesteps=1780000, episode_reward=-430.05 +/- 175.45
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -430        |
| time/                   |             |
|    total_timesteps      | 1780000     |
| train/                  |             |
|    approx_kl            | 0.005470964 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0571      |
|    n_updates            | 4340        |
|    policy_gradient_loss | -0.00102    |
|    std                  | 0.916       |
|    value_loss           | 0.135       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 165     |
|    iterations      | 435     |
|    time_elapsed    | 10768   |
|    total_timesteps | 1781760 |
--------------------------------
Eval num_timesteps=1785000, episode_reward=-363.66 +/- 181.35
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -364         |
| time/                   |              |
|    total_timesteps      | 1785000      |
| train/                  |              |
|    approx_kl            | 0.0044126776 |
|    clip_fraction        | 0.0205       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0992       |
|    n_updates            | 4350         |
|    policy_gradient_loss | -0.00162     |
|    std                  | 0.916        |
|    value_loss           | 4.77         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 165     |
|    iterations      | 436     |
|    time_elapsed    | 10778   |
|    total_timesteps | 1785856 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 165          |
|    iterations           | 437          |
|    time_elapsed         | 10784        |
|    total_timesteps      | 1789952      |
| train/                  |              |
|    approx_kl            | 0.0039649573 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.063        |
|    n_updates            | 4360         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 0.919        |
|    value_loss           | 1.15         |
------------------------------------------
Eval num_timesteps=1790000, episode_reward=-365.47 +/- 79.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 1790000      |
| train/                  |              |
|    approx_kl            | 0.0037635348 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0535       |
|    n_updates            | 4370         |
|    policy_gradient_loss | -0.0011      |
|    std                  | 0.919        |
|    value_loss           | 0.202        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 166     |
|    iterations      | 438     |
|    time_elapsed    | 10794   |
|    total_timesteps | 1794048 |
--------------------------------
Eval num_timesteps=1795000, episode_reward=-200.73 +/- 204.20
Episode length: 816.40 +/- 369.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 816          |
|    mean_reward          | -201         |
| time/                   |              |
|    total_timesteps      | 1795000      |
| train/                  |              |
|    approx_kl            | 0.0038972867 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.012        |
|    n_updates            | 4380         |
|    policy_gradient_loss | -0.00239     |
|    std                  | 0.92         |
|    value_loss           | 0.529        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 166     |
|    iterations      | 439     |
|    time_elapsed    | 10803   |
|    total_timesteps | 1798144 |
--------------------------------
Eval num_timesteps=1800000, episode_reward=-269.80 +/- 126.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -270         |
| time/                   |              |
|    total_timesteps      | 1800000      |
| train/                  |              |
|    approx_kl            | 0.0053918194 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.12         |
|    n_updates            | 4390         |
|    policy_gradient_loss | -0.00181     |
|    std                  | 0.922        |
|    value_loss           | 0.144        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 166     |
|    iterations      | 440     |
|    time_elapsed    | 10813   |
|    total_timesteps | 1802240 |
--------------------------------
Eval num_timesteps=1805000, episode_reward=-332.94 +/- 152.24
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -333         |
| time/                   |              |
|    total_timesteps      | 1805000      |
| train/                  |              |
|    approx_kl            | 0.0049277535 |
|    clip_fraction        | 0.036        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 4.72         |
|    n_updates            | 4400         |
|    policy_gradient_loss | -0.00277     |
|    std                  | 0.919        |
|    value_loss           | 5.8          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 166     |
|    iterations      | 441     |
|    time_elapsed    | 10823   |
|    total_timesteps | 1806336 |
--------------------------------
Eval num_timesteps=1810000, episode_reward=-285.85 +/- 170.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 1810000      |
| train/                  |              |
|    approx_kl            | 0.0028772932 |
|    clip_fraction        | 0.0172       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.104        |
|    n_updates            | 4410         |
|    policy_gradient_loss | -0.00107     |
|    std                  | 0.919        |
|    value_loss           | 6.64         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 167     |
|    iterations      | 442     |
|    time_elapsed    | 10833   |
|    total_timesteps | 1810432 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 167          |
|    iterations           | 443          |
|    time_elapsed         | 10840        |
|    total_timesteps      | 1814528      |
| train/                  |              |
|    approx_kl            | 0.0045441696 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.011        |
|    n_updates            | 4420         |
|    policy_gradient_loss | -0.00165     |
|    std                  | 0.917        |
|    value_loss           | 0.708        |
------------------------------------------
Eval num_timesteps=1815000, episode_reward=-237.81 +/- 81.48
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -238        |
| time/                   |             |
|    total_timesteps      | 1815000     |
| train/                  |             |
|    approx_kl            | 0.008421918 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00745    |
|    n_updates            | 4430        |
|    policy_gradient_loss | -0.00555    |
|    std                  | 0.913       |
|    value_loss           | 0.0537      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 167     |
|    iterations      | 444     |
|    time_elapsed    | 10849   |
|    total_timesteps | 1818624 |
--------------------------------
Eval num_timesteps=1820000, episode_reward=-413.64 +/- 93.83
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 1820000      |
| train/                  |              |
|    approx_kl            | 0.0044253604 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.116        |
|    n_updates            | 4440         |
|    policy_gradient_loss | -0.0037      |
|    std                  | 0.911        |
|    value_loss           | 1.5          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 167     |
|    iterations      | 445     |
|    time_elapsed    | 10859   |
|    total_timesteps | 1822720 |
--------------------------------
Eval num_timesteps=1825000, episode_reward=-324.89 +/- 146.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -325        |
| time/                   |             |
|    total_timesteps      | 1825000     |
| train/                  |             |
|    approx_kl            | 0.005015676 |
|    clip_fraction        | 0.0357      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00672     |
|    n_updates            | 4450        |
|    policy_gradient_loss | -0.00386    |
|    std                  | 0.91        |
|    value_loss           | 0.164       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 168     |
|    iterations      | 446     |
|    time_elapsed    | 10868   |
|    total_timesteps | 1826816 |
--------------------------------
Eval num_timesteps=1830000, episode_reward=-400.02 +/- 57.52
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -400        |
| time/                   |             |
|    total_timesteps      | 1830000     |
| train/                  |             |
|    approx_kl            | 0.007837558 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0314      |
|    n_updates            | 4460        |
|    policy_gradient_loss | -0.00359    |
|    std                  | 0.908       |
|    value_loss           | 0.121       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 168     |
|    iterations      | 447     |
|    time_elapsed    | 10878   |
|    total_timesteps | 1830912 |
--------------------------------
Eval num_timesteps=1835000, episode_reward=-335.77 +/- 175.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -336         |
| time/                   |              |
|    total_timesteps      | 1835000      |
| train/                  |              |
|    approx_kl            | 0.0055677043 |
|    clip_fraction        | 0.0476       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0394       |
|    n_updates            | 4470         |
|    policy_gradient_loss | -0.00427     |
|    std                  | 0.912        |
|    value_loss           | 1.51         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 168     |
|    iterations      | 448     |
|    time_elapsed    | 10888   |
|    total_timesteps | 1835008 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 168          |
|    iterations           | 449          |
|    time_elapsed         | 10895        |
|    total_timesteps      | 1839104      |
| train/                  |              |
|    approx_kl            | 0.0038679827 |
|    clip_fraction        | 0.0192       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.328        |
|    n_updates            | 4480         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 0.908        |
|    value_loss           | 2.69         |
------------------------------------------
Eval num_timesteps=1840000, episode_reward=-295.62 +/- 130.69
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -296         |
| time/                   |              |
|    total_timesteps      | 1840000      |
| train/                  |              |
|    approx_kl            | 0.0027945505 |
|    clip_fraction        | 0.0202       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0105       |
|    n_updates            | 4490         |
|    policy_gradient_loss | -0.0008      |
|    std                  | 0.901        |
|    value_loss           | 0.0357       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 169     |
|    iterations      | 450     |
|    time_elapsed    | 10904   |
|    total_timesteps | 1843200 |
--------------------------------
Eval num_timesteps=1845000, episode_reward=-436.58 +/- 169.54
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -437        |
| time/                   |             |
|    total_timesteps      | 1845000     |
| train/                  |             |
|    approx_kl            | 0.004946003 |
|    clip_fraction        | 0.0332      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0123      |
|    n_updates            | 4500        |
|    policy_gradient_loss | -0.00179    |
|    std                  | 0.903       |
|    value_loss           | 2.12        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 169     |
|    iterations      | 451     |
|    time_elapsed    | 10914   |
|    total_timesteps | 1847296 |
--------------------------------
Eval num_timesteps=1850000, episode_reward=-404.52 +/- 141.03
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -405        |
| time/                   |             |
|    total_timesteps      | 1850000     |
| train/                  |             |
|    approx_kl            | 0.004943069 |
|    clip_fraction        | 0.0274      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.387       |
|    n_updates            | 4510        |
|    policy_gradient_loss | -0.00187    |
|    std                  | 0.899       |
|    value_loss           | 1.25        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 169     |
|    iterations      | 452     |
|    time_elapsed    | 10923   |
|    total_timesteps | 1851392 |
--------------------------------
Eval num_timesteps=1855000, episode_reward=-436.87 +/- 144.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -437         |
| time/                   |              |
|    total_timesteps      | 1855000      |
| train/                  |              |
|    approx_kl            | 0.0040903753 |
|    clip_fraction        | 0.0219       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0428       |
|    n_updates            | 4520         |
|    policy_gradient_loss | -0.00119     |
|    std                  | 0.898        |
|    value_loss           | 1.07         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 169     |
|    iterations      | 453     |
|    time_elapsed    | 10933   |
|    total_timesteps | 1855488 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 169         |
|    iterations           | 454         |
|    time_elapsed         | 10939       |
|    total_timesteps      | 1859584     |
| train/                  |             |
|    approx_kl            | 0.005732646 |
|    clip_fraction        | 0.0459      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.249       |
|    n_updates            | 4530        |
|    policy_gradient_loss | -0.00269    |
|    std                  | 0.896       |
|    value_loss           | 0.757       |
-----------------------------------------
Eval num_timesteps=1860000, episode_reward=-185.46 +/- 110.35
Episode length: 807.20 +/- 387.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 807         |
|    mean_reward          | -185        |
| time/                   |             |
|    total_timesteps      | 1860000     |
| train/                  |             |
|    approx_kl            | 0.004524862 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00679     |
|    n_updates            | 4540        |
|    policy_gradient_loss | -0.00209    |
|    std                  | 0.904       |
|    value_loss           | 0.125       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 170     |
|    iterations      | 455     |
|    time_elapsed    | 10948   |
|    total_timesteps | 1863680 |
--------------------------------
Eval num_timesteps=1865000, episode_reward=-391.70 +/- 59.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -392         |
| time/                   |              |
|    total_timesteps      | 1865000      |
| train/                  |              |
|    approx_kl            | 0.0069082514 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.89        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.38         |
|    n_updates            | 4550         |
|    policy_gradient_loss | -0.00254     |
|    std                  | 0.902        |
|    value_loss           | 21.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 170     |
|    iterations      | 456     |
|    time_elapsed    | 10958   |
|    total_timesteps | 1867776 |
--------------------------------
Eval num_timesteps=1870000, episode_reward=-381.90 +/- 175.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -382         |
| time/                   |              |
|    total_timesteps      | 1870000      |
| train/                  |              |
|    approx_kl            | 0.0053188447 |
|    clip_fraction        | 0.0553       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.102        |
|    n_updates            | 4560         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 0.899        |
|    value_loss           | 20.5         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 170     |
|    iterations      | 457     |
|    time_elapsed    | 10968   |
|    total_timesteps | 1871872 |
--------------------------------
Eval num_timesteps=1875000, episode_reward=-218.51 +/- 52.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -219         |
| time/                   |              |
|    total_timesteps      | 1875000      |
| train/                  |              |
|    approx_kl            | 0.0043473034 |
|    clip_fraction        | 0.0187       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.218        |
|    n_updates            | 4570         |
|    policy_gradient_loss | -0.000712    |
|    std                  | 0.896        |
|    value_loss           | 2.24         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 170     |
|    iterations      | 458     |
|    time_elapsed    | 10977   |
|    total_timesteps | 1875968 |
--------------------------------
Eval num_timesteps=1880000, episode_reward=-250.02 +/- 146.88
Episode length: 819.00 +/- 364.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 819          |
|    mean_reward          | -250         |
| time/                   |              |
|    total_timesteps      | 1880000      |
| train/                  |              |
|    approx_kl            | 0.0054500466 |
|    clip_fraction        | 0.0384       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 13.3         |
|    n_updates            | 4580         |
|    policy_gradient_loss | -0.00206     |
|    std                  | 0.898        |
|    value_loss           | 2.99         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 171     |
|    iterations      | 459     |
|    time_elapsed    | 10987   |
|    total_timesteps | 1880064 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 171          |
|    iterations           | 460          |
|    time_elapsed         | 10993        |
|    total_timesteps      | 1884160      |
| train/                  |              |
|    approx_kl            | 0.0052475887 |
|    clip_fraction        | 0.0413       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.89        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.411        |
|    n_updates            | 4590         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 0.902        |
|    value_loss           | 4.18         |
------------------------------------------
Eval num_timesteps=1885000, episode_reward=-425.63 +/- 142.53
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -426         |
| time/                   |              |
|    total_timesteps      | 1885000      |
| train/                  |              |
|    approx_kl            | 0.0054391855 |
|    clip_fraction        | 0.0528       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0677       |
|    n_updates            | 4600         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 0.906        |
|    value_loss           | 1.42         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 171     |
|    iterations      | 461     |
|    time_elapsed    | 11003   |
|    total_timesteps | 1888256 |
--------------------------------
Eval num_timesteps=1890000, episode_reward=-168.66 +/- 86.26
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -169         |
| time/                   |              |
|    total_timesteps      | 1890000      |
| train/                  |              |
|    approx_kl            | 0.0063922172 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.112        |
|    n_updates            | 4610         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 0.901        |
|    value_loss           | 13           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 171     |
|    iterations      | 462     |
|    time_elapsed    | 11012   |
|    total_timesteps | 1892352 |
--------------------------------
Eval num_timesteps=1895000, episode_reward=-332.54 +/- 139.67
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -333        |
| time/                   |             |
|    total_timesteps      | 1895000     |
| train/                  |             |
|    approx_kl            | 0.006317612 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.111       |
|    n_updates            | 4620        |
|    policy_gradient_loss | -0.00267    |
|    std                  | 0.903       |
|    value_loss           | 13          |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 172     |
|    iterations      | 463     |
|    time_elapsed    | 11021   |
|    total_timesteps | 1896448 |
--------------------------------
Eval num_timesteps=1900000, episode_reward=-415.11 +/- 133.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -415         |
| time/                   |              |
|    total_timesteps      | 1900000      |
| train/                  |              |
|    approx_kl            | 0.0058328724 |
|    clip_fraction        | 0.044        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0112       |
|    n_updates            | 4630         |
|    policy_gradient_loss | -0.00275     |
|    std                  | 0.914        |
|    value_loss           | 0.103        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 172     |
|    iterations      | 464     |
|    time_elapsed    | 11031   |
|    total_timesteps | 1900544 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 172          |
|    iterations           | 465          |
|    time_elapsed         | 11038        |
|    total_timesteps      | 1904640      |
| train/                  |              |
|    approx_kl            | 0.0043351213 |
|    clip_fraction        | 0.0204       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 4640         |
|    policy_gradient_loss | -0.00184     |
|    std                  | 0.91         |
|    value_loss           | 1.42         |
------------------------------------------
Eval num_timesteps=1905000, episode_reward=-264.98 +/- 68.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -265         |
| time/                   |              |
|    total_timesteps      | 1905000      |
| train/                  |              |
|    approx_kl            | 0.0039038802 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0541       |
|    n_updates            | 4650         |
|    policy_gradient_loss | -0.00205     |
|    std                  | 0.907        |
|    value_loss           | 1.07         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 172     |
|    iterations      | 466     |
|    time_elapsed    | 11048   |
|    total_timesteps | 1908736 |
--------------------------------
Eval num_timesteps=1910000, episode_reward=-407.13 +/- 102.44
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 1910000     |
| train/                  |             |
|    approx_kl            | 0.004028746 |
|    clip_fraction        | 0.0331      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0663      |
|    n_updates            | 4660        |
|    policy_gradient_loss | -0.00255    |
|    std                  | 0.907       |
|    value_loss           | 0.374       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 172     |
|    iterations      | 467     |
|    time_elapsed    | 11058   |
|    total_timesteps | 1912832 |
--------------------------------
Eval num_timesteps=1915000, episode_reward=-418.71 +/- 150.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -419         |
| time/                   |              |
|    total_timesteps      | 1915000      |
| train/                  |              |
|    approx_kl            | 0.0026461948 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.87         |
|    n_updates            | 4670         |
|    policy_gradient_loss | -0.000896    |
|    std                  | 0.906        |
|    value_loss           | 7.6          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 173     |
|    iterations      | 468     |
|    time_elapsed    | 11067   |
|    total_timesteps | 1916928 |
--------------------------------
Eval num_timesteps=1920000, episode_reward=-262.47 +/- 163.65
Episode length: 810.80 +/- 380.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -262         |
| time/                   |              |
|    total_timesteps      | 1920000      |
| train/                  |              |
|    approx_kl            | 0.0042775297 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.54         |
|    n_updates            | 4680         |
|    policy_gradient_loss | -0.00334     |
|    std                  | 0.905        |
|    value_loss           | 1.44         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 173     |
|    iterations      | 469     |
|    time_elapsed    | 11077   |
|    total_timesteps | 1921024 |
--------------------------------
Eval num_timesteps=1925000, episode_reward=-378.33 +/- 214.96
Episode length: 817.60 +/- 366.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 818        |
|    mean_reward          | -378       |
| time/                   |            |
|    total_timesteps      | 1925000    |
| train/                  |            |
|    approx_kl            | 0.00550023 |
|    clip_fraction        | 0.0184     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.9       |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0001     |
|    loss                 | 4.2        |
|    n_updates            | 4690       |
|    policy_gradient_loss | -0.00122   |
|    std                  | 0.904      |
|    value_loss           | 12.1       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 173     |
|    iterations      | 470     |
|    time_elapsed    | 11087   |
|    total_timesteps | 1925120 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 173         |
|    iterations           | 471         |
|    time_elapsed         | 11093       |
|    total_timesteps      | 1929216     |
| train/                  |             |
|    approx_kl            | 0.005844293 |
|    clip_fraction        | 0.0587      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.148       |
|    n_updates            | 4700        |
|    policy_gradient_loss | -0.00411    |
|    std                  | 0.899       |
|    value_loss           | 5.53        |
-----------------------------------------
Eval num_timesteps=1930000, episode_reward=-400.33 +/- 93.39
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -400         |
| time/                   |              |
|    total_timesteps      | 1930000      |
| train/                  |              |
|    approx_kl            | 0.0046483655 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.034        |
|    n_updates            | 4710         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 0.898        |
|    value_loss           | 0.197        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 174     |
|    iterations      | 472     |
|    time_elapsed    | 11103   |
|    total_timesteps | 1933312 |
--------------------------------
Eval num_timesteps=1935000, episode_reward=-269.70 +/- 114.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -270         |
| time/                   |              |
|    total_timesteps      | 1935000      |
| train/                  |              |
|    approx_kl            | 0.0088676335 |
|    clip_fraction        | 0.0836       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.87        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.53         |
|    n_updates            | 4720         |
|    policy_gradient_loss | -0.00416     |
|    std                  | 0.893        |
|    value_loss           | 1.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 174     |
|    iterations      | 473     |
|    time_elapsed    | 11113   |
|    total_timesteps | 1937408 |
--------------------------------
Eval num_timesteps=1940000, episode_reward=-301.85 +/- 84.90
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -302      |
| time/                   |           |
|    total_timesteps      | 1940000   |
| train/                  |           |
|    approx_kl            | 0.0060938 |
|    clip_fraction        | 0.0323    |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.86     |
|    explained_variance   | 0.972     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.718     |
|    n_updates            | 4730      |
|    policy_gradient_loss | -0.000797 |
|    std                  | 0.892     |
|    value_loss           | 13.1      |
---------------------------------------
--------------------------------
| time/              |         |
|    fps             | 174     |
|    iterations      | 474     |
|    time_elapsed    | 11123   |
|    total_timesteps | 1941504 |
--------------------------------
Eval num_timesteps=1945000, episode_reward=-333.03 +/- 169.36
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -333        |
| time/                   |             |
|    total_timesteps      | 1945000     |
| train/                  |             |
|    approx_kl            | 0.004030723 |
|    clip_fraction        | 0.0212      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0486      |
|    n_updates            | 4740        |
|    policy_gradient_loss | -0.000953   |
|    std                  | 0.894       |
|    value_loss           | 0.242       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 174     |
|    iterations      | 475     |
|    time_elapsed    | 11134   |
|    total_timesteps | 1945600 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 174         |
|    iterations           | 476         |
|    time_elapsed         | 11141       |
|    total_timesteps      | 1949696     |
| train/                  |             |
|    approx_kl            | 0.004634777 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.064       |
|    n_updates            | 4750        |
|    policy_gradient_loss | -0.00258    |
|    std                  | 0.89        |
|    value_loss           | 2.31        |
-----------------------------------------
Eval num_timesteps=1950000, episode_reward=-314.42 +/- 91.61
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -314      |
| time/                   |           |
|    total_timesteps      | 1950000   |
| train/                  |           |
|    approx_kl            | 0.0042148 |
|    clip_fraction        | 0.0273    |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.87     |
|    explained_variance   | 0.995     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0152    |
|    n_updates            | 4760      |
|    policy_gradient_loss | -0.0022   |
|    std                  | 0.898     |
|    value_loss           | 0.109     |
---------------------------------------
--------------------------------
| time/              |         |
|    fps             | 175     |
|    iterations      | 477     |
|    time_elapsed    | 11150   |
|    total_timesteps | 1953792 |
--------------------------------
Eval num_timesteps=1955000, episode_reward=-377.82 +/- 80.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -378        |
| time/                   |             |
|    total_timesteps      | 1955000     |
| train/                  |             |
|    approx_kl            | 0.007521459 |
|    clip_fraction        | 0.0463      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0302      |
|    n_updates            | 4770        |
|    policy_gradient_loss | -0.00472    |
|    std                  | 0.891       |
|    value_loss           | 0.138       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 175     |
|    iterations      | 478     |
|    time_elapsed    | 11160   |
|    total_timesteps | 1957888 |
--------------------------------
Eval num_timesteps=1960000, episode_reward=-368.54 +/- 144.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -369         |
| time/                   |              |
|    total_timesteps      | 1960000      |
| train/                  |              |
|    approx_kl            | 0.0022905092 |
|    clip_fraction        | 0.0208       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.86        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.57         |
|    n_updates            | 4780         |
|    policy_gradient_loss | -0.00096     |
|    std                  | 0.891        |
|    value_loss           | 2.63         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 175     |
|    iterations      | 479     |
|    time_elapsed    | 11170   |
|    total_timesteps | 1961984 |
--------------------------------
Eval num_timesteps=1965000, episode_reward=-404.80 +/- 105.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -405         |
| time/                   |              |
|    total_timesteps      | 1965000      |
| train/                  |              |
|    approx_kl            | 0.0064287554 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0458       |
|    n_updates            | 4790         |
|    policy_gradient_loss | -0.00246     |
|    std                  | 0.888        |
|    value_loss           | 2.14         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 175     |
|    iterations      | 480     |
|    time_elapsed    | 11179   |
|    total_timesteps | 1966080 |
--------------------------------
Eval num_timesteps=1970000, episode_reward=-238.19 +/- 131.81
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -238        |
| time/                   |             |
|    total_timesteps      | 1970000     |
| train/                  |             |
|    approx_kl            | 0.004514965 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.27        |
|    n_updates            | 4800        |
|    policy_gradient_loss | -0.000326   |
|    std                  | 0.886       |
|    value_loss           | 2.46        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 176     |
|    iterations      | 481     |
|    time_elapsed    | 11189   |
|    total_timesteps | 1970176 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 176          |
|    iterations           | 482          |
|    time_elapsed         | 11196        |
|    total_timesteps      | 1974272      |
| train/                  |              |
|    approx_kl            | 0.0049671456 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.84        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.16         |
|    n_updates            | 4810         |
|    policy_gradient_loss | -0.00299     |
|    std                  | 0.886        |
|    value_loss           | 2            |
------------------------------------------
Eval num_timesteps=1975000, episode_reward=-360.13 +/- 127.06
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -360        |
| time/                   |             |
|    total_timesteps      | 1975000     |
| train/                  |             |
|    approx_kl            | 0.008436431 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0609      |
|    n_updates            | 4820        |
|    policy_gradient_loss | -0.00643    |
|    std                  | 0.893       |
|    value_loss           | 0.0642      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 176     |
|    iterations      | 483     |
|    time_elapsed    | 11205   |
|    total_timesteps | 1978368 |
--------------------------------
Eval num_timesteps=1980000, episode_reward=-415.12 +/- 152.53
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -415         |
| time/                   |              |
|    total_timesteps      | 1980000      |
| train/                  |              |
|    approx_kl            | 0.0039035114 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.87        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0396       |
|    n_updates            | 4830         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.893        |
|    value_loss           | 0.249        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 176     |
|    iterations      | 484     |
|    time_elapsed    | 11215   |
|    total_timesteps | 1982464 |
--------------------------------
Eval num_timesteps=1985000, episode_reward=-395.01 +/- 221.36
Episode length: 814.80 +/- 372.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 815        |
|    mean_reward          | -395       |
| time/                   |            |
|    total_timesteps      | 1985000    |
| train/                  |            |
|    approx_kl            | 0.00556904 |
|    clip_fraction        | 0.0515     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.87      |
|    explained_variance   | 0.991      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.00375    |
|    n_updates            | 4840       |
|    policy_gradient_loss | -0.00323   |
|    std                  | 0.889      |
|    value_loss           | 0.0975     |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 176     |
|    iterations      | 485     |
|    time_elapsed    | 11224   |
|    total_timesteps | 1986560 |
--------------------------------
Eval num_timesteps=1990000, episode_reward=-315.62 +/- 70.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -316         |
| time/                   |              |
|    total_timesteps      | 1990000      |
| train/                  |              |
|    approx_kl            | 0.0043969112 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.86        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.551        |
|    n_updates            | 4850         |
|    policy_gradient_loss | -0.00207     |
|    std                  | 0.887        |
|    value_loss           | 1.1          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 177     |
|    iterations      | 486     |
|    time_elapsed    | 11234   |
|    total_timesteps | 1990656 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 177          |
|    iterations           | 487          |
|    time_elapsed         | 11241        |
|    total_timesteps      | 1994752      |
| train/                  |              |
|    approx_kl            | 0.0027690495 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.222        |
|    n_updates            | 4860         |
|    policy_gradient_loss | -0.00153     |
|    std                  | 0.887        |
|    value_loss           | 0.173        |
------------------------------------------
Eval num_timesteps=1995000, episode_reward=-321.71 +/- 198.04
Episode length: 813.40 +/- 375.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 813         |
|    mean_reward          | -322        |
| time/                   |             |
|    total_timesteps      | 1995000     |
| train/                  |             |
|    approx_kl            | 0.003593527 |
|    clip_fraction        | 0.0155      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0125      |
|    n_updates            | 4870        |
|    policy_gradient_loss | 0.000116    |
|    std                  | 0.893       |
|    value_loss           | 0.0405      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 177     |
|    iterations      | 488     |
|    time_elapsed    | 11250   |
|    total_timesteps | 1998848 |
--------------------------------
Eval num_timesteps=2000000, episode_reward=-258.85 +/- 173.40
Episode length: 812.40 +/- 377.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 812         |
|    mean_reward          | -259        |
| time/                   |             |
|    total_timesteps      | 2000000     |
| train/                  |             |
|    approx_kl            | 0.004972825 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0237      |
|    n_updates            | 4880        |
|    policy_gradient_loss | -0.000535   |
|    std                  | 0.893       |
|    value_loss           | 1.37        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 177     |
|    iterations      | 489     |
|    time_elapsed    | 11260   |
|    total_timesteps | 2002944 |
--------------------------------
Eval num_timesteps=2005000, episode_reward=-218.16 +/- 171.51
Episode length: 812.20 +/- 377.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | -218         |
| time/                   |              |
|    total_timesteps      | 2005000      |
| train/                  |              |
|    approx_kl            | 0.0022264202 |
|    clip_fraction        | 0.00894      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.86        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.725        |
|    n_updates            | 4890         |
|    policy_gradient_loss | -0.000119    |
|    std                  | 0.885        |
|    value_loss           | 0.303        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 178     |
|    iterations      | 490     |
|    time_elapsed    | 11268   |
|    total_timesteps | 2007040 |
--------------------------------
Eval num_timesteps=2010000, episode_reward=-323.38 +/- 193.13
Episode length: 813.80 +/- 374.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | -323        |
| time/                   |             |
|    total_timesteps      | 2010000     |
| train/                  |             |
|    approx_kl            | 0.004152732 |
|    clip_fraction        | 0.0246      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0122      |
|    n_updates            | 4900        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 0.885       |
|    value_loss           | 0.746       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 178     |
|    iterations      | 491     |
|    time_elapsed    | 11277   |
|    total_timesteps | 2011136 |
--------------------------------
Eval num_timesteps=2015000, episode_reward=-200.14 +/- 137.12
Episode length: 814.40 +/- 373.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | -200        |
| time/                   |             |
|    total_timesteps      | 2015000     |
| train/                  |             |
|    approx_kl            | 0.005814063 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0115      |
|    n_updates            | 4910        |
|    policy_gradient_loss | -0.00297    |
|    std                  | 0.885       |
|    value_loss           | 0.187       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 178     |
|    iterations      | 492     |
|    time_elapsed    | 11287   |
|    total_timesteps | 2015232 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 178          |
|    iterations           | 493          |
|    time_elapsed         | 11293        |
|    total_timesteps      | 2019328      |
| train/                  |              |
|    approx_kl            | 0.0045173345 |
|    clip_fraction        | 0.0306       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.431        |
|    n_updates            | 4920         |
|    policy_gradient_loss | -0.00269     |
|    std                  | 0.887        |
|    value_loss           | 1.39         |
------------------------------------------
Eval num_timesteps=2020000, episode_reward=-252.58 +/- 224.33
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -253        |
| time/                   |             |
|    total_timesteps      | 2020000     |
| train/                  |             |
|    approx_kl            | 0.006034998 |
|    clip_fraction        | 0.0544      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.104       |
|    n_updates            | 4930        |
|    policy_gradient_loss | -0.00194    |
|    std                  | 0.888       |
|    value_loss           | 0.752       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 179     |
|    iterations      | 494     |
|    time_elapsed    | 11302   |
|    total_timesteps | 2023424 |
--------------------------------
Eval num_timesteps=2025000, episode_reward=-477.70 +/- 62.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -478         |
| time/                   |              |
|    total_timesteps      | 2025000      |
| train/                  |              |
|    approx_kl            | 0.0037946976 |
|    clip_fraction        | 0.025        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.86        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0326       |
|    n_updates            | 4940         |
|    policy_gradient_loss | -0.000213    |
|    std                  | 0.893        |
|    value_loss           | 0.539        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 179     |
|    iterations      | 495     |
|    time_elapsed    | 11312   |
|    total_timesteps | 2027520 |
--------------------------------
Eval num_timesteps=2030000, episode_reward=-232.28 +/- 120.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -232         |
| time/                   |              |
|    total_timesteps      | 2030000      |
| train/                  |              |
|    approx_kl            | 0.0052111857 |
|    clip_fraction        | 0.0445       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.333        |
|    n_updates            | 4950         |
|    policy_gradient_loss | -0.00376     |
|    std                  | 0.896        |
|    value_loss           | 0.475        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 179     |
|    iterations      | 496     |
|    time_elapsed    | 11322   |
|    total_timesteps | 2031616 |
--------------------------------
Eval num_timesteps=2035000, episode_reward=-344.16 +/- 141.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -344         |
| time/                   |              |
|    total_timesteps      | 2035000      |
| train/                  |              |
|    approx_kl            | 0.0066151144 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.88        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0886       |
|    n_updates            | 4960         |
|    policy_gradient_loss | -0.00291     |
|    std                  | 0.896        |
|    value_loss           | 0.161        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 179     |
|    iterations      | 497     |
|    time_elapsed    | 11331   |
|    total_timesteps | 2035712 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 179         |
|    iterations           | 498         |
|    time_elapsed         | 11338       |
|    total_timesteps      | 2039808     |
| train/                  |             |
|    approx_kl            | 0.005330413 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 21.6        |
|    n_updates            | 4970        |
|    policy_gradient_loss | -0.00103    |
|    std                  | 0.893       |
|    value_loss           | 11.9        |
-----------------------------------------
Eval num_timesteps=2040000, episode_reward=-282.33 +/- 156.40
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -282        |
| time/                   |             |
|    total_timesteps      | 2040000     |
| train/                  |             |
|    approx_kl            | 0.003801967 |
|    clip_fraction        | 0.0276      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0317      |
|    n_updates            | 4980        |
|    policy_gradient_loss | -0.00257    |
|    std                  | 0.892       |
|    value_loss           | 1.53        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 180     |
|    iterations      | 499     |
|    time_elapsed    | 11348   |
|    total_timesteps | 2043904 |
--------------------------------
Eval num_timesteps=2045000, episode_reward=-332.23 +/- 142.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 2045000      |
| train/                  |              |
|    approx_kl            | 0.0034579877 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.86        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.647        |
|    n_updates            | 4990         |
|    policy_gradient_loss | -0.00184     |
|    std                  | 0.892        |
|    value_loss           | 1.61         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 180     |
|    iterations      | 500     |
|    time_elapsed    | 11357   |
|    total_timesteps | 2048000 |
--------------------------------
Eval num_timesteps=2050000, episode_reward=-450.01 +/- 146.49
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -450         |
| time/                   |              |
|    total_timesteps      | 2050000      |
| train/                  |              |
|    approx_kl            | 0.0032216075 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.86        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0204       |
|    n_updates            | 5000         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 0.888        |
|    value_loss           | 0.298        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 180     |
|    iterations      | 501     |
|    time_elapsed    | 11367   |
|    total_timesteps | 2052096 |
--------------------------------
Eval num_timesteps=2055000, episode_reward=-274.20 +/- 116.54
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -274         |
| time/                   |              |
|    total_timesteps      | 2055000      |
| train/                  |              |
|    approx_kl            | 0.0052084057 |
|    clip_fraction        | 0.0496       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.03         |
|    n_updates            | 5010         |
|    policy_gradient_loss | -0.00343     |
|    std                  | 0.887        |
|    value_loss           | 2.14         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 180     |
|    iterations      | 502     |
|    time_elapsed    | 11377   |
|    total_timesteps | 2056192 |
--------------------------------
Eval num_timesteps=2060000, episode_reward=-249.81 +/- 128.13
Episode length: 811.20 +/- 379.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -250        |
| time/                   |             |
|    total_timesteps      | 2060000     |
| train/                  |             |
|    approx_kl            | 0.003766641 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0205      |
|    n_updates            | 5020        |
|    policy_gradient_loss | -0.000484   |
|    std                  | 0.887       |
|    value_loss           | 3.74        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 180     |
|    iterations      | 503     |
|    time_elapsed    | 11387   |
|    total_timesteps | 2060288 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 181         |
|    iterations           | 504         |
|    time_elapsed         | 11394       |
|    total_timesteps      | 2064384     |
| train/                  |             |
|    approx_kl            | 0.005147633 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.84       |
|    explained_variance   | 0.973       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0949      |
|    n_updates            | 5030        |
|    policy_gradient_loss | -0.00374    |
|    std                  | 0.886       |
|    value_loss           | 1.63        |
-----------------------------------------
Eval num_timesteps=2065000, episode_reward=-302.17 +/- 109.54
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -302         |
| time/                   |              |
|    total_timesteps      | 2065000      |
| train/                  |              |
|    approx_kl            | 0.0058870967 |
|    clip_fraction        | 0.0478       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.118        |
|    n_updates            | 5040         |
|    policy_gradient_loss | -0.00283     |
|    std                  | 0.891        |
|    value_loss           | 0.178        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 181     |
|    iterations      | 505     |
|    time_elapsed    | 11403   |
|    total_timesteps | 2068480 |
--------------------------------
Eval num_timesteps=2070000, episode_reward=-493.73 +/- 165.64
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -494        |
| time/                   |             |
|    total_timesteps      | 2070000     |
| train/                  |             |
|    approx_kl            | 0.006124648 |
|    clip_fraction        | 0.0496      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.366       |
|    n_updates            | 5050        |
|    policy_gradient_loss | -0.0024     |
|    std                  | 0.889       |
|    value_loss           | 1.1         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 181     |
|    iterations      | 506     |
|    time_elapsed    | 11413   |
|    total_timesteps | 2072576 |
--------------------------------
Eval num_timesteps=2075000, episode_reward=-262.58 +/- 72.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -263         |
| time/                   |              |
|    total_timesteps      | 2075000      |
| train/                  |              |
|    approx_kl            | 0.0046584057 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.84        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0106       |
|    n_updates            | 5060         |
|    policy_gradient_loss | -0.00272     |
|    std                  | 0.881        |
|    value_loss           | 0.0671       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 181     |
|    iterations      | 507     |
|    time_elapsed    | 11422   |
|    total_timesteps | 2076672 |
--------------------------------
Eval num_timesteps=2080000, episode_reward=-364.27 +/- 104.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -364        |
| time/                   |             |
|    total_timesteps      | 2080000     |
| train/                  |             |
|    approx_kl            | 0.004087045 |
|    clip_fraction        | 0.0268      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.83       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.527       |
|    n_updates            | 5070        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 0.884       |
|    value_loss           | 1.64        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 182     |
|    iterations      | 508     |
|    time_elapsed    | 11432   |
|    total_timesteps | 2080768 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 182          |
|    iterations           | 509          |
|    time_elapsed         | 11438        |
|    total_timesteps      | 2084864      |
| train/                  |              |
|    approx_kl            | 0.0047664074 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.84        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0218       |
|    n_updates            | 5080         |
|    policy_gradient_loss | -0.0018      |
|    std                  | 0.886        |
|    value_loss           | 0.102        |
------------------------------------------
Eval num_timesteps=2085000, episode_reward=-311.92 +/- 136.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -312         |
| time/                   |              |
|    total_timesteps      | 2085000      |
| train/                  |              |
|    approx_kl            | 0.0057106055 |
|    clip_fraction        | 0.0445       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0071       |
|    n_updates            | 5090         |
|    policy_gradient_loss | -0.00304     |
|    std                  | 0.887        |
|    value_loss           | 0.112        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 182     |
|    iterations      | 510     |
|    time_elapsed    | 11448   |
|    total_timesteps | 2088960 |
--------------------------------
Eval num_timesteps=2090000, episode_reward=-388.05 +/- 189.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -388         |
| time/                   |              |
|    total_timesteps      | 2090000      |
| train/                  |              |
|    approx_kl            | 0.0054484415 |
|    clip_fraction        | 0.0382       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.109        |
|    n_updates            | 5100         |
|    policy_gradient_loss | -0.00354     |
|    std                  | 0.885        |
|    value_loss           | 1.43         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 182     |
|    iterations      | 511     |
|    time_elapsed    | 11457   |
|    total_timesteps | 2093056 |
--------------------------------
Eval num_timesteps=2095000, episode_reward=-248.26 +/- 169.97
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -248       |
| time/                   |            |
|    total_timesteps      | 2095000    |
| train/                  |            |
|    approx_kl            | 0.00553458 |
|    clip_fraction        | 0.0347     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.84      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0359     |
|    n_updates            | 5110       |
|    policy_gradient_loss | -0.0017    |
|    std                  | 0.885      |
|    value_loss           | 0.442      |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 182     |
|    iterations      | 512     |
|    time_elapsed    | 11467   |
|    total_timesteps | 2097152 |
--------------------------------
Eval num_timesteps=2100000, episode_reward=-344.98 +/- 204.40
Episode length: 810.60 +/- 380.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -345         |
| time/                   |              |
|    total_timesteps      | 2100000      |
| train/                  |              |
|    approx_kl            | 0.0034010238 |
|    clip_fraction        | 0.0113       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.84        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 17.1         |
|    n_updates            | 5120         |
|    policy_gradient_loss | -0.000447    |
|    std                  | 0.884        |
|    value_loss           | 11.1         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 183     |
|    iterations      | 513     |
|    time_elapsed    | 11476   |
|    total_timesteps | 2101248 |
--------------------------------
Eval num_timesteps=2105000, episode_reward=-287.11 +/- 73.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -287        |
| time/                   |             |
|    total_timesteps      | 2105000     |
| train/                  |             |
|    approx_kl            | 0.006165566 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.86       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00377    |
|    n_updates            | 5130        |
|    policy_gradient_loss | -0.00246    |
|    std                  | 0.892       |
|    value_loss           | 0.0539      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 183     |
|    iterations      | 514     |
|    time_elapsed    | 11485   |
|    total_timesteps | 2105344 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 183         |
|    iterations           | 515         |
|    time_elapsed         | 11492       |
|    total_timesteps      | 2109440     |
| train/                  |             |
|    approx_kl            | 0.007667876 |
|    clip_fraction        | 0.0585      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.87       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.5         |
|    n_updates            | 5140        |
|    policy_gradient_loss | 0.00146     |
|    std                  | 0.895       |
|    value_loss           | 9.02        |
-----------------------------------------
Eval num_timesteps=2110000, episode_reward=-405.33 +/- 192.33
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -405        |
| time/                   |             |
|    total_timesteps      | 2110000     |
| train/                  |             |
|    approx_kl            | 0.003856666 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0669      |
|    n_updates            | 5150        |
|    policy_gradient_loss | -0.0022     |
|    std                  | 0.895       |
|    value_loss           | 0.0779      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 183     |
|    iterations      | 516     |
|    time_elapsed    | 11501   |
|    total_timesteps | 2113536 |
--------------------------------
Eval num_timesteps=2115000, episode_reward=-412.20 +/- 91.85
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -412        |
| time/                   |             |
|    total_timesteps      | 2115000     |
| train/                  |             |
|    approx_kl            | 0.005135882 |
|    clip_fraction        | 0.0461      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.88       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0404      |
|    n_updates            | 5160        |
|    policy_gradient_loss | -0.00347    |
|    std                  | 0.9         |
|    value_loss           | 4.5         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 183     |
|    iterations      | 517     |
|    time_elapsed    | 11511   |
|    total_timesteps | 2117632 |
--------------------------------
Eval num_timesteps=2120000, episode_reward=-280.77 +/- 90.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -281         |
| time/                   |              |
|    total_timesteps      | 2120000      |
| train/                  |              |
|    approx_kl            | 0.0054568266 |
|    clip_fraction        | 0.0364       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.3          |
|    n_updates            | 5170         |
|    policy_gradient_loss | -0.00277     |
|    std                  | 0.908        |
|    value_loss           | 3.75         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 184     |
|    iterations      | 518     |
|    time_elapsed    | 11520   |
|    total_timesteps | 2121728 |
--------------------------------
Eval num_timesteps=2125000, episode_reward=-252.89 +/- 151.23
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -253        |
| time/                   |             |
|    total_timesteps      | 2125000     |
| train/                  |             |
|    approx_kl            | 0.003353828 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.014       |
|    n_updates            | 5180        |
|    policy_gradient_loss | -0.00182    |
|    std                  | 0.911       |
|    value_loss           | 1.43        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 184     |
|    iterations      | 519     |
|    time_elapsed    | 11530   |
|    total_timesteps | 2125824 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 184          |
|    iterations           | 520          |
|    time_elapsed         | 11537        |
|    total_timesteps      | 2129920      |
| train/                  |              |
|    approx_kl            | 0.0057655764 |
|    clip_fraction        | 0.0489       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.33         |
|    n_updates            | 5190         |
|    policy_gradient_loss | -0.00178     |
|    std                  | 0.913        |
|    value_loss           | 0.264        |
------------------------------------------
Eval num_timesteps=2130000, episode_reward=-268.05 +/- 129.14
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -268        |
| time/                   |             |
|    total_timesteps      | 2130000     |
| train/                  |             |
|    approx_kl            | 0.006937312 |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00179    |
|    n_updates            | 5200        |
|    policy_gradient_loss | -0.00139    |
|    std                  | 0.916       |
|    value_loss           | 0.0498      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 184     |
|    iterations      | 521     |
|    time_elapsed    | 11546   |
|    total_timesteps | 2134016 |
--------------------------------
Eval num_timesteps=2135000, episode_reward=-339.07 +/- 148.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -339         |
| time/                   |              |
|    total_timesteps      | 2135000      |
| train/                  |              |
|    approx_kl            | 0.0041888375 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0119       |
|    n_updates            | 5210         |
|    policy_gradient_loss | -0.00313     |
|    std                  | 0.918        |
|    value_loss           | 2.09         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 185     |
|    iterations      | 522     |
|    time_elapsed    | 11556   |
|    total_timesteps | 2138112 |
--------------------------------
Eval num_timesteps=2140000, episode_reward=-267.73 +/- 117.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -268         |
| time/                   |              |
|    total_timesteps      | 2140000      |
| train/                  |              |
|    approx_kl            | 0.0043256115 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0839       |
|    n_updates            | 5220         |
|    policy_gradient_loss | -0.00284     |
|    std                  | 0.918        |
|    value_loss           | 13           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 185     |
|    iterations      | 523     |
|    time_elapsed    | 11565   |
|    total_timesteps | 2142208 |
--------------------------------
Eval num_timesteps=2145000, episode_reward=-308.16 +/- 94.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -308         |
| time/                   |              |
|    total_timesteps      | 2145000      |
| train/                  |              |
|    approx_kl            | 0.0051819272 |
|    clip_fraction        | 0.0506       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.064        |
|    n_updates            | 5230         |
|    policy_gradient_loss | -0.0036      |
|    std                  | 0.917        |
|    value_loss           | 2.74         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 185     |
|    iterations      | 524     |
|    time_elapsed    | 11575   |
|    total_timesteps | 2146304 |
--------------------------------
Eval num_timesteps=2150000, episode_reward=-229.72 +/- 139.64
Episode length: 813.00 +/- 376.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -230         |
| time/                   |              |
|    total_timesteps      | 2150000      |
| train/                  |              |
|    approx_kl            | 0.0057126693 |
|    clip_fraction        | 0.0343       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.183        |
|    n_updates            | 5240         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 0.916        |
|    value_loss           | 18.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 185     |
|    iterations      | 525     |
|    time_elapsed    | 11583   |
|    total_timesteps | 2150400 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 185          |
|    iterations           | 526          |
|    time_elapsed         | 11590        |
|    total_timesteps      | 2154496      |
| train/                  |              |
|    approx_kl            | 0.0043396587 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0248       |
|    n_updates            | 5250         |
|    policy_gradient_loss | -0.00127     |
|    std                  | 0.912        |
|    value_loss           | 0.738        |
------------------------------------------
Eval num_timesteps=2155000, episode_reward=-269.89 +/- 186.32
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -270       |
| time/                   |            |
|    total_timesteps      | 2155000    |
| train/                  |            |
|    approx_kl            | 0.00632336 |
|    clip_fraction        | 0.0462     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.93      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0001     |
|    loss                 | -0.00385   |
|    n_updates            | 5260       |
|    policy_gradient_loss | -0.00274   |
|    std                  | 0.914      |
|    value_loss           | 0.119      |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 186     |
|    iterations      | 527     |
|    time_elapsed    | 11599   |
|    total_timesteps | 2158592 |
--------------------------------
Eval num_timesteps=2160000, episode_reward=-393.00 +/- 115.60
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -393        |
| time/                   |             |
|    total_timesteps      | 2160000     |
| train/                  |             |
|    approx_kl            | 0.004225155 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0215      |
|    n_updates            | 5270        |
|    policy_gradient_loss | -0.000832   |
|    std                  | 0.912       |
|    value_loss           | 0.14        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 186     |
|    iterations      | 528     |
|    time_elapsed    | 11609   |
|    total_timesteps | 2162688 |
--------------------------------
Eval num_timesteps=2165000, episode_reward=-366.38 +/- 202.65
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -366        |
| time/                   |             |
|    total_timesteps      | 2165000     |
| train/                  |             |
|    approx_kl            | 0.002491705 |
|    clip_fraction        | 0.0141      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.15        |
|    n_updates            | 5280        |
|    policy_gradient_loss | -0.000831   |
|    std                  | 0.91        |
|    value_loss           | 1.99        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 186     |
|    iterations      | 529     |
|    time_elapsed    | 11618   |
|    total_timesteps | 2166784 |
--------------------------------
Eval num_timesteps=2170000, episode_reward=-402.88 +/- 218.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -403         |
| time/                   |              |
|    total_timesteps      | 2170000      |
| train/                  |              |
|    approx_kl            | 0.0066743903 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.739        |
|    n_updates            | 5290         |
|    policy_gradient_loss | -0.00275     |
|    std                  | 0.912        |
|    value_loss           | 1.3          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 186     |
|    iterations      | 530     |
|    time_elapsed    | 11628   |
|    total_timesteps | 2170880 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 186         |
|    iterations           | 531         |
|    time_elapsed         | 11634       |
|    total_timesteps      | 2174976     |
| train/                  |             |
|    approx_kl            | 0.005374684 |
|    clip_fraction        | 0.0499      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0001      |
|    loss                 | 12.5        |
|    n_updates            | 5300        |
|    policy_gradient_loss | -0.00324    |
|    std                  | 0.909       |
|    value_loss           | 16          |
-----------------------------------------
Eval num_timesteps=2175000, episode_reward=-327.79 +/- 206.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -328         |
| time/                   |              |
|    total_timesteps      | 2175000      |
| train/                  |              |
|    approx_kl            | 0.0072823297 |
|    clip_fraction        | 0.0645       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0269       |
|    n_updates            | 5310         |
|    policy_gradient_loss | -0.0035      |
|    std                  | 0.912        |
|    value_loss           | 0.0942       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 187     |
|    iterations      | 532     |
|    time_elapsed    | 11644   |
|    total_timesteps | 2179072 |
--------------------------------
Eval num_timesteps=2180000, episode_reward=-292.60 +/- 114.59
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -293         |
| time/                   |              |
|    total_timesteps      | 2180000      |
| train/                  |              |
|    approx_kl            | 0.0063914955 |
|    clip_fraction        | 0.0509       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0029      |
|    n_updates            | 5320         |
|    policy_gradient_loss | -0.00279     |
|    std                  | 0.907        |
|    value_loss           | 0.179        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 187     |
|    iterations      | 533     |
|    time_elapsed    | 11653   |
|    total_timesteps | 2183168 |
--------------------------------
Eval num_timesteps=2185000, episode_reward=-314.77 +/- 147.98
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -315        |
| time/                   |             |
|    total_timesteps      | 2185000     |
| train/                  |             |
|    approx_kl            | 0.006520152 |
|    clip_fraction        | 0.0395      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.484       |
|    n_updates            | 5330        |
|    policy_gradient_loss | -0.0028     |
|    std                  | 0.907       |
|    value_loss           | 12.9        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 187     |
|    iterations      | 534     |
|    time_elapsed    | 11663   |
|    total_timesteps | 2187264 |
--------------------------------
Eval num_timesteps=2190000, episode_reward=-163.44 +/- 149.34
Episode length: 811.20 +/- 379.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -163        |
| time/                   |             |
|    total_timesteps      | 2190000     |
| train/                  |             |
|    approx_kl            | 0.004895396 |
|    clip_fraction        | 0.0364      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0238      |
|    n_updates            | 5340        |
|    policy_gradient_loss | -0.0015     |
|    std                  | 0.912       |
|    value_loss           | 0.0554      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 187     |
|    iterations      | 535     |
|    time_elapsed    | 11673   |
|    total_timesteps | 2191360 |
--------------------------------
Eval num_timesteps=2195000, episode_reward=-322.29 +/- 106.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -322        |
| time/                   |             |
|    total_timesteps      | 2195000     |
| train/                  |             |
|    approx_kl            | 0.006432195 |
|    clip_fraction        | 0.0441      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.974       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0303      |
|    n_updates            | 5350        |
|    policy_gradient_loss | -0.0028     |
|    std                  | 0.92        |
|    value_loss           | 0.14        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 187     |
|    iterations      | 536     |
|    time_elapsed    | 11682   |
|    total_timesteps | 2195456 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 188         |
|    iterations           | 537         |
|    time_elapsed         | 11688       |
|    total_timesteps      | 2199552     |
| train/                  |             |
|    approx_kl            | 0.006100294 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0798      |
|    n_updates            | 5360        |
|    policy_gradient_loss | -0.00197    |
|    std                  | 0.921       |
|    value_loss           | 1.4         |
-----------------------------------------
Eval num_timesteps=2200000, episode_reward=-274.18 +/- 115.68
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -274         |
| time/                   |              |
|    total_timesteps      | 2200000      |
| train/                  |              |
|    approx_kl            | 0.0062041953 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00515     |
|    n_updates            | 5370         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 0.923        |
|    value_loss           | 0.0222       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 188     |
|    iterations      | 538     |
|    time_elapsed    | 11697   |
|    total_timesteps | 2203648 |
--------------------------------
Eval num_timesteps=2205000, episode_reward=-323.21 +/- 91.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -323         |
| time/                   |              |
|    total_timesteps      | 2205000      |
| train/                  |              |
|    approx_kl            | 0.0042127594 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0293       |
|    n_updates            | 5380         |
|    policy_gradient_loss | -0.000629    |
|    std                  | 0.922        |
|    value_loss           | 0.631        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 188     |
|    iterations      | 539     |
|    time_elapsed    | 11706   |
|    total_timesteps | 2207744 |
--------------------------------
Eval num_timesteps=2210000, episode_reward=-366.37 +/- 182.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -366         |
| time/                   |              |
|    total_timesteps      | 2210000      |
| train/                  |              |
|    approx_kl            | 0.0039053857 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 52.4         |
|    n_updates            | 5390         |
|    policy_gradient_loss | -0.00115     |
|    std                  | 0.922        |
|    value_loss           | 8.42         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 188     |
|    iterations      | 540     |
|    time_elapsed    | 11715   |
|    total_timesteps | 2211840 |
--------------------------------
Eval num_timesteps=2215000, episode_reward=-501.24 +/- 101.40
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -501         |
| time/                   |              |
|    total_timesteps      | 2215000      |
| train/                  |              |
|    approx_kl            | 0.0070556086 |
|    clip_fraction        | 0.0663       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.96        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.203        |
|    n_updates            | 5400         |
|    policy_gradient_loss | -0.00248     |
|    std                  | 0.923        |
|    value_loss           | 4.85         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 189     |
|    iterations      | 541     |
|    time_elapsed    | 11724   |
|    total_timesteps | 2215936 |
--------------------------------
Eval num_timesteps=2220000, episode_reward=-291.42 +/- 109.14
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -291        |
| time/                   |             |
|    total_timesteps      | 2220000     |
| train/                  |             |
|    approx_kl            | 0.004534101 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.96       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.373       |
|    n_updates            | 5410        |
|    policy_gradient_loss | -0.00105    |
|    std                  | 0.923       |
|    value_loss           | 1.95        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 189     |
|    iterations      | 542     |
|    time_elapsed    | 11733   |
|    total_timesteps | 2220032 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 189         |
|    iterations           | 543         |
|    time_elapsed         | 11739       |
|    total_timesteps      | 2224128     |
| train/                  |             |
|    approx_kl            | 0.004416881 |
|    clip_fraction        | 0.0365      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0647      |
|    n_updates            | 5420        |
|    policy_gradient_loss | -0.00167    |
|    std                  | 0.919       |
|    value_loss           | 8.95        |
-----------------------------------------
Eval num_timesteps=2225000, episode_reward=-279.99 +/- 165.46
Episode length: 808.60 +/- 384.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 809          |
|    mean_reward          | -280         |
| time/                   |              |
|    total_timesteps      | 2225000      |
| train/                  |              |
|    approx_kl            | 0.0026736264 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.828        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0205       |
|    n_updates            | 5430         |
|    policy_gradient_loss | -0.000146    |
|    std                  | 0.917        |
|    value_loss           | 0.125        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 189     |
|    iterations      | 544     |
|    time_elapsed    | 11747   |
|    total_timesteps | 2228224 |
--------------------------------
Eval num_timesteps=2230000, episode_reward=-270.46 +/- 100.62
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -270        |
| time/                   |             |
|    total_timesteps      | 2230000     |
| train/                  |             |
|    approx_kl            | 0.005928505 |
|    clip_fraction        | 0.0373      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.447       |
|    n_updates            | 5440        |
|    policy_gradient_loss | -0.00357    |
|    std                  | 0.918       |
|    value_loss           | 2.71        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 189     |
|    iterations      | 545     |
|    time_elapsed    | 11756   |
|    total_timesteps | 2232320 |
--------------------------------
Eval num_timesteps=2235000, episode_reward=-362.23 +/- 126.81
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -362        |
| time/                   |             |
|    total_timesteps      | 2235000     |
| train/                  |             |
|    approx_kl            | 0.004012258 |
|    clip_fraction        | 0.0163      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.94       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.47        |
|    n_updates            | 5450        |
|    policy_gradient_loss | -0.000618   |
|    std                  | 0.918       |
|    value_loss           | 0.729       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 190     |
|    iterations      | 546     |
|    time_elapsed    | 11765   |
|    total_timesteps | 2236416 |
--------------------------------
Eval num_timesteps=2240000, episode_reward=-353.82 +/- 143.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -354         |
| time/                   |              |
|    total_timesteps      | 2240000      |
| train/                  |              |
|    approx_kl            | 0.0035650856 |
|    clip_fraction        | 0.0243       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.94        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0132       |
|    n_updates            | 5460         |
|    policy_gradient_loss | -0.00176     |
|    std                  | 0.917        |
|    value_loss           | 2.39         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 190     |
|    iterations      | 547     |
|    time_elapsed    | 11774   |
|    total_timesteps | 2240512 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 190          |
|    iterations           | 548          |
|    time_elapsed         | 11780        |
|    total_timesteps      | 2244608      |
| train/                  |              |
|    approx_kl            | 0.0043286122 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.93        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 13.7         |
|    n_updates            | 5470         |
|    policy_gradient_loss | -0.00169     |
|    std                  | 0.914        |
|    value_loss           | 3.11         |
------------------------------------------
Eval num_timesteps=2245000, episode_reward=-428.23 +/- 166.08
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -428        |
| time/                   |             |
|    total_timesteps      | 2245000     |
| train/                  |             |
|    approx_kl            | 0.004599777 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0468      |
|    n_updates            | 5480        |
|    policy_gradient_loss | -0.00056    |
|    std                  | 0.926       |
|    value_loss           | 0.0873      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 190     |
|    iterations      | 549     |
|    time_elapsed    | 11790   |
|    total_timesteps | 2248704 |
--------------------------------
Eval num_timesteps=2250000, episode_reward=-335.22 +/- 113.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -335         |
| time/                   |              |
|    total_timesteps      | 2250000      |
| train/                  |              |
|    approx_kl            | 0.0054810494 |
|    clip_fraction        | 0.0315       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.95        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00916      |
|    n_updates            | 5490         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 0.915        |
|    value_loss           | 0.0701       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 190     |
|    iterations      | 550     |
|    time_elapsed    | 11799   |
|    total_timesteps | 2252800 |
--------------------------------
Eval num_timesteps=2255000, episode_reward=-412.94 +/- 129.80
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -413        |
| time/                   |             |
|    total_timesteps      | 2255000     |
| train/                  |             |
|    approx_kl            | 0.005127631 |
|    clip_fraction        | 0.0402      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0587      |
|    n_updates            | 5500        |
|    policy_gradient_loss | -0.00306    |
|    std                  | 0.914       |
|    value_loss           | 0.448       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 191     |
|    iterations      | 551     |
|    time_elapsed    | 11808   |
|    total_timesteps | 2256896 |
--------------------------------
Eval num_timesteps=2260000, episode_reward=-311.29 +/- 135.09
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -311        |
| time/                   |             |
|    total_timesteps      | 2260000     |
| train/                  |             |
|    approx_kl            | 0.003113479 |
|    clip_fraction        | 0.033       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0406      |
|    n_updates            | 5510        |
|    policy_gradient_loss | -0.0019     |
|    std                  | 0.91        |
|    value_loss           | 1.27        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 191     |
|    iterations      | 552     |
|    time_elapsed    | 11818   |
|    total_timesteps | 2260992 |
--------------------------------
Eval num_timesteps=2265000, episode_reward=-383.45 +/- 166.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -383         |
| time/                   |              |
|    total_timesteps      | 2265000      |
| train/                  |              |
|    approx_kl            | 0.0054663103 |
|    clip_fraction        | 0.0472       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.026        |
|    n_updates            | 5520         |
|    policy_gradient_loss | -0.00286     |
|    std                  | 0.904        |
|    value_loss           | 0.0424       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 191     |
|    iterations      | 553     |
|    time_elapsed    | 11827   |
|    total_timesteps | 2265088 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 554         |
|    time_elapsed         | 11833       |
|    total_timesteps      | 2269184     |
| train/                  |             |
|    approx_kl            | 0.005674003 |
|    clip_fraction        | 0.0416      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.027       |
|    n_updates            | 5530        |
|    policy_gradient_loss | -0.00248    |
|    std                  | 0.903       |
|    value_loss           | 6.33        |
-----------------------------------------
Eval num_timesteps=2270000, episode_reward=-325.43 +/- 107.87
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -325         |
| time/                   |              |
|    total_timesteps      | 2270000      |
| train/                  |              |
|    approx_kl            | 0.0060203513 |
|    clip_fraction        | 0.0458       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0181       |
|    n_updates            | 5540         |
|    policy_gradient_loss | -0.003       |
|    std                  | 0.908        |
|    value_loss           | 0.0883       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 191     |
|    iterations      | 555     |
|    time_elapsed    | 11842   |
|    total_timesteps | 2273280 |
--------------------------------
Eval num_timesteps=2275000, episode_reward=-397.03 +/- 158.27
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -397        |
| time/                   |             |
|    total_timesteps      | 2275000     |
| train/                  |             |
|    approx_kl            | 0.003636402 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.147       |
|    n_updates            | 5550        |
|    policy_gradient_loss | -0.00241    |
|    std                  | 0.907       |
|    value_loss           | 12.4        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 192     |
|    iterations      | 556     |
|    time_elapsed    | 11850   |
|    total_timesteps | 2277376 |
--------------------------------
Eval num_timesteps=2280000, episode_reward=-435.30 +/- 119.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -435        |
| time/                   |             |
|    total_timesteps      | 2280000     |
| train/                  |             |
|    approx_kl            | 0.004480678 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.89       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0214      |
|    n_updates            | 5560        |
|    policy_gradient_loss | -0.000889   |
|    std                  | 0.905       |
|    value_loss           | 0.0539      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 192     |
|    iterations      | 557     |
|    time_elapsed    | 11859   |
|    total_timesteps | 2281472 |
--------------------------------
Eval num_timesteps=2285000, episode_reward=-369.89 +/- 88.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -370         |
| time/                   |              |
|    total_timesteps      | 2285000      |
| train/                  |              |
|    approx_kl            | 0.0076252585 |
|    clip_fraction        | 0.0719       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.89        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00422     |
|    n_updates            | 5570         |
|    policy_gradient_loss | -0.00424     |
|    std                  | 0.906        |
|    value_loss           | 0.0495       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 192     |
|    iterations      | 558     |
|    time_elapsed    | 11868   |
|    total_timesteps | 2285568 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 192         |
|    iterations           | 559         |
|    time_elapsed         | 11874       |
|    total_timesteps      | 2289664     |
| train/                  |             |
|    approx_kl            | 0.005012223 |
|    clip_fraction        | 0.0394      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0129      |
|    n_updates            | 5580        |
|    policy_gradient_loss | -0.00166    |
|    std                  | 0.909       |
|    value_loss           | 1.33        |
-----------------------------------------
Eval num_timesteps=2290000, episode_reward=-263.16 +/- 131.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -263         |
| time/                   |              |
|    total_timesteps      | 2290000      |
| train/                  |              |
|    approx_kl            | 0.0055629807 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.91        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.102        |
|    n_updates            | 5590         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 0.911        |
|    value_loss           | 1.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 193     |
|    iterations      | 560     |
|    time_elapsed    | 11883   |
|    total_timesteps | 2293760 |
--------------------------------
Eval num_timesteps=2295000, episode_reward=-390.77 +/- 164.42
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -391        |
| time/                   |             |
|    total_timesteps      | 2295000     |
| train/                  |             |
|    approx_kl            | 0.005435833 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.92       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0621      |
|    n_updates            | 5600        |
|    policy_gradient_loss | -0.0024     |
|    std                  | 0.914       |
|    value_loss           | 2.58        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 193     |
|    iterations      | 561     |
|    time_elapsed    | 11892   |
|    total_timesteps | 2297856 |
--------------------------------
Eval num_timesteps=2300000, episode_reward=-270.87 +/- 110.91
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -271        |
| time/                   |             |
|    total_timesteps      | 2300000     |
| train/                  |             |
|    approx_kl            | 0.004381071 |
|    clip_fraction        | 0.015       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.93       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.34        |
|    n_updates            | 5610        |
|    policy_gradient_loss | -0.00147    |
|    std                  | 0.914       |
|    value_loss           | 15.3        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 193     |
|    iterations      | 562     |
|    time_elapsed    | 11901   |
|    total_timesteps | 2301952 |
--------------------------------
Eval num_timesteps=2305000, episode_reward=-268.44 +/- 145.55
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -268         |
| time/                   |              |
|    total_timesteps      | 2305000      |
| train/                  |              |
|    approx_kl            | 0.0047289208 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.92        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.108        |
|    n_updates            | 5620         |
|    policy_gradient_loss | -0.00266     |
|    std                  | 0.911        |
|    value_loss           | 8.19         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 193     |
|    iterations      | 563     |
|    time_elapsed    | 11910   |
|    total_timesteps | 2306048 |
--------------------------------
Eval num_timesteps=2310000, episode_reward=-316.03 +/- 210.09
Episode length: 803.40 +/- 395.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 803         |
|    mean_reward          | -316        |
| time/                   |             |
|    total_timesteps      | 2310000     |
| train/                  |             |
|    approx_kl            | 0.004952305 |
|    clip_fraction        | 0.0491      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.91       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0119      |
|    n_updates            | 5630        |
|    policy_gradient_loss | -0.00415    |
|    std                  | 0.906       |
|    value_loss           | 0.0909      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 193     |
|    iterations      | 564     |
|    time_elapsed    | 11919   |
|    total_timesteps | 2310144 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 194         |
|    iterations           | 565         |
|    time_elapsed         | 11925       |
|    total_timesteps      | 2314240     |
| train/                  |             |
|    approx_kl            | 0.004701969 |
|    clip_fraction        | 0.0129      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.8         |
|    n_updates            | 5640        |
|    policy_gradient_loss | -0.00102    |
|    std                  | 0.906       |
|    value_loss           | 2.75        |
-----------------------------------------
Eval num_timesteps=2315000, episode_reward=-289.64 +/- 75.37
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -290        |
| time/                   |             |
|    total_timesteps      | 2315000     |
| train/                  |             |
|    approx_kl            | 0.005710018 |
|    clip_fraction        | 0.0469      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.9        |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0332      |
|    n_updates            | 5650        |
|    policy_gradient_loss | -0.00369    |
|    std                  | 0.906       |
|    value_loss           | 0.0858      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 566     |
|    time_elapsed    | 11934   |
|    total_timesteps | 2318336 |
--------------------------------
Eval num_timesteps=2320000, episode_reward=-334.39 +/- 214.12
Episode length: 813.40 +/- 375.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -334         |
| time/                   |              |
|    total_timesteps      | 2320000      |
| train/                  |              |
|    approx_kl            | 0.0074297343 |
|    clip_fraction        | 0.0671       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.9         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0386       |
|    n_updates            | 5660         |
|    policy_gradient_loss | -0.00427     |
|    std                  | 0.907        |
|    value_loss           | 1.2          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 567     |
|    time_elapsed    | 11943   |
|    total_timesteps | 2322432 |
--------------------------------
Eval num_timesteps=2325000, episode_reward=-217.31 +/- 62.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -217         |
| time/                   |              |
|    total_timesteps      | 2325000      |
| train/                  |              |
|    approx_kl            | 0.0052124206 |
|    clip_fraction        | 0.0487       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.89        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0516       |
|    n_updates            | 5670         |
|    policy_gradient_loss | -0.00266     |
|    std                  | 0.899        |
|    value_loss           | 0.0565       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 568     |
|    time_elapsed    | 11952   |
|    total_timesteps | 2326528 |
--------------------------------
Eval num_timesteps=2330000, episode_reward=-406.52 +/- 165.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -407         |
| time/                   |              |
|    total_timesteps      | 2330000      |
| train/                  |              |
|    approx_kl            | 0.0061369967 |
|    clip_fraction        | 0.0311       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.87        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.166        |
|    n_updates            | 5680         |
|    policy_gradient_loss | -0.00233     |
|    std                  | 0.895        |
|    value_loss           | 10.6         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 194     |
|    iterations      | 569     |
|    time_elapsed    | 11961   |
|    total_timesteps | 2330624 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 195          |
|    iterations           | 570          |
|    time_elapsed         | 11967        |
|    total_timesteps      | 2334720      |
| train/                  |              |
|    approx_kl            | 0.0055235364 |
|    clip_fraction        | 0.0529       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0518       |
|    n_updates            | 5690         |
|    policy_gradient_loss | -0.00172     |
|    std                  | 0.891        |
|    value_loss           | 3.77         |
------------------------------------------
Eval num_timesteps=2335000, episode_reward=-361.59 +/- 178.35
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -362         |
| time/                   |              |
|    total_timesteps      | 2335000      |
| train/                  |              |
|    approx_kl            | 0.0062662875 |
|    clip_fraction        | 0.0617       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.85        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0203      |
|    n_updates            | 5700         |
|    policy_gradient_loss | -0.00326     |
|    std                  | 0.894        |
|    value_loss           | 0.0474       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 571     |
|    time_elapsed    | 11976   |
|    total_timesteps | 2338816 |
--------------------------------
Eval num_timesteps=2340000, episode_reward=-298.28 +/- 110.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -298        |
| time/                   |             |
|    total_timesteps      | 2340000     |
| train/                  |             |
|    approx_kl            | 0.005501462 |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.85       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.29        |
|    n_updates            | 5710        |
|    policy_gradient_loss | -0.00253    |
|    std                  | 0.892       |
|    value_loss           | 1.24        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 572     |
|    time_elapsed    | 11985   |
|    total_timesteps | 2342912 |
--------------------------------
Eval num_timesteps=2345000, episode_reward=-217.24 +/- 165.63
Episode length: 812.20 +/- 377.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | -217         |
| time/                   |              |
|    total_timesteps      | 2345000      |
| train/                  |              |
|    approx_kl            | 0.0057022544 |
|    clip_fraction        | 0.0412       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.83        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0161       |
|    n_updates            | 5720         |
|    policy_gradient_loss | -0.00301     |
|    std                  | 0.882        |
|    value_loss           | 0.165        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 573     |
|    time_elapsed    | 11994   |
|    total_timesteps | 2347008 |
--------------------------------
Eval num_timesteps=2350000, episode_reward=-318.33 +/- 162.50
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -318        |
| time/                   |             |
|    total_timesteps      | 2350000     |
| train/                  |             |
|    approx_kl            | 0.003858698 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.81       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.021       |
|    n_updates            | 5730        |
|    policy_gradient_loss | -0.0018     |
|    std                  | 0.882       |
|    value_loss           | 0.365       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 195     |
|    iterations      | 574     |
|    time_elapsed    | 12003   |
|    total_timesteps | 2351104 |
--------------------------------
Eval num_timesteps=2355000, episode_reward=-218.32 +/- 147.82
Episode length: 812.80 +/- 376.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 813        |
|    mean_reward          | -218       |
| time/                   |            |
|    total_timesteps      | 2355000    |
| train/                  |            |
|    approx_kl            | 0.00476538 |
|    clip_fraction        | 0.0356     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.81      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.584      |
|    n_updates            | 5740       |
|    policy_gradient_loss | -0.0014    |
|    std                  | 0.877      |
|    value_loss           | 1.49       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 575     |
|    time_elapsed    | 12012   |
|    total_timesteps | 2355200 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 196          |
|    iterations           | 576          |
|    time_elapsed         | 12017        |
|    total_timesteps      | 2359296      |
| train/                  |              |
|    approx_kl            | 0.0064404844 |
|    clip_fraction        | 0.0287       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.79        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.897        |
|    n_updates            | 5750         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 0.875        |
|    value_loss           | 9.79         |
------------------------------------------
Eval num_timesteps=2360000, episode_reward=-276.38 +/- 184.48
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -276        |
| time/                   |             |
|    total_timesteps      | 2360000     |
| train/                  |             |
|    approx_kl            | 0.006218369 |
|    clip_fraction        | 0.0367      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.78       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0517      |
|    n_updates            | 5760        |
|    policy_gradient_loss | -0.0013     |
|    std                  | 0.87        |
|    value_loss           | 0.159       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 577     |
|    time_elapsed    | 12027   |
|    total_timesteps | 2363392 |
--------------------------------
Eval num_timesteps=2365000, episode_reward=-237.30 +/- 73.93
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -237        |
| time/                   |             |
|    total_timesteps      | 2365000     |
| train/                  |             |
|    approx_kl            | 0.006152595 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.012       |
|    n_updates            | 5770        |
|    policy_gradient_loss | -0.00338    |
|    std                  | 0.868       |
|    value_loss           | 2.39        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 578     |
|    time_elapsed    | 12036   |
|    total_timesteps | 2367488 |
--------------------------------
Eval num_timesteps=2370000, episode_reward=-298.39 +/- 119.62
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -298        |
| time/                   |             |
|    total_timesteps      | 2370000     |
| train/                  |             |
|    approx_kl            | 0.005567519 |
|    clip_fraction        | 0.048       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.77       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.118       |
|    n_updates            | 5780        |
|    policy_gradient_loss | -0.00235    |
|    std                  | 0.868       |
|    value_loss           | 0.459       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 196     |
|    iterations      | 579     |
|    time_elapsed    | 12045   |
|    total_timesteps | 2371584 |
--------------------------------
Eval num_timesteps=2375000, episode_reward=-278.01 +/- 189.95
Episode length: 813.00 +/- 376.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -278         |
| time/                   |              |
|    total_timesteps      | 2375000      |
| train/                  |              |
|    approx_kl            | 0.0046476666 |
|    clip_fraction        | 0.0471       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.77        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.83         |
|    n_updates            | 5790         |
|    policy_gradient_loss | -0.00412     |
|    std                  | 0.867        |
|    value_loss           | 3.09         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 580     |
|    time_elapsed    | 12053   |
|    total_timesteps | 2375680 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 581         |
|    time_elapsed         | 12059       |
|    total_timesteps      | 2379776     |
| train/                  |             |
|    approx_kl            | 0.005663377 |
|    clip_fraction        | 0.0424      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0131      |
|    n_updates            | 5800        |
|    policy_gradient_loss | -0.00191    |
|    std                  | 0.86        |
|    value_loss           | 0.0537      |
-----------------------------------------
Eval num_timesteps=2380000, episode_reward=-400.85 +/- 62.78
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -401        |
| time/                   |             |
|    total_timesteps      | 2380000     |
| train/                  |             |
|    approx_kl            | 0.004889735 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.75       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00145    |
|    n_updates            | 5810        |
|    policy_gradient_loss | -0.00252    |
|    std                  | 0.872       |
|    value_loss           | 0.0559      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 582     |
|    time_elapsed    | 12069   |
|    total_timesteps | 2383872 |
--------------------------------
Eval num_timesteps=2385000, episode_reward=-189.40 +/- 66.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -189         |
| time/                   |              |
|    total_timesteps      | 2385000      |
| train/                  |              |
|    approx_kl            | 0.0051162704 |
|    clip_fraction        | 0.0341       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.77        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.75         |
|    n_updates            | 5820         |
|    policy_gradient_loss | -0.00177     |
|    std                  | 0.871        |
|    value_loss           | 7.65         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 583     |
|    time_elapsed    | 12078   |
|    total_timesteps | 2387968 |
--------------------------------
Eval num_timesteps=2390000, episode_reward=-320.74 +/- 138.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -321         |
| time/                   |              |
|    total_timesteps      | 2390000      |
| train/                  |              |
|    approx_kl            | 0.0038646082 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.76        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.44         |
|    n_updates            | 5830         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 0.869        |
|    value_loss           | 2.04         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 197     |
|    iterations      | 584     |
|    time_elapsed    | 12088   |
|    total_timesteps | 2392064 |
--------------------------------
Eval num_timesteps=2395000, episode_reward=-324.52 +/- 126.37
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -325         |
| time/                   |              |
|    total_timesteps      | 2395000      |
| train/                  |              |
|    approx_kl            | 0.0074910643 |
|    clip_fraction        | 0.0665       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.77        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0534       |
|    n_updates            | 5840         |
|    policy_gradient_loss | -0.00428     |
|    std                  | 0.872        |
|    value_loss           | 0.462        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 585     |
|    time_elapsed    | 12097   |
|    total_timesteps | 2396160 |
--------------------------------
Eval num_timesteps=2400000, episode_reward=-351.62 +/- 155.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -352         |
| time/                   |              |
|    total_timesteps      | 2400000      |
| train/                  |              |
|    approx_kl            | 0.0044359686 |
|    clip_fraction        | 0.0405       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.77        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0491       |
|    n_updates            | 5850         |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.87         |
|    value_loss           | 0.213        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 586     |
|    time_elapsed    | 12106   |
|    total_timesteps | 2400256 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 198          |
|    iterations           | 587          |
|    time_elapsed         | 12112        |
|    total_timesteps      | 2404352      |
| train/                  |              |
|    approx_kl            | 0.0043322425 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.76        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.273        |
|    n_updates            | 5860         |
|    policy_gradient_loss | -0.00209     |
|    std                  | 0.866        |
|    value_loss           | 9.44         |
------------------------------------------
Eval num_timesteps=2405000, episode_reward=-267.96 +/- 113.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -268        |
| time/                   |             |
|    total_timesteps      | 2405000     |
| train/                  |             |
|    approx_kl            | 0.005511467 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.74       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0137      |
|    n_updates            | 5870        |
|    policy_gradient_loss | -0.00269    |
|    std                  | 0.858       |
|    value_loss           | 0.0366      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 588     |
|    time_elapsed    | 12121   |
|    total_timesteps | 2408448 |
--------------------------------
Eval num_timesteps=2410000, episode_reward=-274.48 +/- 190.92
Episode length: 814.60 +/- 372.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -274         |
| time/                   |              |
|    total_timesteps      | 2410000      |
| train/                  |              |
|    approx_kl            | 0.0054370277 |
|    clip_fraction        | 0.0271       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.72        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.264        |
|    n_updates            | 5880         |
|    policy_gradient_loss | -0.000939    |
|    std                  | 0.856        |
|    value_loss           | 0.428        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 198     |
|    iterations      | 589     |
|    time_elapsed    | 12129   |
|    total_timesteps | 2412544 |
--------------------------------
Eval num_timesteps=2415000, episode_reward=-448.56 +/- 114.15
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -449         |
| time/                   |              |
|    total_timesteps      | 2415000      |
| train/                  |              |
|    approx_kl            | 0.0031778896 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.73        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0293       |
|    n_updates            | 5890         |
|    policy_gradient_loss | -0.00155     |
|    std                  | 0.86         |
|    value_loss           | 0.714        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 590     |
|    time_elapsed    | 12138   |
|    total_timesteps | 2416640 |
--------------------------------
Eval num_timesteps=2420000, episode_reward=-283.16 +/- 85.30
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -283        |
| time/                   |             |
|    total_timesteps      | 2420000     |
| train/                  |             |
|    approx_kl            | 0.004419261 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0783      |
|    n_updates            | 5900        |
|    policy_gradient_loss | -0.0014     |
|    std                  | 0.859       |
|    value_loss           | 1.52        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 591     |
|    time_elapsed    | 12147   |
|    total_timesteps | 2420736 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 199          |
|    iterations           | 592          |
|    time_elapsed         | 12153        |
|    total_timesteps      | 2424832      |
| train/                  |              |
|    approx_kl            | 0.0054204664 |
|    clip_fraction        | 0.0448       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.72        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0131      |
|    n_updates            | 5910         |
|    policy_gradient_loss | -0.00184     |
|    std                  | 0.856        |
|    value_loss           | 0.0431       |
------------------------------------------
Eval num_timesteps=2425000, episode_reward=-275.61 +/- 84.01
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -276        |
| time/                   |             |
|    total_timesteps      | 2425000     |
| train/                  |             |
|    approx_kl            | 0.004819492 |
|    clip_fraction        | 0.0284      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.72       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0265      |
|    n_updates            | 5920        |
|    policy_gradient_loss | -0.0013     |
|    std                  | 0.857       |
|    value_loss           | 0.0784      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 593     |
|    time_elapsed    | 12162   |
|    total_timesteps | 2428928 |
--------------------------------
Eval num_timesteps=2430000, episode_reward=-367.54 +/- 220.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -368         |
| time/                   |              |
|    total_timesteps      | 2430000      |
| train/                  |              |
|    approx_kl            | 0.0056243967 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.72        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.15         |
|    n_updates            | 5930         |
|    policy_gradient_loss | -0.00222     |
|    std                  | 0.856        |
|    value_loss           | 1.49         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 199     |
|    iterations      | 594     |
|    time_elapsed    | 12171   |
|    total_timesteps | 2433024 |
--------------------------------
Eval num_timesteps=2435000, episode_reward=-329.62 +/- 205.51
Episode length: 813.60 +/- 374.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -330         |
| time/                   |              |
|    total_timesteps      | 2435000      |
| train/                  |              |
|    approx_kl            | 0.0060901237 |
|    clip_fraction        | 0.044        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.72        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.63         |
|    n_updates            | 5940         |
|    policy_gradient_loss | -0.00259     |
|    std                  | 0.855        |
|    value_loss           | 17.6         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 595     |
|    time_elapsed    | 12180   |
|    total_timesteps | 2437120 |
--------------------------------
Eval num_timesteps=2440000, episode_reward=-190.01 +/- 72.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -190        |
| time/                   |             |
|    total_timesteps      | 2440000     |
| train/                  |             |
|    approx_kl            | 0.005318901 |
|    clip_fraction        | 0.0305      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.199       |
|    n_updates            | 5950        |
|    policy_gradient_loss | -0.000383   |
|    std                  | 0.85        |
|    value_loss           | 12          |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 596     |
|    time_elapsed    | 12189   |
|    total_timesteps | 2441216 |
--------------------------------
Eval num_timesteps=2445000, episode_reward=-276.90 +/- 234.25
Episode length: 615.80 +/- 471.78
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 616          |
|    mean_reward          | -277         |
| time/                   |              |
|    total_timesteps      | 2445000      |
| train/                  |              |
|    approx_kl            | 0.0042440863 |
|    clip_fraction        | 0.0301       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.7         |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0428       |
|    n_updates            | 5960         |
|    policy_gradient_loss | -0.00126     |
|    std                  | 0.85         |
|    value_loss           | 3.88         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 597     |
|    time_elapsed    | 12197   |
|    total_timesteps | 2445312 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 200          |
|    iterations           | 598          |
|    time_elapsed         | 12203        |
|    total_timesteps      | 2449408      |
| train/                  |              |
|    approx_kl            | 0.0046401676 |
|    clip_fraction        | 0.0226       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.7         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 16.7         |
|    n_updates            | 5970         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 0.85         |
|    value_loss           | 6.67         |
------------------------------------------
Eval num_timesteps=2450000, episode_reward=-257.89 +/- 175.91
Episode length: 811.60 +/- 378.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 812         |
|    mean_reward          | -258        |
| time/                   |             |
|    total_timesteps      | 2450000     |
| train/                  |             |
|    approx_kl            | 0.005924889 |
|    clip_fraction        | 0.0558      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.7        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00288     |
|    n_updates            | 5980        |
|    policy_gradient_loss | -0.00237    |
|    std                  | 0.853       |
|    value_loss           | 0.112       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 200     |
|    iterations      | 599     |
|    time_elapsed    | 12212   |
|    total_timesteps | 2453504 |
--------------------------------
Eval num_timesteps=2455000, episode_reward=-355.11 +/- 150.92
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -355        |
| time/                   |             |
|    total_timesteps      | 2455000     |
| train/                  |             |
|    approx_kl            | 0.004131248 |
|    clip_fraction        | 0.0311      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0212      |
|    n_updates            | 5990        |
|    policy_gradient_loss | -0.00193    |
|    std                  | 0.847       |
|    value_loss           | 0.111       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 600     |
|    time_elapsed    | 12222   |
|    total_timesteps | 2457600 |
--------------------------------
Eval num_timesteps=2460000, episode_reward=-246.57 +/- 134.03
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -247        |
| time/                   |             |
|    total_timesteps      | 2460000     |
| train/                  |             |
|    approx_kl            | 0.006466142 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.68       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.36        |
|    n_updates            | 6000        |
|    policy_gradient_loss | -0.00176    |
|    std                  | 0.847       |
|    value_loss           | 9.83        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 601     |
|    time_elapsed    | 12231   |
|    total_timesteps | 2461696 |
--------------------------------
Eval num_timesteps=2465000, episode_reward=-270.35 +/- 159.16
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -270         |
| time/                   |              |
|    total_timesteps      | 2465000      |
| train/                  |              |
|    approx_kl            | 0.0041594068 |
|    clip_fraction        | 0.0409       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.68        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.224        |
|    n_updates            | 6010         |
|    policy_gradient_loss | -0.002       |
|    std                  | 0.847        |
|    value_loss           | 15.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 602     |
|    time_elapsed    | 12240   |
|    total_timesteps | 2465792 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 201          |
|    iterations           | 603          |
|    time_elapsed         | 12246        |
|    total_timesteps      | 2469888      |
| train/                  |              |
|    approx_kl            | 0.0040188497 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.67        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0527       |
|    n_updates            | 6020         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 0.841        |
|    value_loss           | 0.181        |
------------------------------------------
Eval num_timesteps=2470000, episode_reward=-356.87 +/- 142.70
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -357         |
| time/                   |              |
|    total_timesteps      | 2470000      |
| train/                  |              |
|    approx_kl            | 0.0042725904 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.65        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0374       |
|    n_updates            | 6030         |
|    policy_gradient_loss | -0.00336     |
|    std                  | 0.832        |
|    value_loss           | 0.203        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 201     |
|    iterations      | 604     |
|    time_elapsed    | 12255   |
|    total_timesteps | 2473984 |
--------------------------------
Eval num_timesteps=2475000, episode_reward=-332.33 +/- 198.67
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -332        |
| time/                   |             |
|    total_timesteps      | 2475000     |
| train/                  |             |
|    approx_kl            | 0.005822193 |
|    clip_fraction        | 0.0464      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00639     |
|    n_updates            | 6040        |
|    policy_gradient_loss | -0.00287    |
|    std                  | 0.83        |
|    value_loss           | 0.147       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 605     |
|    time_elapsed    | 12265   |
|    total_timesteps | 2478080 |
--------------------------------
Eval num_timesteps=2480000, episode_reward=-330.72 +/- 144.54
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -331         |
| time/                   |              |
|    total_timesteps      | 2480000      |
| train/                  |              |
|    approx_kl            | 0.0055425954 |
|    clip_fraction        | 0.0461       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0384       |
|    n_updates            | 6050         |
|    policy_gradient_loss | -0.00234     |
|    std                  | 0.831        |
|    value_loss           | 0.147        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 606     |
|    time_elapsed    | 12274   |
|    total_timesteps | 2482176 |
--------------------------------
Eval num_timesteps=2485000, episode_reward=-242.74 +/- 141.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -243         |
| time/                   |              |
|    total_timesteps      | 2485000      |
| train/                  |              |
|    approx_kl            | 0.0049413103 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0691       |
|    n_updates            | 6060         |
|    policy_gradient_loss | -0.00368     |
|    std                  | 0.83         |
|    value_loss           | 0.426        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 607     |
|    time_elapsed    | 12283   |
|    total_timesteps | 2486272 |
--------------------------------
Eval num_timesteps=2490000, episode_reward=-469.20 +/- 211.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -469         |
| time/                   |              |
|    total_timesteps      | 2490000      |
| train/                  |              |
|    approx_kl            | 0.0047717844 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.6         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.012        |
|    n_updates            | 6070         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.82         |
|    value_loss           | 0.0661       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 202     |
|    iterations      | 608     |
|    time_elapsed    | 12292   |
|    total_timesteps | 2490368 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 202          |
|    iterations           | 609          |
|    time_elapsed         | 12298        |
|    total_timesteps      | 2494464      |
| train/                  |              |
|    approx_kl            | 0.0040848227 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 12.4         |
|    n_updates            | 6080         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 0.82         |
|    value_loss           | 8.95         |
------------------------------------------
Eval num_timesteps=2495000, episode_reward=-266.40 +/- 98.52
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -266        |
| time/                   |             |
|    total_timesteps      | 2495000     |
| train/                  |             |
|    approx_kl            | 0.006458271 |
|    clip_fraction        | 0.0512      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.012      |
|    n_updates            | 6090        |
|    policy_gradient_loss | -0.00297    |
|    std                  | 0.818       |
|    value_loss           | 0.0912      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 610     |
|    time_elapsed    | 12307   |
|    total_timesteps | 2498560 |
--------------------------------
Eval num_timesteps=2500000, episode_reward=-197.12 +/- 110.81
Episode length: 809.80 +/- 382.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | -197        |
| time/                   |             |
|    total_timesteps      | 2500000     |
| train/                  |             |
|    approx_kl            | 0.005134164 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.532       |
|    n_updates            | 6100        |
|    policy_gradient_loss | -0.00118    |
|    std                  | 0.817       |
|    value_loss           | 6.76        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 611     |
|    time_elapsed    | 12316   |
|    total_timesteps | 2502656 |
--------------------------------
Eval num_timesteps=2505000, episode_reward=-390.29 +/- 216.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -390         |
| time/                   |              |
|    total_timesteps      | 2505000      |
| train/                  |              |
|    approx_kl            | 0.0045178887 |
|    clip_fraction        | 0.0229       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.222        |
|    n_updates            | 6110         |
|    policy_gradient_loss | -0.00109     |
|    std                  | 0.818        |
|    value_loss           | 2.75         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 612     |
|    time_elapsed    | 12325   |
|    total_timesteps | 2506752 |
--------------------------------
Eval num_timesteps=2510000, episode_reward=-321.04 +/- 139.31
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -321        |
| time/                   |             |
|    total_timesteps      | 2510000     |
| train/                  |             |
|    approx_kl            | 0.005938082 |
|    clip_fraction        | 0.0438      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00851     |
|    n_updates            | 6120        |
|    policy_gradient_loss | -0.00279    |
|    std                  | 0.817       |
|    value_loss           | 0.731       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 613     |
|    time_elapsed    | 12334   |
|    total_timesteps | 2510848 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 203          |
|    iterations           | 614          |
|    time_elapsed         | 12341        |
|    total_timesteps      | 2514944      |
| train/                  |              |
|    approx_kl            | 0.0061978674 |
|    clip_fraction        | 0.0485       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 14.4         |
|    n_updates            | 6130         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 0.818        |
|    value_loss           | 10.3         |
------------------------------------------
Eval num_timesteps=2515000, episode_reward=-235.20 +/- 188.52
Episode length: 811.40 +/- 379.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -235        |
| time/                   |             |
|    total_timesteps      | 2515000     |
| train/                  |             |
|    approx_kl            | 0.003650389 |
|    clip_fraction        | 0.0262      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0125      |
|    n_updates            | 6140        |
|    policy_gradient_loss | -0.00117    |
|    std                  | 0.811       |
|    value_loss           | 0.0535      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 203     |
|    iterations      | 615     |
|    time_elapsed    | 12349   |
|    total_timesteps | 2519040 |
--------------------------------
Eval num_timesteps=2520000, episode_reward=-408.74 +/- 109.06
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -409         |
| time/                   |              |
|    total_timesteps      | 2520000      |
| train/                  |              |
|    approx_kl            | 0.0055926954 |
|    clip_fraction        | 0.057        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.518        |
|    n_updates            | 6150         |
|    policy_gradient_loss | -0.00249     |
|    std                  | 0.813        |
|    value_loss           | 2.85         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 616     |
|    time_elapsed    | 12360   |
|    total_timesteps | 2523136 |
--------------------------------
Eval num_timesteps=2525000, episode_reward=-233.38 +/- 161.38
Episode length: 809.00 +/- 384.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 809          |
|    mean_reward          | -233         |
| time/                   |              |
|    total_timesteps      | 2525000      |
| train/                  |              |
|    approx_kl            | 0.0057972735 |
|    clip_fraction        | 0.0475       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0374       |
|    n_updates            | 6160         |
|    policy_gradient_loss | -0.0019      |
|    std                  | 0.815        |
|    value_loss           | 0.698        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 617     |
|    time_elapsed    | 12368   |
|    total_timesteps | 2527232 |
--------------------------------
Eval num_timesteps=2530000, episode_reward=-283.73 +/- 172.57
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -284         |
| time/                   |              |
|    total_timesteps      | 2530000      |
| train/                  |              |
|    approx_kl            | 0.0047718324 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0463       |
|    n_updates            | 6170         |
|    policy_gradient_loss | -0.000372    |
|    std                  | 0.817        |
|    value_loss           | 0.142        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 618     |
|    time_elapsed    | 12377   |
|    total_timesteps | 2531328 |
--------------------------------
Eval num_timesteps=2535000, episode_reward=-290.55 +/- 75.03
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -291        |
| time/                   |             |
|    total_timesteps      | 2535000     |
| train/                  |             |
|    approx_kl            | 0.005437265 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.476       |
|    n_updates            | 6180        |
|    policy_gradient_loss | -0.00321    |
|    std                  | 0.818       |
|    value_loss           | 16.9        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 204     |
|    iterations      | 619     |
|    time_elapsed    | 12387   |
|    total_timesteps | 2535424 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 620         |
|    time_elapsed         | 12393       |
|    total_timesteps      | 2539520     |
| train/                  |             |
|    approx_kl            | 0.006082847 |
|    clip_fraction        | 0.0639      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.2         |
|    n_updates            | 6190        |
|    policy_gradient_loss | -0.00238    |
|    std                  | 0.815       |
|    value_loss           | 2.9         |
-----------------------------------------
Eval num_timesteps=2540000, episode_reward=-312.08 +/- 237.00
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -312        |
| time/                   |             |
|    total_timesteps      | 2540000     |
| train/                  |             |
|    approx_kl            | 0.007300374 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0195      |
|    n_updates            | 6200        |
|    policy_gradient_loss | -0.00386    |
|    std                  | 0.807       |
|    value_loss           | 0.0597      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 621     |
|    time_elapsed    | 12402   |
|    total_timesteps | 2543616 |
--------------------------------
Eval num_timesteps=2545000, episode_reward=-217.59 +/- 185.42
Episode length: 620.40 +/- 466.32
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 620          |
|    mean_reward          | -218         |
| time/                   |              |
|    total_timesteps      | 2545000      |
| train/                  |              |
|    approx_kl            | 0.0036826555 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0124       |
|    n_updates            | 6210         |
|    policy_gradient_loss | -0.00195     |
|    std                  | 0.808        |
|    value_loss           | 8.74         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 622     |
|    time_elapsed    | 12410   |
|    total_timesteps | 2547712 |
--------------------------------
Eval num_timesteps=2550000, episode_reward=-459.43 +/- 102.46
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -459        |
| time/                   |             |
|    total_timesteps      | 2550000     |
| train/                  |             |
|    approx_kl            | 0.006088306 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.103       |
|    n_updates            | 6220        |
|    policy_gradient_loss | -0.00274    |
|    std                  | 0.802       |
|    value_loss           | 1.72        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 623     |
|    time_elapsed    | 12419   |
|    total_timesteps | 2551808 |
--------------------------------
Eval num_timesteps=2555000, episode_reward=-338.37 +/- 109.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -338        |
| time/                   |             |
|    total_timesteps      | 2555000     |
| train/                  |             |
|    approx_kl            | 0.004985017 |
|    clip_fraction        | 0.0372      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.787       |
|    n_updates            | 6230        |
|    policy_gradient_loss | -0.00206    |
|    std                  | 0.8         |
|    value_loss           | 0.399       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 624     |
|    time_elapsed    | 12429   |
|    total_timesteps | 2555904 |
--------------------------------
Eval num_timesteps=2560000, episode_reward=-193.08 +/- 101.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -193         |
| time/                   |              |
|    total_timesteps      | 2560000      |
| train/                  |              |
|    approx_kl            | 0.0046005873 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.5         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0678       |
|    n_updates            | 6240         |
|    policy_gradient_loss | -0.00109     |
|    std                  | 0.8          |
|    value_loss           | 0.968        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 205     |
|    iterations      | 625     |
|    time_elapsed    | 12438   |
|    total_timesteps | 2560000 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 626         |
|    time_elapsed         | 12444       |
|    total_timesteps      | 2564096     |
| train/                  |             |
|    approx_kl            | 0.004401638 |
|    clip_fraction        | 0.0323      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00145     |
|    n_updates            | 6250        |
|    policy_gradient_loss | -0.000661   |
|    std                  | 0.806       |
|    value_loss           | 0.113       |
-----------------------------------------
Eval num_timesteps=2565000, episode_reward=-213.55 +/- 147.78
Episode length: 819.20 +/- 363.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 819          |
|    mean_reward          | -214         |
| time/                   |              |
|    total_timesteps      | 2565000      |
| train/                  |              |
|    approx_kl            | 0.0059322882 |
|    clip_fraction        | 0.0497       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.189        |
|    n_updates            | 6260         |
|    policy_gradient_loss | -0.00318     |
|    std                  | 0.811        |
|    value_loss           | 0.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 627     |
|    time_elapsed    | 12453   |
|    total_timesteps | 2568192 |
--------------------------------
Eval num_timesteps=2570000, episode_reward=-190.84 +/- 128.15
Episode length: 812.60 +/- 376.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -191         |
| time/                   |              |
|    total_timesteps      | 2570000      |
| train/                  |              |
|    approx_kl            | 0.0059985826 |
|    clip_fraction        | 0.043        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.137        |
|    n_updates            | 6270         |
|    policy_gradient_loss | -0.00384     |
|    std                  | 0.812        |
|    value_loss           | 1.31         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 628     |
|    time_elapsed    | 12462   |
|    total_timesteps | 2572288 |
--------------------------------
Eval num_timesteps=2575000, episode_reward=-409.35 +/- 213.64
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -409         |
| time/                   |              |
|    total_timesteps      | 2575000      |
| train/                  |              |
|    approx_kl            | 0.0033945385 |
|    clip_fraction        | 0.0303       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.98         |
|    n_updates            | 6280         |
|    policy_gradient_loss | -0.000948    |
|    std                  | 0.814        |
|    value_loss           | 3.31         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 629     |
|    time_elapsed    | 12471   |
|    total_timesteps | 2576384 |
--------------------------------
Eval num_timesteps=2580000, episode_reward=-315.54 +/- 195.28
Episode length: 811.00 +/- 380.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -316         |
| time/                   |              |
|    total_timesteps      | 2580000      |
| train/                  |              |
|    approx_kl            | 0.0039011785 |
|    clip_fraction        | 0.0257       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.52         |
|    n_updates            | 6290         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 0.81         |
|    value_loss           | 5.14         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 206     |
|    iterations      | 630     |
|    time_elapsed    | 12480   |
|    total_timesteps | 2580480 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 206          |
|    iterations           | 631          |
|    time_elapsed         | 12486        |
|    total_timesteps      | 2584576      |
| train/                  |              |
|    approx_kl            | 0.0054243486 |
|    clip_fraction        | 0.0308       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.516        |
|    n_updates            | 6300         |
|    policy_gradient_loss | -0.000425    |
|    std                  | 0.81         |
|    value_loss           | 0.549        |
------------------------------------------
Eval num_timesteps=2585000, episode_reward=-167.18 +/- 67.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -167         |
| time/                   |              |
|    total_timesteps      | 2585000      |
| train/                  |              |
|    approx_kl            | 0.0054374803 |
|    clip_fraction        | 0.0452       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00444      |
|    n_updates            | 6310         |
|    policy_gradient_loss | -0.00376     |
|    std                  | 0.821        |
|    value_loss           | 0.0939       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 632     |
|    time_elapsed    | 12495   |
|    total_timesteps | 2588672 |
--------------------------------
Eval num_timesteps=2590000, episode_reward=-218.75 +/- 119.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -219         |
| time/                   |              |
|    total_timesteps      | 2590000      |
| train/                  |              |
|    approx_kl            | 0.0045479424 |
|    clip_fraction        | 0.045        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.19         |
|    n_updates            | 6320         |
|    policy_gradient_loss | -0.00204     |
|    std                  | 0.825        |
|    value_loss           | 1.06         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 633     |
|    time_elapsed    | 12504   |
|    total_timesteps | 2592768 |
--------------------------------
Eval num_timesteps=2595000, episode_reward=-481.76 +/- 108.68
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -482        |
| time/                   |             |
|    total_timesteps      | 2595000     |
| train/                  |             |
|    approx_kl            | 0.005039714 |
|    clip_fraction        | 0.028       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0191      |
|    n_updates            | 6330        |
|    policy_gradient_loss | -0.00151    |
|    std                  | 0.823       |
|    value_loss           | 1.4         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 634     |
|    time_elapsed    | 12514   |
|    total_timesteps | 2596864 |
--------------------------------
Eval num_timesteps=2600000, episode_reward=-401.18 +/- 110.08
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -401        |
| time/                   |             |
|    total_timesteps      | 2600000     |
| train/                  |             |
|    approx_kl            | 0.005320214 |
|    clip_fraction        | 0.0425      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 13.3        |
|    n_updates            | 6340        |
|    policy_gradient_loss | -0.00256    |
|    std                  | 0.822       |
|    value_loss           | 4.31        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 635     |
|    time_elapsed    | 12523   |
|    total_timesteps | 2600960 |
--------------------------------
Eval num_timesteps=2605000, episode_reward=-300.66 +/- 137.87
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -301         |
| time/                   |              |
|    total_timesteps      | 2605000      |
| train/                  |              |
|    approx_kl            | 0.0043256087 |
|    clip_fraction        | 0.0321       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.59         |
|    n_updates            | 6350         |
|    policy_gradient_loss | -0.000489    |
|    std                  | 0.821        |
|    value_loss           | 16.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 207     |
|    iterations      | 636     |
|    time_elapsed    | 12532   |
|    total_timesteps | 2605056 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 208          |
|    iterations           | 637          |
|    time_elapsed         | 12538        |
|    total_timesteps      | 2609152      |
| train/                  |              |
|    approx_kl            | 0.0033211187 |
|    clip_fraction        | 0.0217       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.901        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0199       |
|    n_updates            | 6360         |
|    policy_gradient_loss | -0.00018     |
|    std                  | 0.822        |
|    value_loss           | 9.05         |
------------------------------------------
Eval num_timesteps=2610000, episode_reward=-321.76 +/- 101.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -322        |
| time/                   |             |
|    total_timesteps      | 2610000     |
| train/                  |             |
|    approx_kl            | 0.008161923 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0168      |
|    n_updates            | 6370        |
|    policy_gradient_loss | -0.00438    |
|    std                  | 0.822       |
|    value_loss           | 0.0334      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 638     |
|    time_elapsed    | 12547   |
|    total_timesteps | 2613248 |
--------------------------------
Eval num_timesteps=2615000, episode_reward=-368.55 +/- 52.74
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -369        |
| time/                   |             |
|    total_timesteps      | 2615000     |
| train/                  |             |
|    approx_kl            | 0.006591603 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00885     |
|    n_updates            | 6380        |
|    policy_gradient_loss | -0.0058     |
|    std                  | 0.819       |
|    value_loss           | 0.762       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 639     |
|    time_elapsed    | 12556   |
|    total_timesteps | 2617344 |
--------------------------------
Eval num_timesteps=2620000, episode_reward=-279.31 +/- 48.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -279         |
| time/                   |              |
|    total_timesteps      | 2620000      |
| train/                  |              |
|    approx_kl            | 0.0041644676 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0169       |
|    n_updates            | 6390         |
|    policy_gradient_loss | -0.000627    |
|    std                  | 0.815        |
|    value_loss           | 0.218        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 640     |
|    time_elapsed    | 12565   |
|    total_timesteps | 2621440 |
--------------------------------
Eval num_timesteps=2625000, episode_reward=-307.77 +/- 105.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -308         |
| time/                   |              |
|    total_timesteps      | 2625000      |
| train/                  |              |
|    approx_kl            | 0.0050896807 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0232       |
|    n_updates            | 6400         |
|    policy_gradient_loss | -0.00278     |
|    std                  | 0.818        |
|    value_loss           | 0.913        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 208     |
|    iterations      | 641     |
|    time_elapsed    | 12574   |
|    total_timesteps | 2625536 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 642         |
|    time_elapsed         | 12581       |
|    total_timesteps      | 2629632     |
| train/                  |             |
|    approx_kl            | 0.005627218 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0813      |
|    n_updates            | 6410        |
|    policy_gradient_loss | -0.00226    |
|    std                  | 0.818       |
|    value_loss           | 0.51        |
-----------------------------------------
Eval num_timesteps=2630000, episode_reward=-264.37 +/- 117.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -264         |
| time/                   |              |
|    total_timesteps      | 2630000      |
| train/                  |              |
|    approx_kl            | 0.0071941493 |
|    clip_fraction        | 0.0597       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0118       |
|    n_updates            | 6420         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 0.81         |
|    value_loss           | 0.0375       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 643     |
|    time_elapsed    | 12590   |
|    total_timesteps | 2633728 |
--------------------------------
Eval num_timesteps=2635000, episode_reward=-139.60 +/- 92.53
Episode length: 812.60 +/- 376.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -140         |
| time/                   |              |
|    total_timesteps      | 2635000      |
| train/                  |              |
|    approx_kl            | 0.0047763214 |
|    clip_fraction        | 0.0427       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00253     |
|    n_updates            | 6430         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 0.81         |
|    value_loss           | 0.056        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 644     |
|    time_elapsed    | 12599   |
|    total_timesteps | 2637824 |
--------------------------------
Eval num_timesteps=2640000, episode_reward=-372.43 +/- 158.12
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -372        |
| time/                   |             |
|    total_timesteps      | 2640000     |
| train/                  |             |
|    approx_kl            | 0.004116716 |
|    clip_fraction        | 0.0217      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.59        |
|    n_updates            | 6440        |
|    policy_gradient_loss | -0.00203    |
|    std                  | 0.809       |
|    value_loss           | 1.57        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 645     |
|    time_elapsed    | 12608   |
|    total_timesteps | 2641920 |
--------------------------------
Eval num_timesteps=2645000, episode_reward=-237.71 +/- 114.36
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -238        |
| time/                   |             |
|    total_timesteps      | 2645000     |
| train/                  |             |
|    approx_kl            | 0.005649655 |
|    clip_fraction        | 0.0468      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00348     |
|    n_updates            | 6450        |
|    policy_gradient_loss | -0.00239    |
|    std                  | 0.811       |
|    value_loss           | 0.188       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 646     |
|    time_elapsed    | 12617   |
|    total_timesteps | 2646016 |
--------------------------------
Eval num_timesteps=2650000, episode_reward=-349.64 +/- 124.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -350         |
| time/                   |              |
|    total_timesteps      | 2650000      |
| train/                  |              |
|    approx_kl            | 0.0053689843 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.043        |
|    n_updates            | 6460         |
|    policy_gradient_loss | -0.000777    |
|    std                  | 0.806        |
|    value_loss           | 3.1          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 209     |
|    iterations      | 647     |
|    time_elapsed    | 12627   |
|    total_timesteps | 2650112 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 210         |
|    iterations           | 648         |
|    time_elapsed         | 12634       |
|    total_timesteps      | 2654208     |
| train/                  |             |
|    approx_kl            | 0.004878902 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.5        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.8         |
|    n_updates            | 6470        |
|    policy_gradient_loss | -0.00211    |
|    std                  | 0.806       |
|    value_loss           | 2.25        |
-----------------------------------------
Eval num_timesteps=2655000, episode_reward=-244.52 +/- 128.29
Episode length: 816.40 +/- 369.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 816         |
|    mean_reward          | -245        |
| time/                   |             |
|    total_timesteps      | 2655000     |
| train/                  |             |
|    approx_kl            | 0.007272969 |
|    clip_fraction        | 0.0804      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0155      |
|    n_updates            | 6480        |
|    policy_gradient_loss | -0.00456    |
|    std                  | 0.801       |
|    value_loss           | 0.0677      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 649     |
|    time_elapsed    | 12643   |
|    total_timesteps | 2658304 |
--------------------------------
Eval num_timesteps=2660000, episode_reward=-257.11 +/- 89.19
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -257        |
| time/                   |             |
|    total_timesteps      | 2660000     |
| train/                  |             |
|    approx_kl            | 0.003560757 |
|    clip_fraction        | 0.0174      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 7.41        |
|    n_updates            | 6490        |
|    policy_gradient_loss | -0.00123    |
|    std                  | 0.802       |
|    value_loss           | 8.45        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 650     |
|    time_elapsed    | 12652   |
|    total_timesteps | 2662400 |
--------------------------------
Eval num_timesteps=2665000, episode_reward=-324.12 +/- 120.74
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -324        |
| time/                   |             |
|    total_timesteps      | 2665000     |
| train/                  |             |
|    approx_kl            | 0.007050775 |
|    clip_fraction        | 0.0502      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00215    |
|    n_updates            | 6500        |
|    policy_gradient_loss | -0.00207    |
|    std                  | 0.802       |
|    value_loss           | 2.33        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 651     |
|    time_elapsed    | 12661   |
|    total_timesteps | 2666496 |
--------------------------------
Eval num_timesteps=2670000, episode_reward=-179.29 +/- 164.37
Episode length: 620.20 +/- 466.49
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 620          |
|    mean_reward          | -179         |
| time/                   |              |
|    total_timesteps      | 2670000      |
| train/                  |              |
|    approx_kl            | 0.0042746253 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.48        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0233       |
|    n_updates            | 6510         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 0.799        |
|    value_loss           | 0.207        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 210     |
|    iterations      | 652     |
|    time_elapsed    | 12669   |
|    total_timesteps | 2670592 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 653         |
|    time_elapsed         | 12675       |
|    total_timesteps      | 2674688     |
| train/                  |             |
|    approx_kl            | 0.009111058 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0415      |
|    n_updates            | 6520        |
|    policy_gradient_loss | -0.00651    |
|    std                  | 0.797       |
|    value_loss           | 0.0994      |
-----------------------------------------
Eval num_timesteps=2675000, episode_reward=-285.76 +/- 104.23
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -286        |
| time/                   |             |
|    total_timesteps      | 2675000     |
| train/                  |             |
|    approx_kl            | 0.004667001 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0178      |
|    n_updates            | 6530        |
|    policy_gradient_loss | -0.00119    |
|    std                  | 0.8         |
|    value_loss           | 0.0441      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 654     |
|    time_elapsed    | 12684   |
|    total_timesteps | 2678784 |
--------------------------------
Eval num_timesteps=2680000, episode_reward=-379.44 +/- 73.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -379         |
| time/                   |              |
|    total_timesteps      | 2680000      |
| train/                  |              |
|    approx_kl            | 0.0052527324 |
|    clip_fraction        | 0.0369       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.49        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0202       |
|    n_updates            | 6540         |
|    policy_gradient_loss | -0.00215     |
|    std                  | 0.8          |
|    value_loss           | 0.203        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 655     |
|    time_elapsed    | 12694   |
|    total_timesteps | 2682880 |
--------------------------------
Eval num_timesteps=2685000, episode_reward=-268.24 +/- 165.47
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -268         |
| time/                   |              |
|    total_timesteps      | 2685000      |
| train/                  |              |
|    approx_kl            | 0.0071252557 |
|    clip_fraction        | 0.0688       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.49        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.16         |
|    n_updates            | 6550         |
|    policy_gradient_loss | -0.00457     |
|    std                  | 0.8          |
|    value_loss           | 6.39         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 656     |
|    time_elapsed    | 12703   |
|    total_timesteps | 2686976 |
--------------------------------
Eval num_timesteps=2690000, episode_reward=-321.61 +/- 158.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -322         |
| time/                   |              |
|    total_timesteps      | 2690000      |
| train/                  |              |
|    approx_kl            | 0.0050230785 |
|    clip_fraction        | 0.0304       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.48        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0403       |
|    n_updates            | 6560         |
|    policy_gradient_loss | -0.000496    |
|    std                  | 0.799        |
|    value_loss           | 0.129        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 657     |
|    time_elapsed    | 12712   |
|    total_timesteps | 2691072 |
--------------------------------
Eval num_timesteps=2695000, episode_reward=-450.93 +/- 120.40
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -451         |
| time/                   |              |
|    total_timesteps      | 2695000      |
| train/                  |              |
|    approx_kl            | 0.0042069335 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.49        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.345        |
|    n_updates            | 6570         |
|    policy_gradient_loss | -0.000874    |
|    std                  | 0.801        |
|    value_loss           | 0.194        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 211     |
|    iterations      | 658     |
|    time_elapsed    | 12722   |
|    total_timesteps | 2695168 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 659          |
|    time_elapsed         | 12728        |
|    total_timesteps      | 2699264      |
| train/                  |              |
|    approx_kl            | 0.0046347794 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.49        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00966     |
|    n_updates            | 6580         |
|    policy_gradient_loss | -0.00112     |
|    std                  | 0.802        |
|    value_loss           | 1.37         |
------------------------------------------
Eval num_timesteps=2700000, episode_reward=-282.87 +/- 142.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -283         |
| time/                   |              |
|    total_timesteps      | 2700000      |
| train/                  |              |
|    approx_kl            | 0.0059825927 |
|    clip_fraction        | 0.0373       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.5         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0277       |
|    n_updates            | 6590         |
|    policy_gradient_loss | -0.00245     |
|    std                  | 0.803        |
|    value_loss           | 0.0336       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 660     |
|    time_elapsed    | 12737   |
|    total_timesteps | 2703360 |
--------------------------------
Eval num_timesteps=2705000, episode_reward=-382.47 +/- 178.97
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -382         |
| time/                   |              |
|    total_timesteps      | 2705000      |
| train/                  |              |
|    approx_kl            | 0.0049596243 |
|    clip_fraction        | 0.0333       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.49        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00103     |
|    n_updates            | 6600         |
|    policy_gradient_loss | -0.000747    |
|    std                  | 0.8          |
|    value_loss           | 0.745        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 661     |
|    time_elapsed    | 12746   |
|    total_timesteps | 2707456 |
--------------------------------
Eval num_timesteps=2710000, episode_reward=-286.14 +/- 191.73
Episode length: 810.00 +/- 382.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 810          |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 2710000      |
| train/                  |              |
|    approx_kl            | 0.0056884345 |
|    clip_fraction        | 0.0602       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.48        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0173       |
|    n_updates            | 6610         |
|    policy_gradient_loss | -0.00271     |
|    std                  | 0.799        |
|    value_loss           | 0.475        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 662     |
|    time_elapsed    | 12755   |
|    total_timesteps | 2711552 |
--------------------------------
Eval num_timesteps=2715000, episode_reward=-352.25 +/- 180.68
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -352         |
| time/                   |              |
|    total_timesteps      | 2715000      |
| train/                  |              |
|    approx_kl            | 0.0057851234 |
|    clip_fraction        | 0.0468       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.48        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0832       |
|    n_updates            | 6620         |
|    policy_gradient_loss | -0.00268     |
|    std                  | 0.799        |
|    value_loss           | 4.17         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 212     |
|    iterations      | 663     |
|    time_elapsed    | 12765   |
|    total_timesteps | 2715648 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 212          |
|    iterations           | 664          |
|    time_elapsed         | 12772        |
|    total_timesteps      | 2719744      |
| train/                  |              |
|    approx_kl            | 0.0049434067 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.48        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.318        |
|    n_updates            | 6630         |
|    policy_gradient_loss | -0.00198     |
|    std                  | 0.799        |
|    value_loss           | 0.938        |
------------------------------------------
Eval num_timesteps=2720000, episode_reward=-414.58 +/- 97.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -415         |
| time/                   |              |
|    total_timesteps      | 2720000      |
| train/                  |              |
|    approx_kl            | 0.0055880984 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.47        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0138       |
|    n_updates            | 6640         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 0.797        |
|    value_loss           | 0.105        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 665     |
|    time_elapsed    | 12781   |
|    total_timesteps | 2723840 |
--------------------------------
Eval num_timesteps=2725000, episode_reward=-354.45 +/- 181.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -354         |
| time/                   |              |
|    total_timesteps      | 2725000      |
| train/                  |              |
|    approx_kl            | 0.0036053183 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.46        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0189       |
|    n_updates            | 6650         |
|    policy_gradient_loss | -0.00216     |
|    std                  | 0.795        |
|    value_loss           | 4.54         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 666     |
|    time_elapsed    | 12790   |
|    total_timesteps | 2727936 |
--------------------------------
Eval num_timesteps=2730000, episode_reward=-427.87 +/- 115.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -428         |
| time/                   |              |
|    total_timesteps      | 2730000      |
| train/                  |              |
|    approx_kl            | 0.0077912016 |
|    clip_fraction        | 0.086        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.46        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.33         |
|    n_updates            | 6660         |
|    policy_gradient_loss | -0.00396     |
|    std                  | 0.796        |
|    value_loss           | 4.52         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 667     |
|    time_elapsed    | 12800   |
|    total_timesteps | 2732032 |
--------------------------------
Eval num_timesteps=2735000, episode_reward=-468.38 +/- 96.22
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -468        |
| time/                   |             |
|    total_timesteps      | 2735000     |
| train/                  |             |
|    approx_kl            | 0.006209489 |
|    clip_fraction        | 0.0415      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0703      |
|    n_updates            | 6670        |
|    policy_gradient_loss | -0.00125    |
|    std                  | 0.793       |
|    value_loss           | 6.24        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 668     |
|    time_elapsed    | 12809   |
|    total_timesteps | 2736128 |
--------------------------------
Eval num_timesteps=2740000, episode_reward=-475.21 +/- 136.67
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -475        |
| time/                   |             |
|    total_timesteps      | 2740000     |
| train/                  |             |
|    approx_kl            | 0.006766403 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.229       |
|    n_updates            | 6680        |
|    policy_gradient_loss | -0.00178    |
|    std                  | 0.791       |
|    value_loss           | 1.47        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 213     |
|    iterations      | 669     |
|    time_elapsed    | 12820   |
|    total_timesteps | 2740224 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 670         |
|    time_elapsed         | 12827       |
|    total_timesteps      | 2744320     |
| train/                  |             |
|    approx_kl            | 0.004650567 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.43       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.057       |
|    n_updates            | 6690        |
|    policy_gradient_loss | -0.00276    |
|    std                  | 0.787       |
|    value_loss           | 1.43        |
-----------------------------------------
Eval num_timesteps=2745000, episode_reward=-392.33 +/- 95.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -392         |
| time/                   |              |
|    total_timesteps      | 2745000      |
| train/                  |              |
|    approx_kl            | 0.0029997302 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.42        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00539      |
|    n_updates            | 6700         |
|    policy_gradient_loss | -4.09e-05    |
|    std                  | 0.786        |
|    value_loss           | 0.128        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 671     |
|    time_elapsed    | 12837   |
|    total_timesteps | 2748416 |
--------------------------------
Eval num_timesteps=2750000, episode_reward=-299.21 +/- 166.34
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -299        |
| time/                   |             |
|    total_timesteps      | 2750000     |
| train/                  |             |
|    approx_kl            | 0.005728809 |
|    clip_fraction        | 0.0517      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.04        |
|    n_updates            | 6710        |
|    policy_gradient_loss | -0.00212    |
|    std                  | 0.786       |
|    value_loss           | 0.649       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 672     |
|    time_elapsed    | 12850   |
|    total_timesteps | 2752512 |
--------------------------------
Eval num_timesteps=2755000, episode_reward=-356.44 +/- 182.12
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -356         |
| time/                   |              |
|    total_timesteps      | 2755000      |
| train/                  |              |
|    approx_kl            | 0.0053147417 |
|    clip_fraction        | 0.049        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.42        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0511       |
|    n_updates            | 6720         |
|    policy_gradient_loss | -0.00275     |
|    std                  | 0.788        |
|    value_loss           | 0.35         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 673     |
|    time_elapsed    | 12870   |
|    total_timesteps | 2756608 |
--------------------------------
Eval num_timesteps=2760000, episode_reward=-149.60 +/- 155.97
Episode length: 627.20 +/- 458.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 627         |
|    mean_reward          | -150        |
| time/                   |             |
|    total_timesteps      | 2760000     |
| train/                  |             |
|    approx_kl            | 0.006206932 |
|    clip_fraction        | 0.0788      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0292      |
|    n_updates            | 6730        |
|    policy_gradient_loss | -0.00579    |
|    std                  | 0.789       |
|    value_loss           | 4.75        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 674     |
|    time_elapsed    | 12881   |
|    total_timesteps | 2760704 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 214          |
|    iterations           | 675          |
|    time_elapsed         | 12888        |
|    total_timesteps      | 2764800      |
| train/                  |              |
|    approx_kl            | 0.0047069257 |
|    clip_fraction        | 0.037        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.43        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 6740         |
|    policy_gradient_loss | -0.00113     |
|    std                  | 0.788        |
|    value_loss           | 1.34         |
------------------------------------------
Eval num_timesteps=2765000, episode_reward=-345.24 +/- 156.60
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -345         |
| time/                   |              |
|    total_timesteps      | 2765000      |
| train/                  |              |
|    approx_kl            | 0.0042172996 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.43        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.068        |
|    n_updates            | 6750         |
|    policy_gradient_loss | -0.000471    |
|    std                  | 0.792        |
|    value_loss           | 0.233        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 676     |
|    time_elapsed    | 12897   |
|    total_timesteps | 2768896 |
--------------------------------
Eval num_timesteps=2770000, episode_reward=-346.56 +/- 208.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -347         |
| time/                   |              |
|    total_timesteps      | 2770000      |
| train/                  |              |
|    approx_kl            | 0.0060325777 |
|    clip_fraction        | 0.0538       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.42        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.163        |
|    n_updates            | 6760         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 0.784        |
|    value_loss           | 0.324        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 677     |
|    time_elapsed    | 12908   |
|    total_timesteps | 2772992 |
--------------------------------
Eval num_timesteps=2775000, episode_reward=-311.63 +/- 106.55
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -312        |
| time/                   |             |
|    total_timesteps      | 2775000     |
| train/                  |             |
|    approx_kl            | 0.004651715 |
|    clip_fraction        | 0.0242      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.29        |
|    n_updates            | 6770        |
|    policy_gradient_loss | -0.000529   |
|    std                  | 0.781       |
|    value_loss           | 8.41        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 214     |
|    iterations      | 678     |
|    time_elapsed    | 12917   |
|    total_timesteps | 2777088 |
--------------------------------
Eval num_timesteps=2780000, episode_reward=-391.16 +/- 189.62
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -391        |
| time/                   |             |
|    total_timesteps      | 2780000     |
| train/                  |             |
|    approx_kl            | 0.005410052 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 14.6        |
|    n_updates            | 6780        |
|    policy_gradient_loss | -0.00296    |
|    std                  | 0.781       |
|    value_loss           | 5.45        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 679     |
|    time_elapsed    | 12928   |
|    total_timesteps | 2781184 |
--------------------------------
Eval num_timesteps=2785000, episode_reward=-344.59 +/- 128.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -345         |
| time/                   |              |
|    total_timesteps      | 2785000      |
| train/                  |              |
|    approx_kl            | 0.0052014478 |
|    clip_fraction        | 0.0315       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.4         |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.03         |
|    n_updates            | 6790         |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.779        |
|    value_loss           | 7.2          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 680     |
|    time_elapsed    | 12939   |
|    total_timesteps | 2785280 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 215          |
|    iterations           | 681          |
|    time_elapsed         | 12946        |
|    total_timesteps      | 2789376      |
| train/                  |              |
|    approx_kl            | 0.0069018253 |
|    clip_fraction        | 0.0547       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.39        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0375       |
|    n_updates            | 6800         |
|    policy_gradient_loss | -0.00138     |
|    std                  | 0.774        |
|    value_loss           | 0.553        |
------------------------------------------
Eval num_timesteps=2790000, episode_reward=-306.79 +/- 93.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -307         |
| time/                   |              |
|    total_timesteps      | 2790000      |
| train/                  |              |
|    approx_kl            | 0.0058831535 |
|    clip_fraction        | 0.0483       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.39        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00304     |
|    n_updates            | 6810         |
|    policy_gradient_loss | -0.0021      |
|    std                  | 0.78         |
|    value_loss           | 0.087        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 682     |
|    time_elapsed    | 12956   |
|    total_timesteps | 2793472 |
--------------------------------
Eval num_timesteps=2795000, episode_reward=-220.41 +/- 76.59
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -220        |
| time/                   |             |
|    total_timesteps      | 2795000     |
| train/                  |             |
|    approx_kl            | 0.004032233 |
|    clip_fraction        | 0.0291      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.126       |
|    n_updates            | 6820        |
|    policy_gradient_loss | -0.00232    |
|    std                  | 0.781       |
|    value_loss           | 2.04        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 683     |
|    time_elapsed    | 12966   |
|    total_timesteps | 2797568 |
--------------------------------
Eval num_timesteps=2800000, episode_reward=-404.27 +/- 128.89
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -404        |
| time/                   |             |
|    total_timesteps      | 2800000     |
| train/                  |             |
|    approx_kl            | 0.006886086 |
|    clip_fraction        | 0.0682      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0653      |
|    n_updates            | 6830        |
|    policy_gradient_loss | -0.00381    |
|    std                  | 0.78        |
|    value_loss           | 5.92        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 215     |
|    iterations      | 684     |
|    time_elapsed    | 12976   |
|    total_timesteps | 2801664 |
--------------------------------
Eval num_timesteps=2805000, episode_reward=-277.78 +/- 150.75
Episode length: 801.20 +/- 399.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 801         |
|    mean_reward          | -278        |
| time/                   |             |
|    total_timesteps      | 2805000     |
| train/                  |             |
|    approx_kl            | 0.002927511 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0941      |
|    n_updates            | 6840        |
|    policy_gradient_loss | -0.00156    |
|    std                  | 0.779       |
|    value_loss           | 3.47        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 685     |
|    time_elapsed    | 12987   |
|    total_timesteps | 2805760 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 216          |
|    iterations           | 686          |
|    time_elapsed         | 12996        |
|    total_timesteps      | 2809856      |
| train/                  |              |
|    approx_kl            | 0.0057612117 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.41        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.18         |
|    n_updates            | 6850         |
|    policy_gradient_loss | -0.00239     |
|    std                  | 0.784        |
|    value_loss           | 0.839        |
------------------------------------------
Eval num_timesteps=2810000, episode_reward=-264.53 +/- 148.92
Episode length: 809.60 +/- 382.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | -265        |
| time/                   |             |
|    total_timesteps      | 2810000     |
| train/                  |             |
|    approx_kl            | 0.006486469 |
|    clip_fraction        | 0.0774      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.43       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00746     |
|    n_updates            | 6860        |
|    policy_gradient_loss | -0.00419    |
|    std                  | 0.79        |
|    value_loss           | 0.225       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 687     |
|    time_elapsed    | 13008   |
|    total_timesteps | 2813952 |
--------------------------------
Eval num_timesteps=2815000, episode_reward=-330.62 +/- 119.00
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -331         |
| time/                   |              |
|    total_timesteps      | 2815000      |
| train/                  |              |
|    approx_kl            | 0.0075681056 |
|    clip_fraction        | 0.0592       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.44        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.02         |
|    n_updates            | 6870         |
|    policy_gradient_loss | -0.00295     |
|    std                  | 0.791        |
|    value_loss           | 8.66         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 688     |
|    time_elapsed    | 13020   |
|    total_timesteps | 2818048 |
--------------------------------
Eval num_timesteps=2820000, episode_reward=-346.22 +/- 200.54
Episode length: 821.80 +/- 358.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 822         |
|    mean_reward          | -346        |
| time/                   |             |
|    total_timesteps      | 2820000     |
| train/                  |             |
|    approx_kl            | 0.004735017 |
|    clip_fraction        | 0.0362      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0412      |
|    n_updates            | 6880        |
|    policy_gradient_loss | -0.000761   |
|    std                  | 0.794       |
|    value_loss           | 8.06        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 689     |
|    time_elapsed    | 13034   |
|    total_timesteps | 2822144 |
--------------------------------
Eval num_timesteps=2825000, episode_reward=-312.68 +/- 101.53
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -313        |
| time/                   |             |
|    total_timesteps      | 2825000     |
| train/                  |             |
|    approx_kl            | 0.004314928 |
|    clip_fraction        | 0.0315      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0596      |
|    n_updates            | 6890        |
|    policy_gradient_loss | 8.16e-05    |
|    std                  | 0.793       |
|    value_loss           | 0.973       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 690     |
|    time_elapsed    | 13046   |
|    total_timesteps | 2826240 |
--------------------------------
Eval num_timesteps=2830000, episode_reward=-306.14 +/- 154.12
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -306         |
| time/                   |              |
|    total_timesteps      | 2830000      |
| train/                  |              |
|    approx_kl            | 0.0047328724 |
|    clip_fraction        | 0.0527       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.46        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0877       |
|    n_updates            | 6900         |
|    policy_gradient_loss | -0.00276     |
|    std                  | 0.797        |
|    value_loss           | 1.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 216     |
|    iterations      | 691     |
|    time_elapsed    | 13056   |
|    total_timesteps | 2830336 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 216          |
|    iterations           | 692          |
|    time_elapsed         | 13062        |
|    total_timesteps      | 2834432      |
| train/                  |              |
|    approx_kl            | 0.0060965065 |
|    clip_fraction        | 0.0469       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.46        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.000978     |
|    n_updates            | 6910         |
|    policy_gradient_loss | -0.00173     |
|    std                  | 0.796        |
|    value_loss           | 0.14         |
------------------------------------------
Eval num_timesteps=2835000, episode_reward=-215.40 +/- 134.20
Episode length: 808.00 +/- 386.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 808         |
|    mean_reward          | -215        |
| time/                   |             |
|    total_timesteps      | 2835000     |
| train/                  |             |
|    approx_kl            | 0.008570783 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0122      |
|    n_updates            | 6920        |
|    policy_gradient_loss | -0.0074     |
|    std                  | 0.793       |
|    value_loss           | 0.076       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 693     |
|    time_elapsed    | 13071   |
|    total_timesteps | 2838528 |
--------------------------------
Eval num_timesteps=2840000, episode_reward=-216.44 +/- 43.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -216         |
| time/                   |              |
|    total_timesteps      | 2840000      |
| train/                  |              |
|    approx_kl            | 0.0038219686 |
|    clip_fraction        | 0.0299       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.44        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0276       |
|    n_updates            | 6930         |
|    policy_gradient_loss | -0.00133     |
|    std                  | 0.789        |
|    value_loss           | 0.251        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 694     |
|    time_elapsed    | 13081   |
|    total_timesteps | 2842624 |
--------------------------------
Eval num_timesteps=2845000, episode_reward=-359.86 +/- 196.21
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -360        |
| time/                   |             |
|    total_timesteps      | 2845000     |
| train/                  |             |
|    approx_kl            | 0.004039201 |
|    clip_fraction        | 0.0253      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0001      |
|    loss                 | 4.23        |
|    n_updates            | 6940        |
|    policy_gradient_loss | -0.00119    |
|    std                  | 0.789       |
|    value_loss           | 4.4         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 695     |
|    time_elapsed    | 13091   |
|    total_timesteps | 2846720 |
--------------------------------
Eval num_timesteps=2850000, episode_reward=-269.50 +/- 185.67
Episode length: 812.40 +/- 377.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | -270         |
| time/                   |              |
|    total_timesteps      | 2850000      |
| train/                  |              |
|    approx_kl            | 0.0074700555 |
|    clip_fraction        | 0.108        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.44        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0333       |
|    n_updates            | 6950         |
|    policy_gradient_loss | -0.00722     |
|    std                  | 0.788        |
|    value_loss           | 0.314        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 696     |
|    time_elapsed    | 13100   |
|    total_timesteps | 2850816 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 697         |
|    time_elapsed         | 13106       |
|    total_timesteps      | 2854912     |
| train/                  |             |
|    approx_kl            | 0.005154623 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0001      |
|    loss                 | 48.2        |
|    n_updates            | 6960        |
|    policy_gradient_loss | -0.00511    |
|    std                  | 0.789       |
|    value_loss           | 17          |
-----------------------------------------
Eval num_timesteps=2855000, episode_reward=-365.34 +/- 176.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 2855000      |
| train/                  |              |
|    approx_kl            | 0.0058306227 |
|    clip_fraction        | 0.0601       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.45        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00655     |
|    n_updates            | 6970         |
|    policy_gradient_loss | -0.00318     |
|    std                  | 0.796        |
|    value_loss           | 0.0179       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 217     |
|    iterations      | 698     |
|    time_elapsed    | 13116   |
|    total_timesteps | 2859008 |
--------------------------------
Eval num_timesteps=2860000, episode_reward=-244.24 +/- 89.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -244         |
| time/                   |              |
|    total_timesteps      | 2860000      |
| train/                  |              |
|    approx_kl            | 0.0073861307 |
|    clip_fraction        | 0.0679       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.46        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00525      |
|    n_updates            | 6980         |
|    policy_gradient_loss | -0.00353     |
|    std                  | 0.794        |
|    value_loss           | 0.147        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 699     |
|    time_elapsed    | 13126   |
|    total_timesteps | 2863104 |
--------------------------------
Eval num_timesteps=2865000, episode_reward=-247.67 +/- 121.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -248         |
| time/                   |              |
|    total_timesteps      | 2865000      |
| train/                  |              |
|    approx_kl            | 0.0045442875 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.45        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 3.83         |
|    n_updates            | 6990         |
|    policy_gradient_loss | -0.00172     |
|    std                  | 0.794        |
|    value_loss           | 8.17         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 700     |
|    time_elapsed    | 13136   |
|    total_timesteps | 2867200 |
--------------------------------
Eval num_timesteps=2870000, episode_reward=-196.54 +/- 41.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -197         |
| time/                   |              |
|    total_timesteps      | 2870000      |
| train/                  |              |
|    approx_kl            | 0.0062200194 |
|    clip_fraction        | 0.0536       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.45        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0105       |
|    n_updates            | 7000         |
|    policy_gradient_loss | -0.00225     |
|    std                  | 0.793        |
|    value_loss           | 1.53         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 701     |
|    time_elapsed    | 13147   |
|    total_timesteps | 2871296 |
--------------------------------
Eval num_timesteps=2875000, episode_reward=-413.69 +/- 121.70
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -414         |
| time/                   |              |
|    total_timesteps      | 2875000      |
| train/                  |              |
|    approx_kl            | 0.0048776343 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.45        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.121        |
|    n_updates            | 7010         |
|    policy_gradient_loss | -0.00247     |
|    std                  | 0.796        |
|    value_loss           | 1.48         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 702     |
|    time_elapsed    | 13161   |
|    total_timesteps | 2875392 |
--------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 703        |
|    time_elapsed         | 13167      |
|    total_timesteps      | 2879488    |
| train/                  |            |
|    approx_kl            | 0.00632658 |
|    clip_fraction        | 0.0509     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.47      |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0001     |
|    loss                 | 8.58       |
|    n_updates            | 7020       |
|    policy_gradient_loss | -0.0013    |
|    std                  | 0.803      |
|    value_loss           | 2.43       |
----------------------------------------
Eval num_timesteps=2880000, episode_reward=-379.36 +/- 156.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -379         |
| time/                   |              |
|    total_timesteps      | 2880000      |
| train/                  |              |
|    approx_kl            | 0.0051805964 |
|    clip_fraction        | 0.0444       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.49        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00158      |
|    n_updates            | 7030         |
|    policy_gradient_loss | -0.00237     |
|    std                  | 0.811        |
|    value_loss           | 0.0742       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 704     |
|    time_elapsed    | 13178   |
|    total_timesteps | 2883584 |
--------------------------------
Eval num_timesteps=2885000, episode_reward=-239.95 +/- 187.99
Episode length: 813.40 +/- 375.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -240         |
| time/                   |              |
|    total_timesteps      | 2885000      |
| train/                  |              |
|    approx_kl            | 0.0053506494 |
|    clip_fraction        | 0.0345       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.5         |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0602       |
|    n_updates            | 7040         |
|    policy_gradient_loss | -0.00243     |
|    std                  | 0.812        |
|    value_loss           | 1.88         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 218     |
|    iterations      | 705     |
|    time_elapsed    | 13187   |
|    total_timesteps | 2887680 |
--------------------------------
Eval num_timesteps=2890000, episode_reward=-255.16 +/- 43.00
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -255         |
| time/                   |              |
|    total_timesteps      | 2890000      |
| train/                  |              |
|    approx_kl            | 0.0046897256 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.202        |
|    n_updates            | 7050         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 0.814        |
|    value_loss           | 0.616        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 706     |
|    time_elapsed    | 13197   |
|    total_timesteps | 2891776 |
--------------------------------
Eval num_timesteps=2895000, episode_reward=-312.97 +/- 113.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -313        |
| time/                   |             |
|    total_timesteps      | 2895000     |
| train/                  |             |
|    approx_kl            | 0.004942634 |
|    clip_fraction        | 0.0353      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0001      |
|    loss                 | 1.23        |
|    n_updates            | 7060        |
|    policy_gradient_loss | -0.00168    |
|    std                  | 0.813       |
|    value_loss           | 5.47        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 707     |
|    time_elapsed    | 13206   |
|    total_timesteps | 2895872 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 708         |
|    time_elapsed         | 13213       |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.010204206 |
|    clip_fraction        | 0.0971      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0001      |
|    loss                 | 24.8        |
|    n_updates            | 7070        |
|    policy_gradient_loss | -0.00467    |
|    std                  | 0.815       |
|    value_loss           | 9.62        |
-----------------------------------------
Eval num_timesteps=2900000, episode_reward=-375.09 +/- 133.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -375         |
| time/                   |              |
|    total_timesteps      | 2900000      |
| train/                  |              |
|    approx_kl            | 0.0042654453 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0438       |
|    n_updates            | 7080         |
|    policy_gradient_loss | -3.74e-05    |
|    std                  | 0.812        |
|    value_loss           | 0.0899       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 709     |
|    time_elapsed    | 13222   |
|    total_timesteps | 2904064 |
--------------------------------
Eval num_timesteps=2905000, episode_reward=-210.49 +/- 70.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -210         |
| time/                   |              |
|    total_timesteps      | 2905000      |
| train/                  |              |
|    approx_kl            | 0.0063280193 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.5         |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.13         |
|    n_updates            | 7090         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 0.812        |
|    value_loss           | 3.31         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 710     |
|    time_elapsed    | 13231   |
|    total_timesteps | 2908160 |
--------------------------------
Eval num_timesteps=2910000, episode_reward=-464.70 +/- 200.91
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -465         |
| time/                   |              |
|    total_timesteps      | 2910000      |
| train/                  |              |
|    approx_kl            | 0.0036212488 |
|    clip_fraction        | 0.0383       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.85         |
|    n_updates            | 7100         |
|    policy_gradient_loss | -0.00086     |
|    std                  | 0.813        |
|    value_loss           | 3.43         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 219     |
|    iterations      | 711     |
|    time_elapsed    | 13241   |
|    total_timesteps | 2912256 |
--------------------------------
Eval num_timesteps=2915000, episode_reward=-353.43 +/- 107.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -353         |
| time/                   |              |
|    total_timesteps      | 2915000      |
| train/                  |              |
|    approx_kl            | 0.0034714919 |
|    clip_fraction        | 0.0406       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0143       |
|    n_updates            | 7110         |
|    policy_gradient_loss | -0.00105     |
|    std                  | 0.811        |
|    value_loss           | 0.131        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 712     |
|    time_elapsed    | 13251   |
|    total_timesteps | 2916352 |
--------------------------------
Eval num_timesteps=2920000, episode_reward=-247.25 +/- 80.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -247         |
| time/                   |              |
|    total_timesteps      | 2920000      |
| train/                  |              |
|    approx_kl            | 0.0047715483 |
|    clip_fraction        | 0.0467       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0151       |
|    n_updates            | 7120         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 0.812        |
|    value_loss           | 2.21         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 713     |
|    time_elapsed    | 13261   |
|    total_timesteps | 2920448 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 220          |
|    iterations           | 714          |
|    time_elapsed         | 13269        |
|    total_timesteps      | 2924544      |
| train/                  |              |
|    approx_kl            | 0.0042512855 |
|    clip_fraction        | 0.0355       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.692        |
|    n_updates            | 7130         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 0.81         |
|    value_loss           | 6.42         |
------------------------------------------
Eval num_timesteps=2925000, episode_reward=-247.22 +/- 149.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -247         |
| time/                   |              |
|    total_timesteps      | 2925000      |
| train/                  |              |
|    approx_kl            | 0.0038913954 |
|    clip_fraction        | 0.0366       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.5         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00562      |
|    n_updates            | 7140         |
|    policy_gradient_loss | -0.00139     |
|    std                  | 0.811        |
|    value_loss           | 0.0384       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 715     |
|    time_elapsed    | 13282   |
|    total_timesteps | 2928640 |
--------------------------------
Eval num_timesteps=2930000, episode_reward=-343.83 +/- 153.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -344         |
| time/                   |              |
|    total_timesteps      | 2930000      |
| train/                  |              |
|    approx_kl            | 0.0059816977 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0599       |
|    n_updates            | 7150         |
|    policy_gradient_loss | -0.000424    |
|    std                  | 0.812        |
|    value_loss           | 17.8         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 716     |
|    time_elapsed    | 13296   |
|    total_timesteps | 2932736 |
--------------------------------
Eval num_timesteps=2935000, episode_reward=-199.51 +/- 11.05
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -200       |
| time/                   |            |
|    total_timesteps      | 2935000    |
| train/                  |            |
|    approx_kl            | 0.00493541 |
|    clip_fraction        | 0.0614     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.52      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.363      |
|    n_updates            | 7160       |
|    policy_gradient_loss | -0.00245   |
|    std                  | 0.815      |
|    value_loss           | 0.883      |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 717     |
|    time_elapsed    | 13310   |
|    total_timesteps | 2936832 |
--------------------------------
Eval num_timesteps=2940000, episode_reward=-257.31 +/- 131.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -257         |
| time/                   |              |
|    total_timesteps      | 2940000      |
| train/                  |              |
|    approx_kl            | 0.0039097928 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.014        |
|    n_updates            | 7170         |
|    policy_gradient_loss | -0.000981    |
|    std                  | 0.812        |
|    value_loss           | 2.25         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 718     |
|    time_elapsed    | 13321   |
|    total_timesteps | 2940928 |
--------------------------------
Eval num_timesteps=2945000, episode_reward=-221.51 +/- 123.52
Episode length: 812.00 +/- 378.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 812          |
|    mean_reward          | -222         |
| time/                   |              |
|    total_timesteps      | 2945000      |
| train/                  |              |
|    approx_kl            | 0.0045093885 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.796        |
|    n_updates            | 7180         |
|    policy_gradient_loss | -0.000746    |
|    std                  | 0.816        |
|    value_loss           | 11.8         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 220     |
|    iterations      | 719     |
|    time_elapsed    | 13331   |
|    total_timesteps | 2945024 |
--------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 720        |
|    time_elapsed         | 13339      |
|    total_timesteps      | 2949120    |
| train/                  |            |
|    approx_kl            | 0.00575849 |
|    clip_fraction        | 0.0558     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.53      |
|    explained_variance   | 0.96       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0111     |
|    n_updates            | 7190       |
|    policy_gradient_loss | -0.0024    |
|    std                  | 0.814      |
|    value_loss           | 9.34       |
----------------------------------------
Eval num_timesteps=2950000, episode_reward=-501.48 +/- 58.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -501         |
| time/                   |              |
|    total_timesteps      | 2950000      |
| train/                  |              |
|    approx_kl            | 0.0041025067 |
|    clip_fraction        | 0.0491       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0257       |
|    n_updates            | 7200         |
|    policy_gradient_loss | -0.00173     |
|    std                  | 0.812        |
|    value_loss           | 0.0348       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 721     |
|    time_elapsed    | 13350   |
|    total_timesteps | 2953216 |
--------------------------------
Eval num_timesteps=2955000, episode_reward=-331.67 +/- 176.77
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 2955000      |
| train/                  |              |
|    approx_kl            | 0.0075119273 |
|    clip_fraction        | 0.0477       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0341       |
|    n_updates            | 7210         |
|    policy_gradient_loss | -0.000817    |
|    std                  | 0.814        |
|    value_loss           | 0.849        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 722     |
|    time_elapsed    | 13362   |
|    total_timesteps | 2957312 |
--------------------------------
Eval num_timesteps=2960000, episode_reward=-346.09 +/- 113.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -346         |
| time/                   |              |
|    total_timesteps      | 2960000      |
| train/                  |              |
|    approx_kl            | 0.0041806772 |
|    clip_fraction        | 0.0268       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0825       |
|    n_updates            | 7220         |
|    policy_gradient_loss | -0.00134     |
|    std                  | 0.817        |
|    value_loss           | 1.41         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 723     |
|    time_elapsed    | 13372   |
|    total_timesteps | 2961408 |
--------------------------------
Eval num_timesteps=2965000, episode_reward=-408.74 +/- 84.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -409         |
| time/                   |              |
|    total_timesteps      | 2965000      |
| train/                  |              |
|    approx_kl            | 0.0064114723 |
|    clip_fraction        | 0.0556       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00617      |
|    n_updates            | 7230         |
|    policy_gradient_loss | -0.00405     |
|    std                  | 0.821        |
|    value_loss           | 0.539        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 724     |
|    time_elapsed    | 13386   |
|    total_timesteps | 2965504 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 221          |
|    iterations           | 725          |
|    time_elapsed         | 13393        |
|    total_timesteps      | 2969600      |
| train/                  |              |
|    approx_kl            | 0.0072839013 |
|    clip_fraction        | 0.0782       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0474       |
|    n_updates            | 7240         |
|    policy_gradient_loss | -0.00287     |
|    std                  | 0.819        |
|    value_loss           | 1.6          |
------------------------------------------
Eval num_timesteps=2970000, episode_reward=-349.51 +/- 151.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -350         |
| time/                   |              |
|    total_timesteps      | 2970000      |
| train/                  |              |
|    approx_kl            | 0.0041254526 |
|    clip_fraction        | 0.0381       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0197       |
|    n_updates            | 7250         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 0.825        |
|    value_loss           | 0.0335       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 726     |
|    time_elapsed    | 13406   |
|    total_timesteps | 2973696 |
--------------------------------
Eval num_timesteps=2975000, episode_reward=-462.61 +/- 139.38
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -463        |
| time/                   |             |
|    total_timesteps      | 2975000     |
| train/                  |             |
|    approx_kl            | 0.005457747 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0119      |
|    n_updates            | 7260        |
|    policy_gradient_loss | -0.00217    |
|    std                  | 0.828       |
|    value_loss           | 0.431       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 221     |
|    iterations      | 727     |
|    time_elapsed    | 13417   |
|    total_timesteps | 2977792 |
--------------------------------
Eval num_timesteps=2980000, episode_reward=-327.33 +/- 210.95
Episode length: 814.00 +/- 374.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -327         |
| time/                   |              |
|    total_timesteps      | 2980000      |
| train/                  |              |
|    approx_kl            | 0.0041882163 |
|    clip_fraction        | 0.0349       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 16.1         |
|    n_updates            | 7270         |
|    policy_gradient_loss | -0.00286     |
|    std                  | 0.828        |
|    value_loss           | 6.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 728     |
|    time_elapsed    | 13426   |
|    total_timesteps | 2981888 |
--------------------------------
Eval num_timesteps=2985000, episode_reward=-290.30 +/- 89.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -290        |
| time/                   |             |
|    total_timesteps      | 2985000     |
| train/                  |             |
|    approx_kl            | 0.007535233 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.67        |
|    n_updates            | 7280        |
|    policy_gradient_loss | -0.00268    |
|    std                  | 0.834       |
|    value_loss           | 3.87        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 729     |
|    time_elapsed    | 13437   |
|    total_timesteps | 2985984 |
--------------------------------
Eval num_timesteps=2990000, episode_reward=-266.21 +/- 159.34
Episode length: 808.00 +/- 386.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -266         |
| time/                   |              |
|    total_timesteps      | 2990000      |
| train/                  |              |
|    approx_kl            | 0.0074631553 |
|    clip_fraction        | 0.0847       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.6         |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 10.8         |
|    n_updates            | 7290         |
|    policy_gradient_loss | -0.00537     |
|    std                  | 0.836        |
|    value_loss           | 4.51         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 730     |
|    time_elapsed    | 13446   |
|    total_timesteps | 2990080 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 222          |
|    iterations           | 731          |
|    time_elapsed         | 13454        |
|    total_timesteps      | 2994176      |
| train/                  |              |
|    approx_kl            | 0.0039940467 |
|    clip_fraction        | 0.045        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.6         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0334       |
|    n_updates            | 7300         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 0.834        |
|    value_loss           | 2.35         |
------------------------------------------
Eval num_timesteps=2995000, episode_reward=-317.32 +/- 129.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -317         |
| time/                   |              |
|    total_timesteps      | 2995000      |
| train/                  |              |
|    approx_kl            | 0.0035448803 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00892      |
|    n_updates            | 7310         |
|    policy_gradient_loss | -0.000917    |
|    std                  | 0.832        |
|    value_loss           | 0.0478       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 732     |
|    time_elapsed    | 13465   |
|    total_timesteps | 2998272 |
--------------------------------
Eval num_timesteps=3000000, episode_reward=-319.00 +/- 96.43
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -319        |
| time/                   |             |
|    total_timesteps      | 3000000     |
| train/                  |             |
|    approx_kl            | 0.004225175 |
|    clip_fraction        | 0.0338      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.287       |
|    n_updates            | 7320        |
|    policy_gradient_loss | -0.00243    |
|    std                  | 0.834       |
|    value_loss           | 6.04        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 733     |
|    time_elapsed    | 13476   |
|    total_timesteps | 3002368 |
--------------------------------
Eval num_timesteps=3005000, episode_reward=-231.25 +/- 123.72
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -231         |
| time/                   |              |
|    total_timesteps      | 3005000      |
| train/                  |              |
|    approx_kl            | 0.0026118923 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0179       |
|    n_updates            | 7330         |
|    policy_gradient_loss | 0.000124     |
|    std                  | 0.838        |
|    value_loss           | 1.45         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 222     |
|    iterations      | 734     |
|    time_elapsed    | 13486   |
|    total_timesteps | 3006464 |
--------------------------------
Eval num_timesteps=3010000, episode_reward=-234.49 +/- 83.77
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -234       |
| time/                   |            |
|    total_timesteps      | 3010000    |
| train/                  |            |
|    approx_kl            | 0.00712043 |
|    clip_fraction        | 0.0732     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.61      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0768     |
|    n_updates            | 7340       |
|    policy_gradient_loss | -0.00318   |
|    std                  | 0.837      |
|    value_loss           | 2.59       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 735     |
|    time_elapsed    | 13496   |
|    total_timesteps | 3010560 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 736         |
|    time_elapsed         | 13503       |
|    total_timesteps      | 3014656     |
| train/                  |             |
|    approx_kl            | 0.004465385 |
|    clip_fraction        | 0.0192      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00242    |
|    n_updates            | 7350        |
|    policy_gradient_loss | -0.00014    |
|    std                  | 0.836       |
|    value_loss           | 0.389       |
-----------------------------------------
Eval num_timesteps=3015000, episode_reward=-494.26 +/- 140.65
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -494         |
| time/                   |              |
|    total_timesteps      | 3015000      |
| train/                  |              |
|    approx_kl            | 0.0047113374 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.033        |
|    n_updates            | 7360         |
|    policy_gradient_loss | -0.000337    |
|    std                  | 0.838        |
|    value_loss           | 0.0247       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 737     |
|    time_elapsed    | 13513   |
|    total_timesteps | 3018752 |
--------------------------------
Eval num_timesteps=3020000, episode_reward=-144.13 +/- 144.84
Episode length: 618.00 +/- 469.10
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 618          |
|    mean_reward          | -144         |
| time/                   |              |
|    total_timesteps      | 3020000      |
| train/                  |              |
|    approx_kl            | 0.0041532703 |
|    clip_fraction        | 0.0251       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0251       |
|    n_updates            | 7370         |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.839        |
|    value_loss           | 2.45         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 738     |
|    time_elapsed    | 13522   |
|    total_timesteps | 3022848 |
--------------------------------
Eval num_timesteps=3025000, episode_reward=-381.53 +/- 173.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -382        |
| time/                   |             |
|    total_timesteps      | 3025000     |
| train/                  |             |
|    approx_kl            | 0.004543972 |
|    clip_fraction        | 0.0224      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.967       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.492       |
|    n_updates            | 7380        |
|    policy_gradient_loss | -0.00159    |
|    std                  | 0.839       |
|    value_loss           | 7.73        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 739     |
|    time_elapsed    | 13532   |
|    total_timesteps | 3026944 |
--------------------------------
Eval num_timesteps=3030000, episode_reward=-273.78 +/- 99.97
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -274        |
| time/                   |             |
|    total_timesteps      | 3030000     |
| train/                  |             |
|    approx_kl            | 0.007450144 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0334      |
|    n_updates            | 7390        |
|    policy_gradient_loss | -0.00456    |
|    std                  | 0.843       |
|    value_loss           | 0.0637      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 740     |
|    time_elapsed    | 13543   |
|    total_timesteps | 3031040 |
--------------------------------
Eval num_timesteps=3035000, episode_reward=-319.45 +/- 98.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 3035000      |
| train/                  |              |
|    approx_kl            | 0.0058377534 |
|    clip_fraction        | 0.0362       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.67         |
|    n_updates            | 7400         |
|    policy_gradient_loss | -0.000702    |
|    std                  | 0.84         |
|    value_loss           | 10.1         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 223     |
|    iterations      | 741     |
|    time_elapsed    | 13553   |
|    total_timesteps | 3035136 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 224          |
|    iterations           | 742          |
|    time_elapsed         | 13559        |
|    total_timesteps      | 3039232      |
| train/                  |              |
|    approx_kl            | 0.0059976024 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0237       |
|    n_updates            | 7410         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 0.838        |
|    value_loss           | 4.35         |
------------------------------------------
Eval num_timesteps=3040000, episode_reward=-369.66 +/- 89.40
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -370        |
| time/                   |             |
|    total_timesteps      | 3040000     |
| train/                  |             |
|    approx_kl            | 0.004134124 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0217      |
|    n_updates            | 7420        |
|    policy_gradient_loss | -0.00137    |
|    std                  | 0.834       |
|    value_loss           | 0.087       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 743     |
|    time_elapsed    | 13570   |
|    total_timesteps | 3043328 |
--------------------------------
Eval num_timesteps=3045000, episode_reward=-336.65 +/- 163.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -337         |
| time/                   |              |
|    total_timesteps      | 3045000      |
| train/                  |              |
|    approx_kl            | 0.0031986935 |
|    clip_fraction        | 0.0193       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00412     |
|    n_updates            | 7430         |
|    policy_gradient_loss | 0.000382     |
|    std                  | 0.834        |
|    value_loss           | 0.555        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 744     |
|    time_elapsed    | 13581   |
|    total_timesteps | 3047424 |
--------------------------------
Eval num_timesteps=3050000, episode_reward=-490.21 +/- 142.27
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -490        |
| time/                   |             |
|    total_timesteps      | 3050000     |
| train/                  |             |
|    approx_kl            | 0.005758507 |
|    clip_fraction        | 0.0282      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 18.5        |
|    n_updates            | 7440        |
|    policy_gradient_loss | -0.00236    |
|    std                  | 0.834       |
|    value_loss           | 7.51        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 745     |
|    time_elapsed    | 13593   |
|    total_timesteps | 3051520 |
--------------------------------
Eval num_timesteps=3055000, episode_reward=-404.44 +/- 189.90
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -404         |
| time/                   |              |
|    total_timesteps      | 3055000      |
| train/                  |              |
|    approx_kl            | 0.0044302396 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 9.6          |
|    n_updates            | 7450         |
|    policy_gradient_loss | -0.00172     |
|    std                  | 0.835        |
|    value_loss           | 4.48         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 746     |
|    time_elapsed    | 13604   |
|    total_timesteps | 3055616 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 224          |
|    iterations           | 747          |
|    time_elapsed         | 13611        |
|    total_timesteps      | 3059712      |
| train/                  |              |
|    approx_kl            | 0.0055754557 |
|    clip_fraction        | 0.0535       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.6         |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.155        |
|    n_updates            | 7460         |
|    policy_gradient_loss | -0.00167     |
|    std                  | 0.835        |
|    value_loss           | 6.73         |
------------------------------------------
Eval num_timesteps=3060000, episode_reward=-339.75 +/- 164.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -340         |
| time/                   |              |
|    total_timesteps      | 3060000      |
| train/                  |              |
|    approx_kl            | 0.0056608412 |
|    clip_fraction        | 0.057        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0116       |
|    n_updates            | 7470         |
|    policy_gradient_loss | -0.00227     |
|    std                  | 0.829        |
|    value_loss           | 0.0701       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 224     |
|    iterations      | 748     |
|    time_elapsed    | 13621   |
|    total_timesteps | 3063808 |
--------------------------------
Eval num_timesteps=3065000, episode_reward=-303.48 +/- 83.40
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -303         |
| time/                   |              |
|    total_timesteps      | 3065000      |
| train/                  |              |
|    approx_kl            | 0.0051438212 |
|    clip_fraction        | 0.0388       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0254       |
|    n_updates            | 7480         |
|    policy_gradient_loss | -0.000773    |
|    std                  | 0.835        |
|    value_loss           | 0.0642       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 749     |
|    time_elapsed    | 13631   |
|    total_timesteps | 3067904 |
--------------------------------
Eval num_timesteps=3070000, episode_reward=-385.26 +/- 128.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -385         |
| time/                   |              |
|    total_timesteps      | 3070000      |
| train/                  |              |
|    approx_kl            | 0.0043064086 |
|    clip_fraction        | 0.034        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00577     |
|    n_updates            | 7490         |
|    policy_gradient_loss | -0.00238     |
|    std                  | 0.837        |
|    value_loss           | 0.193        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 750     |
|    time_elapsed    | 13641   |
|    total_timesteps | 3072000 |
--------------------------------
Eval num_timesteps=3075000, episode_reward=-257.36 +/- 67.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -257         |
| time/                   |              |
|    total_timesteps      | 3075000      |
| train/                  |              |
|    approx_kl            | 0.0060266256 |
|    clip_fraction        | 0.0507       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 12.4         |
|    n_updates            | 7500         |
|    policy_gradient_loss | -0.00413     |
|    std                  | 0.839        |
|    value_loss           | 4.21         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 751     |
|    time_elapsed    | 13652   |
|    total_timesteps | 3076096 |
--------------------------------
Eval num_timesteps=3080000, episode_reward=-389.13 +/- 160.85
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -389        |
| time/                   |             |
|    total_timesteps      | 3080000     |
| train/                  |             |
|    approx_kl            | 0.005595579 |
|    clip_fraction        | 0.0544      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.73        |
|    n_updates            | 7510        |
|    policy_gradient_loss | -0.00315    |
|    std                  | 0.838       |
|    value_loss           | 13.8        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 752     |
|    time_elapsed    | 13663   |
|    total_timesteps | 3080192 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 225          |
|    iterations           | 753          |
|    time_elapsed         | 13670        |
|    total_timesteps      | 3084288      |
| train/                  |              |
|    approx_kl            | 0.0058773425 |
|    clip_fraction        | 0.0546       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.17         |
|    n_updates            | 7520         |
|    policy_gradient_loss | -0.0014      |
|    std                  | 0.833        |
|    value_loss           | 0.519        |
------------------------------------------
Eval num_timesteps=3085000, episode_reward=-280.94 +/- 90.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -281        |
| time/                   |             |
|    total_timesteps      | 3085000     |
| train/                  |             |
|    approx_kl            | 0.005333621 |
|    clip_fraction        | 0.0597      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0252      |
|    n_updates            | 7530        |
|    policy_gradient_loss | -0.00293    |
|    std                  | 0.829       |
|    value_loss           | 0.0182      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 754     |
|    time_elapsed    | 13680   |
|    total_timesteps | 3088384 |
--------------------------------
Eval num_timesteps=3090000, episode_reward=-333.29 +/- 133.35
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -333         |
| time/                   |              |
|    total_timesteps      | 3090000      |
| train/                  |              |
|    approx_kl            | 0.0063417885 |
|    clip_fraction        | 0.0804       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.889        |
|    n_updates            | 7540         |
|    policy_gradient_loss | -0.00381     |
|    std                  | 0.829        |
|    value_loss           | 1.18         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 755     |
|    time_elapsed    | 13691   |
|    total_timesteps | 3092480 |
--------------------------------
Eval num_timesteps=3095000, episode_reward=-306.03 +/- 110.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -306         |
| time/                   |              |
|    total_timesteps      | 3095000      |
| train/                  |              |
|    approx_kl            | 0.0047494266 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0536       |
|    n_updates            | 7550         |
|    policy_gradient_loss | -0.000936    |
|    std                  | 0.831        |
|    value_loss           | 4.42         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 225     |
|    iterations      | 756     |
|    time_elapsed    | 13701   |
|    total_timesteps | 3096576 |
--------------------------------
Eval num_timesteps=3100000, episode_reward=-289.19 +/- 169.14
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -289        |
| time/                   |             |
|    total_timesteps      | 3100000     |
| train/                  |             |
|    approx_kl            | 0.005588915 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.612       |
|    n_updates            | 7560        |
|    policy_gradient_loss | -0.00157    |
|    std                  | 0.83        |
|    value_loss           | 2.48        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 757     |
|    time_elapsed    | 13711   |
|    total_timesteps | 3100672 |
--------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 758        |
|    time_elapsed         | 13718      |
|    total_timesteps      | 3104768    |
| train/                  |            |
|    approx_kl            | 0.01031876 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.57      |
|    explained_variance   | 0.983      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0274     |
|    n_updates            | 7570       |
|    policy_gradient_loss | -0.00338   |
|    std                  | 0.829      |
|    value_loss           | 9.66       |
----------------------------------------
Eval num_timesteps=3105000, episode_reward=-403.79 +/- 66.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -404         |
| time/                   |              |
|    total_timesteps      | 3105000      |
| train/                  |              |
|    approx_kl            | 0.0065293377 |
|    clip_fraction        | 0.0543       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00714      |
|    n_updates            | 7580         |
|    policy_gradient_loss | -0.00144     |
|    std                  | 0.834        |
|    value_loss           | 0.0364       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 759     |
|    time_elapsed    | 13727   |
|    total_timesteps | 3108864 |
--------------------------------
Eval num_timesteps=3110000, episode_reward=-291.88 +/- 201.18
Episode length: 804.60 +/- 392.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 805          |
|    mean_reward          | -292         |
| time/                   |              |
|    total_timesteps      | 3110000      |
| train/                  |              |
|    approx_kl            | 0.0059004626 |
|    clip_fraction        | 0.0537       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.191        |
|    n_updates            | 7590         |
|    policy_gradient_loss | -0.00224     |
|    std                  | 0.835        |
|    value_loss           | 5.92         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 760     |
|    time_elapsed    | 13736   |
|    total_timesteps | 3112960 |
--------------------------------
Eval num_timesteps=3115000, episode_reward=-261.27 +/- 149.14
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -261         |
| time/                   |              |
|    total_timesteps      | 3115000      |
| train/                  |              |
|    approx_kl            | 0.0059235496 |
|    clip_fraction        | 0.0445       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.15         |
|    n_updates            | 7600         |
|    policy_gradient_loss | -0.00278     |
|    std                  | 0.835        |
|    value_loss           | 1.21         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 761     |
|    time_elapsed    | 13747   |
|    total_timesteps | 3117056 |
--------------------------------
Eval num_timesteps=3120000, episode_reward=-376.86 +/- 171.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -377         |
| time/                   |              |
|    total_timesteps      | 3120000      |
| train/                  |              |
|    approx_kl            | 0.0039774515 |
|    clip_fraction        | 0.0218       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0974       |
|    n_updates            | 7610         |
|    policy_gradient_loss | -0.000724    |
|    std                  | 0.833        |
|    value_loss           | 4.46         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 762     |
|    time_elapsed    | 13759   |
|    total_timesteps | 3121152 |
--------------------------------
Eval num_timesteps=3125000, episode_reward=-227.79 +/- 164.24
Episode length: 812.60 +/- 376.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -228         |
| time/                   |              |
|    total_timesteps      | 3125000      |
| train/                  |              |
|    approx_kl            | 0.0045763794 |
|    clip_fraction        | 0.0605       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.145        |
|    n_updates            | 7620         |
|    policy_gradient_loss | -0.00148     |
|    std                  | 0.835        |
|    value_loss           | 0.069        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 226     |
|    iterations      | 763     |
|    time_elapsed    | 13769   |
|    total_timesteps | 3125248 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 764         |
|    time_elapsed         | 13776       |
|    total_timesteps      | 3129344     |
| train/                  |             |
|    approx_kl            | 0.004995199 |
|    clip_fraction        | 0.0387      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0242      |
|    n_updates            | 7630        |
|    policy_gradient_loss | -0.00184    |
|    std                  | 0.84        |
|    value_loss           | 0.213       |
-----------------------------------------
Eval num_timesteps=3130000, episode_reward=-241.79 +/- 146.56
Episode length: 813.60 +/- 374.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 814        |
|    mean_reward          | -242       |
| time/                   |            |
|    total_timesteps      | 3130000    |
| train/                  |            |
|    approx_kl            | 0.00402895 |
|    clip_fraction        | 0.0318     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.6       |
|    explained_variance   | 0.998      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0339     |
|    n_updates            | 7640       |
|    policy_gradient_loss | -0.000806  |
|    std                  | 0.838      |
|    value_loss           | 0.033      |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 765     |
|    time_elapsed    | 13786   |
|    total_timesteps | 3133440 |
--------------------------------
Eval num_timesteps=3135000, episode_reward=-277.81 +/- 156.34
Episode length: 805.40 +/- 391.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 805         |
|    mean_reward          | -278        |
| time/                   |             |
|    total_timesteps      | 3135000     |
| train/                  |             |
|    approx_kl            | 0.005360727 |
|    clip_fraction        | 0.0414      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0383      |
|    n_updates            | 7650        |
|    policy_gradient_loss | -0.00189    |
|    std                  | 0.839       |
|    value_loss           | 2.72        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 766     |
|    time_elapsed    | 13797   |
|    total_timesteps | 3137536 |
--------------------------------
Eval num_timesteps=3140000, episode_reward=-273.43 +/- 128.98
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -273         |
| time/                   |              |
|    total_timesteps      | 3140000      |
| train/                  |              |
|    approx_kl            | 0.0065711206 |
|    clip_fraction        | 0.047        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.26         |
|    n_updates            | 7660         |
|    policy_gradient_loss | -0.00244     |
|    std                  | 0.843        |
|    value_loss           | 0.389        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 767     |
|    time_elapsed    | 13808   |
|    total_timesteps | 3141632 |
--------------------------------
Eval num_timesteps=3145000, episode_reward=-377.89 +/- 180.97
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -378         |
| time/                   |              |
|    total_timesteps      | 3145000      |
| train/                  |              |
|    approx_kl            | 0.0038446677 |
|    clip_fraction        | 0.0288       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.195        |
|    n_updates            | 7670         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.843        |
|    value_loss           | 16.1         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 768     |
|    time_elapsed    | 13819   |
|    total_timesteps | 3145728 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 769         |
|    time_elapsed         | 13826       |
|    total_timesteps      | 3149824     |
| train/                  |             |
|    approx_kl            | 0.005060625 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.27        |
|    n_updates            | 7680        |
|    policy_gradient_loss | -0.000906   |
|    std                  | 0.842       |
|    value_loss           | 4.59        |
-----------------------------------------
Eval num_timesteps=3150000, episode_reward=-141.60 +/- 102.17
Episode length: 811.60 +/- 378.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 812         |
|    mean_reward          | -142        |
| time/                   |             |
|    total_timesteps      | 3150000     |
| train/                  |             |
|    approx_kl            | 0.005973233 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.343       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0213     |
|    n_updates            | 7690        |
|    policy_gradient_loss | -0.00528    |
|    std                  | 0.842       |
|    value_loss           | 0.198       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 227     |
|    iterations      | 770     |
|    time_elapsed    | 13837   |
|    total_timesteps | 3153920 |
--------------------------------
Eval num_timesteps=3155000, episode_reward=-263.44 +/- 162.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -263        |
| time/                   |             |
|    total_timesteps      | 3155000     |
| train/                  |             |
|    approx_kl            | 0.007484986 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.02        |
|    n_updates            | 7700        |
|    policy_gradient_loss | -0.00608    |
|    std                  | 0.841       |
|    value_loss           | 1.83        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 771     |
|    time_elapsed    | 13848   |
|    total_timesteps | 3158016 |
--------------------------------
Eval num_timesteps=3160000, episode_reward=-372.84 +/- 213.74
Episode length: 812.80 +/- 376.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -373         |
| time/                   |              |
|    total_timesteps      | 3160000      |
| train/                  |              |
|    approx_kl            | 0.0041930564 |
|    clip_fraction        | 0.0361       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.845        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.528        |
|    n_updates            | 7710         |
|    policy_gradient_loss | -0.00137     |
|    std                  | 0.838        |
|    value_loss           | 7.14         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 772     |
|    time_elapsed    | 13858   |
|    total_timesteps | 3162112 |
--------------------------------
Eval num_timesteps=3165000, episode_reward=-278.42 +/- 111.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -278         |
| time/                   |              |
|    total_timesteps      | 3165000      |
| train/                  |              |
|    approx_kl            | 0.0060945586 |
|    clip_fraction        | 0.0544       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.6         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00797      |
|    n_updates            | 7720         |
|    policy_gradient_loss | -0.00287     |
|    std                  | 0.832        |
|    value_loss           | 0.0295       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 773     |
|    time_elapsed    | 13871   |
|    total_timesteps | 3166208 |
--------------------------------
Eval num_timesteps=3170000, episode_reward=-388.91 +/- 211.26
Episode length: 803.80 +/- 394.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 804          |
|    mean_reward          | -389         |
| time/                   |              |
|    total_timesteps      | 3170000      |
| train/                  |              |
|    approx_kl            | 0.0039531235 |
|    clip_fraction        | 0.038        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.498        |
|    n_updates            | 7730         |
|    policy_gradient_loss | -0.00091     |
|    std                  | 0.832        |
|    value_loss           | 8.88         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 774     |
|    time_elapsed    | 13880   |
|    total_timesteps | 3170304 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 775         |
|    time_elapsed         | 13888       |
|    total_timesteps      | 3174400     |
| train/                  |             |
|    approx_kl            | 0.005516386 |
|    clip_fraction        | 0.0523      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0211      |
|    n_updates            | 7740        |
|    policy_gradient_loss | -0.00208    |
|    std                  | 0.827       |
|    value_loss           | 0.455       |
-----------------------------------------
Eval num_timesteps=3175000, episode_reward=-224.58 +/- 152.26
Episode length: 808.80 +/- 384.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 809         |
|    mean_reward          | -225        |
| time/                   |             |
|    total_timesteps      | 3175000     |
| train/                  |             |
|    approx_kl            | 0.004977888 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00896     |
|    n_updates            | 7750        |
|    policy_gradient_loss | -0.00191    |
|    std                  | 0.82        |
|    value_loss           | 0.0165      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 776     |
|    time_elapsed    | 13898   |
|    total_timesteps | 3178496 |
--------------------------------
Eval num_timesteps=3180000, episode_reward=-363.35 +/- 213.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -363         |
| time/                   |              |
|    total_timesteps      | 3180000      |
| train/                  |              |
|    approx_kl            | 0.0045931498 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0433       |
|    n_updates            | 7760         |
|    policy_gradient_loss | -0.00311     |
|    std                  | 0.819        |
|    value_loss           | 1.28         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 777     |
|    time_elapsed    | 13910   |
|    total_timesteps | 3182592 |
--------------------------------
Eval num_timesteps=3185000, episode_reward=-420.06 +/- 144.68
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -420         |
| time/                   |              |
|    total_timesteps      | 3185000      |
| train/                  |              |
|    approx_kl            | 0.0052288817 |
|    clip_fraction        | 0.053        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0281       |
|    n_updates            | 7770         |
|    policy_gradient_loss | -0.0028      |
|    std                  | 0.821        |
|    value_loss           | 3.51         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 228     |
|    iterations      | 778     |
|    time_elapsed    | 13921   |
|    total_timesteps | 3186688 |
--------------------------------
Eval num_timesteps=3190000, episode_reward=-223.54 +/- 135.32
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -224         |
| time/                   |              |
|    total_timesteps      | 3190000      |
| train/                  |              |
|    approx_kl            | 0.0035410547 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0188      |
|    n_updates            | 7780         |
|    policy_gradient_loss | -0.000599    |
|    std                  | 0.822        |
|    value_loss           | 3.17         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 229     |
|    iterations      | 779     |
|    time_elapsed    | 13931   |
|    total_timesteps | 3190784 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 780         |
|    time_elapsed         | 13937       |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.004504102 |
|    clip_fraction        | 0.0585      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0109     |
|    n_updates            | 7790        |
|    policy_gradient_loss | -0.00382    |
|    std                  | 0.824       |
|    value_loss           | 0.0802      |
-----------------------------------------
Eval num_timesteps=3195000, episode_reward=-305.20 +/- 227.94
Episode length: 813.00 +/- 376.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -305         |
| time/                   |              |
|    total_timesteps      | 3195000      |
| train/                  |              |
|    approx_kl            | 0.0063807564 |
|    clip_fraction        | 0.0516       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.9          |
|    n_updates            | 7800         |
|    policy_gradient_loss | -0.00455     |
|    std                  | 0.819        |
|    value_loss           | 0.366        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 229     |
|    iterations      | 781     |
|    time_elapsed    | 13947   |
|    total_timesteps | 3198976 |
--------------------------------
Eval num_timesteps=3200000, episode_reward=-330.01 +/- 133.76
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -330        |
| time/                   |             |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.006130618 |
|    clip_fraction        | 0.0478      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.118       |
|    n_updates            | 7810        |
|    policy_gradient_loss | -0.00287    |
|    std                  | 0.817       |
|    value_loss           | 0.741       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 229     |
|    iterations      | 782     |
|    time_elapsed    | 13958   |
|    total_timesteps | 3203072 |
--------------------------------
Eval num_timesteps=3205000, episode_reward=-324.89 +/- 118.10
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -325         |
| time/                   |              |
|    total_timesteps      | 3205000      |
| train/                  |              |
|    approx_kl            | 0.0077500837 |
|    clip_fraction        | 0.0899       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0589       |
|    n_updates            | 7820         |
|    policy_gradient_loss | -0.00663     |
|    std                  | 0.822        |
|    value_loss           | 0.0312       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 229     |
|    iterations      | 783     |
|    time_elapsed    | 13970   |
|    total_timesteps | 3207168 |
--------------------------------
Eval num_timesteps=3210000, episode_reward=-362.03 +/- 222.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -362         |
| time/                   |              |
|    total_timesteps      | 3210000      |
| train/                  |              |
|    approx_kl            | 0.0041885134 |
|    clip_fraction        | 0.0247       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0267       |
|    n_updates            | 7830         |
|    policy_gradient_loss | -0.000918    |
|    std                  | 0.822        |
|    value_loss           | 1.62         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 229     |
|    iterations      | 784     |
|    time_elapsed    | 13980   |
|    total_timesteps | 3211264 |
--------------------------------
Eval num_timesteps=3215000, episode_reward=-214.95 +/- 117.33
Episode length: 802.80 +/- 396.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 803         |
|    mean_reward          | -215        |
| time/                   |             |
|    total_timesteps      | 3215000     |
| train/                  |             |
|    approx_kl            | 0.006351512 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0001      |
|    loss                 | 24          |
|    n_updates            | 7840        |
|    policy_gradient_loss | -0.00161    |
|    std                  | 0.821       |
|    value_loss           | 10.2        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 229     |
|    iterations      | 785     |
|    time_elapsed    | 13990   |
|    total_timesteps | 3215360 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 230          |
|    iterations           | 786          |
|    time_elapsed         | 13997        |
|    total_timesteps      | 3219456      |
| train/                  |              |
|    approx_kl            | 0.0049302643 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.075        |
|    n_updates            | 7850         |
|    policy_gradient_loss | -0.000958    |
|    std                  | 0.821        |
|    value_loss           | 12.2         |
------------------------------------------
Eval num_timesteps=3220000, episode_reward=-211.10 +/- 176.85
Episode length: 618.40 +/- 468.72
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 618          |
|    mean_reward          | -211         |
| time/                   |              |
|    total_timesteps      | 3220000      |
| train/                  |              |
|    approx_kl            | 0.0054820618 |
|    clip_fraction        | 0.0401       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0385       |
|    n_updates            | 7860         |
|    policy_gradient_loss | -0.0012      |
|    std                  | 0.826        |
|    value_loss           | 0.0582       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 787     |
|    time_elapsed    | 14006   |
|    total_timesteps | 3223552 |
--------------------------------
Eval num_timesteps=3225000, episode_reward=-347.20 +/- 221.29
Episode length: 814.80 +/- 372.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -347         |
| time/                   |              |
|    total_timesteps      | 3225000      |
| train/                  |              |
|    approx_kl            | 0.0077053197 |
|    clip_fraction        | 0.0591       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.97         |
|    n_updates            | 7870         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 0.826        |
|    value_loss           | 11           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 788     |
|    time_elapsed    | 14016   |
|    total_timesteps | 3227648 |
--------------------------------
Eval num_timesteps=3230000, episode_reward=-230.88 +/- 124.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -231         |
| time/                   |              |
|    total_timesteps      | 3230000      |
| train/                  |              |
|    approx_kl            | 0.0059724413 |
|    clip_fraction        | 0.0454       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 21.5         |
|    n_updates            | 7880         |
|    policy_gradient_loss | -0.000495    |
|    std                  | 0.824        |
|    value_loss           | 7.5          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 789     |
|    time_elapsed    | 14030   |
|    total_timesteps | 3231744 |
--------------------------------
Eval num_timesteps=3235000, episode_reward=-270.01 +/- 142.38
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -270        |
| time/                   |             |
|    total_timesteps      | 3235000     |
| train/                  |             |
|    approx_kl            | 0.004975359 |
|    clip_fraction        | 0.0323      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.229       |
|    n_updates            | 7890        |
|    policy_gradient_loss | -0.000971   |
|    std                  | 0.822       |
|    value_loss           | 12.8        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 790     |
|    time_elapsed    | 14040   |
|    total_timesteps | 3235840 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 791         |
|    time_elapsed         | 14047       |
|    total_timesteps      | 3239936     |
| train/                  |             |
|    approx_kl            | 0.004369159 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0213      |
|    n_updates            | 7900        |
|    policy_gradient_loss | -0.00254    |
|    std                  | 0.822       |
|    value_loss           | 0.0511      |
-----------------------------------------
Eval num_timesteps=3240000, episode_reward=-302.19 +/- 169.45
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -302        |
| time/                   |             |
|    total_timesteps      | 3240000     |
| train/                  |             |
|    approx_kl            | 0.005532045 |
|    clip_fraction        | 0.0531      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.67        |
|    n_updates            | 7910        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 0.815       |
|    value_loss           | 0.988       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 792     |
|    time_elapsed    | 14057   |
|    total_timesteps | 3244032 |
--------------------------------
Eval num_timesteps=3245000, episode_reward=-395.57 +/- 192.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -396         |
| time/                   |              |
|    total_timesteps      | 3245000      |
| train/                  |              |
|    approx_kl            | 0.0075139054 |
|    clip_fraction        | 0.0582       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.67         |
|    n_updates            | 7920         |
|    policy_gradient_loss | -0.00289     |
|    std                  | 0.815        |
|    value_loss           | 2.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 230     |
|    iterations      | 793     |
|    time_elapsed    | 14066   |
|    total_timesteps | 3248128 |
--------------------------------
Eval num_timesteps=3250000, episode_reward=-320.26 +/- 171.48
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -320        |
| time/                   |             |
|    total_timesteps      | 3250000     |
| train/                  |             |
|    approx_kl            | 0.004061939 |
|    clip_fraction        | 0.022       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.511       |
|    n_updates            | 7930        |
|    policy_gradient_loss | -0.000874   |
|    std                  | 0.815       |
|    value_loss           | 2.1         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 794     |
|    time_elapsed    | 14077   |
|    total_timesteps | 3252224 |
--------------------------------
Eval num_timesteps=3255000, episode_reward=-249.44 +/- 58.58
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -249        |
| time/                   |             |
|    total_timesteps      | 3255000     |
| train/                  |             |
|    approx_kl            | 0.005706517 |
|    clip_fraction        | 0.0407      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.62        |
|    n_updates            | 7940        |
|    policy_gradient_loss | -0.00121    |
|    std                  | 0.814       |
|    value_loss           | 1.35        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 795     |
|    time_elapsed    | 14091   |
|    total_timesteps | 3256320 |
--------------------------------
Eval num_timesteps=3260000, episode_reward=-270.17 +/- 52.74
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -270        |
| time/                   |             |
|    total_timesteps      | 3260000     |
| train/                  |             |
|    approx_kl            | 0.004155563 |
|    clip_fraction        | 0.0413      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.202       |
|    n_updates            | 7950        |
|    policy_gradient_loss | -0.00202    |
|    std                  | 0.813       |
|    value_loss           | 10.8        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 796     |
|    time_elapsed    | 14101   |
|    total_timesteps | 3260416 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 231          |
|    iterations           | 797          |
|    time_elapsed         | 14108        |
|    total_timesteps      | 3264512      |
| train/                  |              |
|    approx_kl            | 0.0040667355 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.193        |
|    n_updates            | 7960         |
|    policy_gradient_loss | -0.00159     |
|    std                  | 0.814        |
|    value_loss           | 2.5          |
------------------------------------------
Eval num_timesteps=3265000, episode_reward=-286.06 +/- 148.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 3265000      |
| train/                  |              |
|    approx_kl            | 0.0065498417 |
|    clip_fraction        | 0.0716       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00465     |
|    n_updates            | 7970         |
|    policy_gradient_loss | -0.00427     |
|    std                  | 0.811        |
|    value_loss           | 0.0271       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 798     |
|    time_elapsed    | 14118   |
|    total_timesteps | 3268608 |
--------------------------------
Eval num_timesteps=3270000, episode_reward=-397.07 +/- 136.98
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -397         |
| time/                   |              |
|    total_timesteps      | 3270000      |
| train/                  |              |
|    approx_kl            | 0.0063037137 |
|    clip_fraction        | 0.0594       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0721       |
|    n_updates            | 7980         |
|    policy_gradient_loss | -0.00355     |
|    std                  | 0.812        |
|    value_loss           | 1.65         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 799     |
|    time_elapsed    | 14129   |
|    total_timesteps | 3272704 |
--------------------------------
Eval num_timesteps=3275000, episode_reward=-253.15 +/- 63.02
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -253       |
| time/                   |            |
|    total_timesteps      | 3275000    |
| train/                  |            |
|    approx_kl            | 0.00484511 |
|    clip_fraction        | 0.0507     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.54      |
|    explained_variance   | 0.994      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0128     |
|    n_updates            | 7990       |
|    policy_gradient_loss | -0.00184   |
|    std                  | 0.812      |
|    value_loss           | 2.02       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 800     |
|    time_elapsed    | 14140   |
|    total_timesteps | 3276800 |
--------------------------------
Eval num_timesteps=3280000, episode_reward=-333.09 +/- 81.91
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -333        |
| time/                   |             |
|    total_timesteps      | 3280000     |
| train/                  |             |
|    approx_kl            | 0.005381835 |
|    clip_fraction        | 0.0486      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0325      |
|    n_updates            | 8000        |
|    policy_gradient_loss | -0.00145    |
|    std                  | 0.815       |
|    value_loss           | 1.13        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 231     |
|    iterations      | 801     |
|    time_elapsed    | 14152   |
|    total_timesteps | 3280896 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 231          |
|    iterations           | 802          |
|    time_elapsed         | 14161        |
|    total_timesteps      | 3284992      |
| train/                  |              |
|    approx_kl            | 0.0059115076 |
|    clip_fraction        | 0.0612       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.116        |
|    n_updates            | 8010         |
|    policy_gradient_loss | -0.00353     |
|    std                  | 0.822        |
|    value_loss           | 0.0662       |
------------------------------------------
Eval num_timesteps=3285000, episode_reward=-231.28 +/- 125.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -231         |
| time/                   |              |
|    total_timesteps      | 3285000      |
| train/                  |              |
|    approx_kl            | 0.0045982627 |
|    clip_fraction        | 0.0485       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0184      |
|    n_updates            | 8020         |
|    policy_gradient_loss | -0.00301     |
|    std                  | 0.824        |
|    value_loss           | 0.00803      |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 803     |
|    time_elapsed    | 14171   |
|    total_timesteps | 3289088 |
--------------------------------
Eval num_timesteps=3290000, episode_reward=-204.25 +/- 122.93
Episode length: 807.60 +/- 386.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -204         |
| time/                   |              |
|    total_timesteps      | 3290000      |
| train/                  |              |
|    approx_kl            | 0.0042915503 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0209       |
|    n_updates            | 8030         |
|    policy_gradient_loss | -0.000926    |
|    std                  | 0.824        |
|    value_loss           | 1.36         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 804     |
|    time_elapsed    | 14181   |
|    total_timesteps | 3293184 |
--------------------------------
Eval num_timesteps=3295000, episode_reward=-315.19 +/- 179.39
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -315        |
| time/                   |             |
|    total_timesteps      | 3295000     |
| train/                  |             |
|    approx_kl            | 0.004986168 |
|    clip_fraction        | 0.0385      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00703     |
|    n_updates            | 8040        |
|    policy_gradient_loss | -0.00201    |
|    std                  | 0.825       |
|    value_loss           | 2.15        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 805     |
|    time_elapsed    | 14194   |
|    total_timesteps | 3297280 |
--------------------------------
Eval num_timesteps=3300000, episode_reward=-412.98 +/- 153.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -413         |
| time/                   |              |
|    total_timesteps      | 3300000      |
| train/                  |              |
|    approx_kl            | 0.0034399685 |
|    clip_fraction        | 0.0468       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.632        |
|    n_updates            | 8050         |
|    policy_gradient_loss | -0.000601    |
|    std                  | 0.826        |
|    value_loss           | 0.255        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 806     |
|    time_elapsed    | 14205   |
|    total_timesteps | 3301376 |
--------------------------------
Eval num_timesteps=3305000, episode_reward=-259.86 +/- 166.85
Episode length: 806.40 +/- 389.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 806          |
|    mean_reward          | -260         |
| time/                   |              |
|    total_timesteps      | 3305000      |
| train/                  |              |
|    approx_kl            | 0.0068149846 |
|    clip_fraction        | 0.0464       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.122        |
|    n_updates            | 8060         |
|    policy_gradient_loss | -0.00129     |
|    std                  | 0.822        |
|    value_loss           | 8.38         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 807     |
|    time_elapsed    | 14215   |
|    total_timesteps | 3305472 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 232          |
|    iterations           | 808          |
|    time_elapsed         | 14223        |
|    total_timesteps      | 3309568      |
| train/                  |              |
|    approx_kl            | 0.0062212045 |
|    clip_fraction        | 0.0465       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.222        |
|    n_updates            | 8070         |
|    policy_gradient_loss | -0.00251     |
|    std                  | 0.818        |
|    value_loss           | 0.672        |
------------------------------------------
Eval num_timesteps=3310000, episode_reward=-272.06 +/- 126.32
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -272         |
| time/                   |              |
|    total_timesteps      | 3310000      |
| train/                  |              |
|    approx_kl            | 0.0049790055 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00649      |
|    n_updates            | 8080         |
|    policy_gradient_loss | -0.00125     |
|    std                  | 0.824        |
|    value_loss           | 0.0414       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 809     |
|    time_elapsed    | 14235   |
|    total_timesteps | 3313664 |
--------------------------------
Eval num_timesteps=3315000, episode_reward=-365.96 +/- 171.80
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -366        |
| time/                   |             |
|    total_timesteps      | 3315000     |
| train/                  |             |
|    approx_kl            | 0.003904785 |
|    clip_fraction        | 0.0486      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 39.4        |
|    n_updates            | 8090        |
|    policy_gradient_loss | -0.00313    |
|    std                  | 0.823       |
|    value_loss           | 14.4        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 232     |
|    iterations      | 810     |
|    time_elapsed    | 14246   |
|    total_timesteps | 3317760 |
--------------------------------
Eval num_timesteps=3320000, episode_reward=-442.82 +/- 146.71
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -443        |
| time/                   |             |
|    total_timesteps      | 3320000     |
| train/                  |             |
|    approx_kl            | 0.005520241 |
|    clip_fraction        | 0.0472      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.103       |
|    n_updates            | 8100        |
|    policy_gradient_loss | -0.00339    |
|    std                  | 0.821       |
|    value_loss           | 1.69        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 811     |
|    time_elapsed    | 14256   |
|    total_timesteps | 3321856 |
--------------------------------
Eval num_timesteps=3325000, episode_reward=-286.17 +/- 128.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 3325000      |
| train/                  |              |
|    approx_kl            | 0.0048781717 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.378        |
|    n_updates            | 8110         |
|    policy_gradient_loss | -0.000668    |
|    std                  | 0.816        |
|    value_loss           | 0.149        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 812     |
|    time_elapsed    | 14265   |
|    total_timesteps | 3325952 |
--------------------------------
Eval num_timesteps=3330000, episode_reward=-296.37 +/- 159.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -296         |
| time/                   |              |
|    total_timesteps      | 3330000      |
| train/                  |              |
|    approx_kl            | 0.0091632735 |
|    clip_fraction        | 0.0688       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0395       |
|    n_updates            | 8120         |
|    policy_gradient_loss | -0.00407     |
|    std                  | 0.814        |
|    value_loss           | 1.12         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 813     |
|    time_elapsed    | 14276   |
|    total_timesteps | 3330048 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 233          |
|    iterations           | 814          |
|    time_elapsed         | 14283        |
|    total_timesteps      | 3334144      |
| train/                  |              |
|    approx_kl            | 0.0039286115 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0569       |
|    n_updates            | 8130         |
|    policy_gradient_loss | -0.00237     |
|    std                  | 0.817        |
|    value_loss           | 1.49         |
------------------------------------------
Eval num_timesteps=3335000, episode_reward=-290.99 +/- 123.86
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -291         |
| time/                   |              |
|    total_timesteps      | 3335000      |
| train/                  |              |
|    approx_kl            | 0.0061835824 |
|    clip_fraction        | 0.0429       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00736     |
|    n_updates            | 8140         |
|    policy_gradient_loss | -0.00183     |
|    std                  | 0.812        |
|    value_loss           | 0.0248       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 815     |
|    time_elapsed    | 14293   |
|    total_timesteps | 3338240 |
--------------------------------
Eval num_timesteps=3340000, episode_reward=-311.07 +/- 154.33
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -311         |
| time/                   |              |
|    total_timesteps      | 3340000      |
| train/                  |              |
|    approx_kl            | 0.0060595274 |
|    clip_fraction        | 0.0813       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0677       |
|    n_updates            | 8150         |
|    policy_gradient_loss | -0.00379     |
|    std                  | 0.816        |
|    value_loss           | 0.238        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 816     |
|    time_elapsed    | 14302   |
|    total_timesteps | 3342336 |
--------------------------------
Eval num_timesteps=3345000, episode_reward=-424.87 +/- 188.61
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -425         |
| time/                   |              |
|    total_timesteps      | 3345000      |
| train/                  |              |
|    approx_kl            | 0.0046333214 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0673       |
|    n_updates            | 8160         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.815        |
|    value_loss           | 2.02         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 817     |
|    time_elapsed    | 14314   |
|    total_timesteps | 3346432 |
--------------------------------
Eval num_timesteps=3350000, episode_reward=-392.42 +/- 134.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -392         |
| time/                   |              |
|    total_timesteps      | 3350000      |
| train/                  |              |
|    approx_kl            | 0.0040883035 |
|    clip_fraction        | 0.0397       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0404       |
|    n_updates            | 8170         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 0.813        |
|    value_loss           | 0.516        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 233     |
|    iterations      | 818     |
|    time_elapsed    | 14327   |
|    total_timesteps | 3350528 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 233          |
|    iterations           | 819          |
|    time_elapsed         | 14337        |
|    total_timesteps      | 3354624      |
| train/                  |              |
|    approx_kl            | 0.0070852414 |
|    clip_fraction        | 0.0548       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.226        |
|    n_updates            | 8180         |
|    policy_gradient_loss | -0.00301     |
|    std                  | 0.812        |
|    value_loss           | 6.68         |
------------------------------------------
Eval num_timesteps=3355000, episode_reward=-357.77 +/- 159.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -358         |
| time/                   |              |
|    total_timesteps      | 3355000      |
| train/                  |              |
|    approx_kl            | 0.0052084033 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0296       |
|    n_updates            | 8190         |
|    policy_gradient_loss | -0.000355    |
|    std                  | 0.805        |
|    value_loss           | 0.0663       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 820     |
|    time_elapsed    | 14349   |
|    total_timesteps | 3358720 |
--------------------------------
Eval num_timesteps=3360000, episode_reward=-349.76 +/- 150.91
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -350        |
| time/                   |             |
|    total_timesteps      | 3360000     |
| train/                  |             |
|    approx_kl            | 0.005576895 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0222      |
|    n_updates            | 8200        |
|    policy_gradient_loss | -0.00167    |
|    std                  | 0.805       |
|    value_loss           | 0.222       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 821     |
|    time_elapsed    | 14359   |
|    total_timesteps | 3362816 |
--------------------------------
Eval num_timesteps=3365000, episode_reward=-222.91 +/- 171.33
Episode length: 809.60 +/- 382.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 810          |
|    mean_reward          | -223         |
| time/                   |              |
|    total_timesteps      | 3365000      |
| train/                  |              |
|    approx_kl            | 0.0019644862 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.111        |
|    n_updates            | 8210         |
|    policy_gradient_loss | 0.0002       |
|    std                  | 0.806        |
|    value_loss           | 6.81         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 822     |
|    time_elapsed    | 14369   |
|    total_timesteps | 3366912 |
--------------------------------
Eval num_timesteps=3370000, episode_reward=-391.70 +/- 128.65
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -392        |
| time/                   |             |
|    total_timesteps      | 3370000     |
| train/                  |             |
|    approx_kl            | 0.006239484 |
|    clip_fraction        | 0.0505      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.69        |
|    n_updates            | 8220        |
|    policy_gradient_loss | -0.00283    |
|    std                  | 0.805       |
|    value_loss           | 1.25        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 823     |
|    time_elapsed    | 14380   |
|    total_timesteps | 3371008 |
--------------------------------
Eval num_timesteps=3375000, episode_reward=-424.65 +/- 165.86
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -425         |
| time/                   |              |
|    total_timesteps      | 3375000      |
| train/                  |              |
|    approx_kl            | 0.0064345603 |
|    clip_fraction        | 0.0675       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0143       |
|    n_updates            | 8230         |
|    policy_gradient_loss | -0.00387     |
|    std                  | 0.812        |
|    value_loss           | 1.51         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 824     |
|    time_elapsed    | 14392   |
|    total_timesteps | 3375104 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 234          |
|    iterations           | 825          |
|    time_elapsed         | 14399        |
|    total_timesteps      | 3379200      |
| train/                  |              |
|    approx_kl            | 0.0054898052 |
|    clip_fraction        | 0.0427       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00304     |
|    n_updates            | 8240         |
|    policy_gradient_loss | -0.00248     |
|    std                  | 0.812        |
|    value_loss           | 0.544        |
------------------------------------------
Eval num_timesteps=3380000, episode_reward=-310.35 +/- 106.10
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -310         |
| time/                   |              |
|    total_timesteps      | 3380000      |
| train/                  |              |
|    approx_kl            | 0.0057236953 |
|    clip_fraction        | 0.0453       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.048        |
|    n_updates            | 8250         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 0.818        |
|    value_loss           | 0.0575       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 826     |
|    time_elapsed    | 14410   |
|    total_timesteps | 3383296 |
--------------------------------
Eval num_timesteps=3385000, episode_reward=-186.63 +/- 102.90
Episode length: 812.20 +/- 377.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 812         |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 3385000     |
| train/                  |             |
|    approx_kl            | 0.005515148 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0107     |
|    n_updates            | 8260        |
|    policy_gradient_loss | -0.00347    |
|    std                  | 0.823       |
|    value_loss           | 1.31        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 827     |
|    time_elapsed    | 14423   |
|    total_timesteps | 3387392 |
--------------------------------
Eval num_timesteps=3390000, episode_reward=-367.43 +/- 104.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -367         |
| time/                   |              |
|    total_timesteps      | 3390000      |
| train/                  |              |
|    approx_kl            | 0.0063065537 |
|    clip_fraction        | 0.0524       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0652       |
|    n_updates            | 8270         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 0.823        |
|    value_loss           | 14.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 234     |
|    iterations      | 828     |
|    time_elapsed    | 14434   |
|    total_timesteps | 3391488 |
--------------------------------
Eval num_timesteps=3395000, episode_reward=-272.99 +/- 141.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -273         |
| time/                   |              |
|    total_timesteps      | 3395000      |
| train/                  |              |
|    approx_kl            | 0.0042453227 |
|    clip_fraction        | 0.0354       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0132       |
|    n_updates            | 8280         |
|    policy_gradient_loss | -0.000767    |
|    std                  | 0.829        |
|    value_loss           | 0.43         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 235     |
|    iterations      | 829     |
|    time_elapsed    | 14445   |
|    total_timesteps | 3395584 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 235         |
|    iterations           | 830         |
|    time_elapsed         | 14452       |
|    total_timesteps      | 3399680     |
| train/                  |             |
|    approx_kl            | 0.006481515 |
|    clip_fraction        | 0.0589      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0297      |
|    n_updates            | 8290        |
|    policy_gradient_loss | -0.00356    |
|    std                  | 0.838       |
|    value_loss           | 0.149       |
-----------------------------------------
Eval num_timesteps=3400000, episode_reward=-260.39 +/- 84.04
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -260        |
| time/                   |             |
|    total_timesteps      | 3400000     |
| train/                  |             |
|    approx_kl            | 0.005581547 |
|    clip_fraction        | 0.0525      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0077     |
|    n_updates            | 8300        |
|    policy_gradient_loss | -0.00231    |
|    std                  | 0.842       |
|    value_loss           | 0.0359      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 235     |
|    iterations      | 831     |
|    time_elapsed    | 14465   |
|    total_timesteps | 3403776 |
--------------------------------
Eval num_timesteps=3405000, episode_reward=-226.81 +/- 190.36
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -227       |
| time/                   |            |
|    total_timesteps      | 3405000    |
| train/                  |            |
|    approx_kl            | 0.00365166 |
|    clip_fraction        | 0.0345     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.64      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.327      |
|    n_updates            | 8310       |
|    policy_gradient_loss | -0.00167   |
|    std                  | 0.841      |
|    value_loss           | 1.61       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 235     |
|    iterations      | 832     |
|    time_elapsed    | 14476   |
|    total_timesteps | 3407872 |
--------------------------------
Eval num_timesteps=3410000, episode_reward=-299.30 +/- 88.39
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -299        |
| time/                   |             |
|    total_timesteps      | 3410000     |
| train/                  |             |
|    approx_kl            | 0.003224011 |
|    clip_fraction        | 0.0321      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.996       |
|    n_updates            | 8320        |
|    policy_gradient_loss | -0.00167    |
|    std                  | 0.842       |
|    value_loss           | 15.8        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 235     |
|    iterations      | 833     |
|    time_elapsed    | 14487   |
|    total_timesteps | 3411968 |
--------------------------------
Eval num_timesteps=3415000, episode_reward=-292.49 +/- 125.66
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -292        |
| time/                   |             |
|    total_timesteps      | 3415000     |
| train/                  |             |
|    approx_kl            | 0.006494271 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.64       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.157       |
|    n_updates            | 8330        |
|    policy_gradient_loss | -0.000872   |
|    std                  | 0.844       |
|    value_loss           | 2.66        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 235     |
|    iterations      | 834     |
|    time_elapsed    | 14497   |
|    total_timesteps | 3416064 |
--------------------------------
Eval num_timesteps=3420000, episode_reward=-379.81 +/- 153.97
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -380         |
| time/                   |              |
|    total_timesteps      | 3420000      |
| train/                  |              |
|    approx_kl            | 0.0046442687 |
|    clip_fraction        | 0.0403       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.64        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0487       |
|    n_updates            | 8340         |
|    policy_gradient_loss | -0.00145     |
|    std                  | 0.843        |
|    value_loss           | 3.56         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 235     |
|    iterations      | 835     |
|    time_elapsed    | 14508   |
|    total_timesteps | 3420160 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 235          |
|    iterations           | 836          |
|    time_elapsed         | 14514        |
|    total_timesteps      | 3424256      |
| train/                  |              |
|    approx_kl            | 0.0072196024 |
|    clip_fraction        | 0.0727       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.64        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0561       |
|    n_updates            | 8350         |
|    policy_gradient_loss | -0.00311     |
|    std                  | 0.845        |
|    value_loss           | 4.72         |
------------------------------------------
Eval num_timesteps=3425000, episode_reward=-357.10 +/- 204.05
Episode length: 816.00 +/- 370.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 816          |
|    mean_reward          | -357         |
| time/                   |              |
|    total_timesteps      | 3425000      |
| train/                  |              |
|    approx_kl            | 0.0041922266 |
|    clip_fraction        | 0.0597       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.65        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0104       |
|    n_updates            | 8360         |
|    policy_gradient_loss | -0.000335    |
|    std                  | 0.843        |
|    value_loss           | 0.0231       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 236     |
|    iterations      | 837     |
|    time_elapsed    | 14524   |
|    total_timesteps | 3428352 |
--------------------------------
Eval num_timesteps=3430000, episode_reward=-433.21 +/- 189.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -433        |
| time/                   |             |
|    total_timesteps      | 3430000     |
| train/                  |             |
|    approx_kl            | 0.004742966 |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00681     |
|    n_updates            | 8370        |
|    policy_gradient_loss | -0.00257    |
|    std                  | 0.839       |
|    value_loss           | 0.0549      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 236     |
|    iterations      | 838     |
|    time_elapsed    | 14535   |
|    total_timesteps | 3432448 |
--------------------------------
Eval num_timesteps=3435000, episode_reward=-436.72 +/- 130.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -437         |
| time/                   |              |
|    total_timesteps      | 3435000      |
| train/                  |              |
|    approx_kl            | 0.0049001095 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0113       |
|    n_updates            | 8380         |
|    policy_gradient_loss | -0.0002      |
|    std                  | 0.834        |
|    value_loss           | 0.0602       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 236     |
|    iterations      | 839     |
|    time_elapsed    | 14548   |
|    total_timesteps | 3436544 |
--------------------------------
Eval num_timesteps=3440000, episode_reward=-286.74 +/- 207.39
Episode length: 814.20 +/- 373.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | -287        |
| time/                   |             |
|    total_timesteps      | 3440000     |
| train/                  |             |
|    approx_kl            | 0.004088207 |
|    clip_fraction        | 0.0268      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.32        |
|    n_updates            | 8390        |
|    policy_gradient_loss | -0.000406   |
|    std                  | 0.833       |
|    value_loss           | 1.77        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 236     |
|    iterations      | 840     |
|    time_elapsed    | 14558   |
|    total_timesteps | 3440640 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 841         |
|    time_elapsed         | 14564       |
|    total_timesteps      | 3444736     |
| train/                  |             |
|    approx_kl            | 0.004776855 |
|    clip_fraction        | 0.0442      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00309    |
|    n_updates            | 8400        |
|    policy_gradient_loss | -0.00288    |
|    std                  | 0.835       |
|    value_loss           | 2.48        |
-----------------------------------------
Eval num_timesteps=3445000, episode_reward=-279.61 +/- 194.48
Episode length: 810.00 +/- 382.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | -280        |
| time/                   |             |
|    total_timesteps      | 3445000     |
| train/                  |             |
|    approx_kl            | 0.006364959 |
|    clip_fraction        | 0.0624      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0155     |
|    n_updates            | 8410        |
|    policy_gradient_loss | -0.00287    |
|    std                  | 0.834       |
|    value_loss           | 0.433       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 236     |
|    iterations      | 842     |
|    time_elapsed    | 14573   |
|    total_timesteps | 3448832 |
--------------------------------
Eval num_timesteps=3450000, episode_reward=-302.42 +/- 149.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -302         |
| time/                   |              |
|    total_timesteps      | 3450000      |
| train/                  |              |
|    approx_kl            | 0.0052368687 |
|    clip_fraction        | 0.0358       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.398        |
|    n_updates            | 8420         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 0.832        |
|    value_loss           | 0.601        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 236     |
|    iterations      | 843     |
|    time_elapsed    | 14583   |
|    total_timesteps | 3452928 |
--------------------------------
Eval num_timesteps=3455000, episode_reward=-340.70 +/- 103.44
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -341        |
| time/                   |             |
|    total_timesteps      | 3455000     |
| train/                  |             |
|    approx_kl            | 0.004007265 |
|    clip_fraction        | 0.0368      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.379       |
|    n_updates            | 8430        |
|    policy_gradient_loss | -0.00201    |
|    std                  | 0.833       |
|    value_loss           | 4.02        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 236     |
|    iterations      | 844     |
|    time_elapsed    | 14592   |
|    total_timesteps | 3457024 |
--------------------------------
Eval num_timesteps=3460000, episode_reward=-280.87 +/- 128.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -281        |
| time/                   |             |
|    total_timesteps      | 3460000     |
| train/                  |             |
|    approx_kl            | 0.005852623 |
|    clip_fraction        | 0.0577      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.116       |
|    n_updates            | 8440        |
|    policy_gradient_loss | -0.00397    |
|    std                  | 0.838       |
|    value_loss           | 0.0859      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 237     |
|    iterations      | 845     |
|    time_elapsed    | 14602   |
|    total_timesteps | 3461120 |
--------------------------------
Eval num_timesteps=3465000, episode_reward=-236.63 +/- 81.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -237         |
| time/                   |              |
|    total_timesteps      | 3465000      |
| train/                  |              |
|    approx_kl            | 0.0054634563 |
|    clip_fraction        | 0.0616       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.63        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0169       |
|    n_updates            | 8450         |
|    policy_gradient_loss | -0.00331     |
|    std                  | 0.837        |
|    value_loss           | 0.81         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 237     |
|    iterations      | 846     |
|    time_elapsed    | 14612   |
|    total_timesteps | 3465216 |
--------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 847        |
|    time_elapsed         | 14620      |
|    total_timesteps      | 3469312    |
| train/                  |            |
|    approx_kl            | 0.00326332 |
|    clip_fraction        | 0.0421     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.63      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0322     |
|    n_updates            | 8460       |
|    policy_gradient_loss | -0.00211   |
|    std                  | 0.838      |
|    value_loss           | 0.744      |
----------------------------------------
Eval num_timesteps=3470000, episode_reward=-237.72 +/- 85.45
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -238        |
| time/                   |             |
|    total_timesteps      | 3470000     |
| train/                  |             |
|    approx_kl            | 0.006912497 |
|    clip_fraction        | 0.0617      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0273      |
|    n_updates            | 8470        |
|    policy_gradient_loss | -0.004      |
|    std                  | 0.84        |
|    value_loss           | 0.301       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 237     |
|    iterations      | 848     |
|    time_elapsed    | 14630   |
|    total_timesteps | 3473408 |
--------------------------------
Eval num_timesteps=3475000, episode_reward=-409.44 +/- 130.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -409         |
| time/                   |              |
|    total_timesteps      | 3475000      |
| train/                  |              |
|    approx_kl            | 0.0068791276 |
|    clip_fraction        | 0.0568       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.207        |
|    n_updates            | 8480         |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.834        |
|    value_loss           | 0.18         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 237     |
|    iterations      | 849     |
|    time_elapsed    | 14640   |
|    total_timesteps | 3477504 |
--------------------------------
Eval num_timesteps=3480000, episode_reward=-326.99 +/- 88.34
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -327        |
| time/                   |             |
|    total_timesteps      | 3480000     |
| train/                  |             |
|    approx_kl            | 0.003938942 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0417      |
|    n_updates            | 8490        |
|    policy_gradient_loss | -0.00208    |
|    std                  | 0.833       |
|    value_loss           | 8.48        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 237     |
|    iterations      | 850     |
|    time_elapsed    | 14651   |
|    total_timesteps | 3481600 |
--------------------------------
Eval num_timesteps=3485000, episode_reward=-307.08 +/- 163.11
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -307         |
| time/                   |              |
|    total_timesteps      | 3485000      |
| train/                  |              |
|    approx_kl            | 0.0058599925 |
|    clip_fraction        | 0.0393       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0195       |
|    n_updates            | 8500         |
|    policy_gradient_loss | -0.00106     |
|    std                  | 0.837        |
|    value_loss           | 0.214        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 237     |
|    iterations      | 851     |
|    time_elapsed    | 14661   |
|    total_timesteps | 3485696 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 237          |
|    iterations           | 852          |
|    time_elapsed         | 14669        |
|    total_timesteps      | 3489792      |
| train/                  |              |
|    approx_kl            | 0.0058075795 |
|    clip_fraction        | 0.0516       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.17         |
|    n_updates            | 8510         |
|    policy_gradient_loss | -0.00261     |
|    std                  | 0.837        |
|    value_loss           | 9.43         |
------------------------------------------
Eval num_timesteps=3490000, episode_reward=-187.44 +/- 177.02
Episode length: 811.40 +/- 379.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -187         |
| time/                   |              |
|    total_timesteps      | 3490000      |
| train/                  |              |
|    approx_kl            | 0.0049941046 |
|    clip_fraction        | 0.0369       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.63        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0373       |
|    n_updates            | 8520         |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.836        |
|    value_loss           | 0.0354       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 238     |
|    iterations      | 853     |
|    time_elapsed    | 14678   |
|    total_timesteps | 3493888 |
--------------------------------
Eval num_timesteps=3495000, episode_reward=-408.04 +/- 194.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -408         |
| time/                   |              |
|    total_timesteps      | 3495000      |
| train/                  |              |
|    approx_kl            | 0.0052735293 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0209       |
|    n_updates            | 8530         |
|    policy_gradient_loss | -0.00328     |
|    std                  | 0.834        |
|    value_loss           | 0.203        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 238     |
|    iterations      | 854     |
|    time_elapsed    | 14689   |
|    total_timesteps | 3497984 |
--------------------------------
Eval num_timesteps=3500000, episode_reward=-405.29 +/- 201.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -405         |
| time/                   |              |
|    total_timesteps      | 3500000      |
| train/                  |              |
|    approx_kl            | 0.0038213728 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0231       |
|    n_updates            | 8540         |
|    policy_gradient_loss | -0.000761    |
|    std                  | 0.834        |
|    value_loss           | 1.08         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 238     |
|    iterations      | 855     |
|    time_elapsed    | 14699   |
|    total_timesteps | 3502080 |
--------------------------------
Eval num_timesteps=3505000, episode_reward=-360.51 +/- 119.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -361         |
| time/                   |              |
|    total_timesteps      | 3505000      |
| train/                  |              |
|    approx_kl            | 0.0046658698 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0376       |
|    n_updates            | 8550         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 0.829        |
|    value_loss           | 4.95         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 238     |
|    iterations      | 856     |
|    time_elapsed    | 14710   |
|    total_timesteps | 3506176 |
--------------------------------
Eval num_timesteps=3510000, episode_reward=-237.17 +/- 65.37
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -237       |
| time/                   |            |
|    total_timesteps      | 3510000    |
| train/                  |            |
|    approx_kl            | 0.00456201 |
|    clip_fraction        | 0.0312     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.6       |
|    explained_variance   | 0.988      |
|    learning_rate        | 0.0001     |
|    loss                 | 54.9       |
|    n_updates            | 8560       |
|    policy_gradient_loss | -5.4e-05   |
|    std                  | 0.827      |
|    value_loss           | 18.4       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 238     |
|    iterations      | 857     |
|    time_elapsed    | 14720   |
|    total_timesteps | 3510272 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 238          |
|    iterations           | 858          |
|    time_elapsed         | 14727        |
|    total_timesteps      | 3514368      |
| train/                  |              |
|    approx_kl            | 0.0041007237 |
|    clip_fraction        | 0.0375       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00808      |
|    n_updates            | 8570         |
|    policy_gradient_loss | -0.00171     |
|    std                  | 0.827        |
|    value_loss           | 4.04         |
------------------------------------------
Eval num_timesteps=3515000, episode_reward=-341.59 +/- 181.22
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -342         |
| time/                   |              |
|    total_timesteps      | 3515000      |
| train/                  |              |
|    approx_kl            | 0.0058399215 |
|    clip_fraction        | 0.0452       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.6         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0174       |
|    n_updates            | 8580         |
|    policy_gradient_loss | -0.00231     |
|    std                  | 0.831        |
|    value_loss           | 0.0491       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 238     |
|    iterations      | 859     |
|    time_elapsed    | 14739   |
|    total_timesteps | 3518464 |
--------------------------------
Eval num_timesteps=3520000, episode_reward=-280.58 +/- 137.07
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -281         |
| time/                   |              |
|    total_timesteps      | 3520000      |
| train/                  |              |
|    approx_kl            | 0.0054592984 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.000383    |
|    n_updates            | 8590         |
|    policy_gradient_loss | -0.00236     |
|    std                  | 0.831        |
|    value_loss           | 1.58         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 238     |
|    iterations      | 860     |
|    time_elapsed    | 14749   |
|    total_timesteps | 3522560 |
--------------------------------
Eval num_timesteps=3525000, episode_reward=-346.70 +/- 134.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -347         |
| time/                   |              |
|    total_timesteps      | 3525000      |
| train/                  |              |
|    approx_kl            | 0.0053280084 |
|    clip_fraction        | 0.0261       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0169       |
|    n_updates            | 8600         |
|    policy_gradient_loss | -0.000821    |
|    std                  | 0.829        |
|    value_loss           | 4.45         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 238     |
|    iterations      | 861     |
|    time_elapsed    | 14761   |
|    total_timesteps | 3526656 |
--------------------------------
Eval num_timesteps=3530000, episode_reward=-323.52 +/- 192.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 3530000      |
| train/                  |              |
|    approx_kl            | 0.0045205406 |
|    clip_fraction        | 0.0307       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.6         |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.294        |
|    n_updates            | 8610         |
|    policy_gradient_loss | -0.00102     |
|    std                  | 0.828        |
|    value_loss           | 1.94         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 238     |
|    iterations      | 862     |
|    time_elapsed    | 14773   |
|    total_timesteps | 3530752 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 239          |
|    iterations           | 863          |
|    time_elapsed         | 14782        |
|    total_timesteps      | 3534848      |
| train/                  |              |
|    approx_kl            | 0.0069019627 |
|    clip_fraction        | 0.067        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.106        |
|    n_updates            | 8620         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.832        |
|    value_loss           | 0.47         |
------------------------------------------
Eval num_timesteps=3535000, episode_reward=-373.23 +/- 158.51
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -373        |
| time/                   |             |
|    total_timesteps      | 3535000     |
| train/                  |             |
|    approx_kl            | 0.008041807 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00168    |
|    n_updates            | 8630        |
|    policy_gradient_loss | -0.00525    |
|    std                  | 0.829       |
|    value_loss           | 0.0605      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 239     |
|    iterations      | 864     |
|    time_elapsed    | 14795   |
|    total_timesteps | 3538944 |
--------------------------------
Eval num_timesteps=3540000, episode_reward=-324.33 +/- 204.33
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 3540000      |
| train/                  |              |
|    approx_kl            | 0.0051098866 |
|    clip_fraction        | 0.0313       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.62        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.18         |
|    n_updates            | 8640         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 0.833        |
|    value_loss           | 0.389        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 239     |
|    iterations      | 865     |
|    time_elapsed    | 14807   |
|    total_timesteps | 3543040 |
--------------------------------
Eval num_timesteps=3545000, episode_reward=-370.46 +/- 52.55
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -370        |
| time/                   |             |
|    total_timesteps      | 3545000     |
| train/                  |             |
|    approx_kl            | 0.004533529 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.62       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.89        |
|    n_updates            | 8650        |
|    policy_gradient_loss | -0.00167    |
|    std                  | 0.827       |
|    value_loss           | 0.553       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 239     |
|    iterations      | 866     |
|    time_elapsed    | 14817   |
|    total_timesteps | 3547136 |
--------------------------------
Eval num_timesteps=3550000, episode_reward=-333.40 +/- 171.33
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -333         |
| time/                   |              |
|    total_timesteps      | 3550000      |
| train/                  |              |
|    approx_kl            | 0.0048243357 |
|    clip_fraction        | 0.029        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.61        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.126        |
|    n_updates            | 8660         |
|    policy_gradient_loss | -0.000805    |
|    std                  | 0.827        |
|    value_loss           | 5.91         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 239     |
|    iterations      | 867     |
|    time_elapsed    | 14826   |
|    total_timesteps | 3551232 |
--------------------------------
Eval num_timesteps=3555000, episode_reward=-273.66 +/- 179.99
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -274        |
| time/                   |             |
|    total_timesteps      | 3555000     |
| train/                  |             |
|    approx_kl            | 0.005771198 |
|    clip_fraction        | 0.0357      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.8         |
|    n_updates            | 8670        |
|    policy_gradient_loss | -0.0011     |
|    std                  | 0.824       |
|    value_loss           | 0.936       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 239     |
|    iterations      | 868     |
|    time_elapsed    | 14836   |
|    total_timesteps | 3555328 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 239          |
|    iterations           | 869          |
|    time_elapsed         | 14843        |
|    total_timesteps      | 3559424      |
| train/                  |              |
|    approx_kl            | 0.0073835836 |
|    clip_fraction        | 0.0609       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0001       |
|    loss                 | 4.86         |
|    n_updates            | 8680         |
|    policy_gradient_loss | -0.00053     |
|    std                  | 0.823        |
|    value_loss           | 3.13         |
------------------------------------------
Eval num_timesteps=3560000, episode_reward=-314.37 +/- 197.36
Episode length: 804.60 +/- 392.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 805          |
|    mean_reward          | -314         |
| time/                   |              |
|    total_timesteps      | 3560000      |
| train/                  |              |
|    approx_kl            | 0.0071128076 |
|    clip_fraction        | 0.077        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.6         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0332       |
|    n_updates            | 8690         |
|    policy_gradient_loss | -0.00461     |
|    std                  | 0.831        |
|    value_loss           | 0.0407       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 239     |
|    iterations      | 870     |
|    time_elapsed    | 14852   |
|    total_timesteps | 3563520 |
--------------------------------
Eval num_timesteps=3565000, episode_reward=-388.24 +/- 252.11
Episode length: 807.20 +/- 387.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 807         |
|    mean_reward          | -388        |
| time/                   |             |
|    total_timesteps      | 3565000     |
| train/                  |             |
|    approx_kl            | 0.005769916 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 6.9         |
|    n_updates            | 8700        |
|    policy_gradient_loss | -0.00243    |
|    std                  | 0.826       |
|    value_loss           | 3.58        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 240     |
|    iterations      | 871     |
|    time_elapsed    | 14862   |
|    total_timesteps | 3567616 |
--------------------------------
Eval num_timesteps=3570000, episode_reward=-370.65 +/- 179.02
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -371        |
| time/                   |             |
|    total_timesteps      | 3570000     |
| train/                  |             |
|    approx_kl            | 0.004912848 |
|    clip_fraction        | 0.04        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0765      |
|    n_updates            | 8710        |
|    policy_gradient_loss | -0.00177    |
|    std                  | 0.825       |
|    value_loss           | 10.6        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 240     |
|    iterations      | 872     |
|    time_elapsed    | 14873   |
|    total_timesteps | 3571712 |
--------------------------------
Eval num_timesteps=3575000, episode_reward=-318.41 +/- 109.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -318         |
| time/                   |              |
|    total_timesteps      | 3575000      |
| train/                  |              |
|    approx_kl            | 0.0037161326 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.6         |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0382       |
|    n_updates            | 8720         |
|    policy_gradient_loss | -0.000936    |
|    std                  | 0.825        |
|    value_loss           | 6.92         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 240     |
|    iterations      | 873     |
|    time_elapsed    | 14883   |
|    total_timesteps | 3575808 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 874         |
|    time_elapsed         | 14890       |
|    total_timesteps      | 3579904     |
| train/                  |             |
|    approx_kl            | 0.004216058 |
|    clip_fraction        | 0.0235      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.6        |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.63        |
|    n_updates            | 8730        |
|    policy_gradient_loss | 0.000425    |
|    std                  | 0.825       |
|    value_loss           | 5.25        |
-----------------------------------------
Eval num_timesteps=3580000, episode_reward=-206.26 +/- 123.76
Episode length: 812.80 +/- 376.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 813         |
|    mean_reward          | -206        |
| time/                   |             |
|    total_timesteps      | 3580000     |
| train/                  |             |
|    approx_kl            | 0.005022517 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0284      |
|    n_updates            | 8740        |
|    policy_gradient_loss | -0.00343    |
|    std                  | 0.824       |
|    value_loss           | 0.0383      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 240     |
|    iterations      | 875     |
|    time_elapsed    | 14900   |
|    total_timesteps | 3584000 |
--------------------------------
Eval num_timesteps=3585000, episode_reward=-286.02 +/- 140.39
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 3585000      |
| train/                  |              |
|    approx_kl            | 0.0036047953 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0548       |
|    n_updates            | 8750         |
|    policy_gradient_loss | -0.000247    |
|    std                  | 0.823        |
|    value_loss           | 0.656        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 240     |
|    iterations      | 876     |
|    time_elapsed    | 14911   |
|    total_timesteps | 3588096 |
--------------------------------
Eval num_timesteps=3590000, episode_reward=-357.57 +/- 118.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -358         |
| time/                   |              |
|    total_timesteps      | 3590000      |
| train/                  |              |
|    approx_kl            | 0.0057321116 |
|    clip_fraction        | 0.0569       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.17         |
|    n_updates            | 8760         |
|    policy_gradient_loss | -0.00308     |
|    std                  | 0.824        |
|    value_loss           | 0.319        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 240     |
|    iterations      | 877     |
|    time_elapsed    | 14922   |
|    total_timesteps | 3592192 |
--------------------------------
Eval num_timesteps=3595000, episode_reward=-329.70 +/- 173.26
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -330         |
| time/                   |              |
|    total_timesteps      | 3595000      |
| train/                  |              |
|    approx_kl            | 0.0059660934 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.111        |
|    n_updates            | 8770         |
|    policy_gradient_loss | -0.00337     |
|    std                  | 0.821        |
|    value_loss           | 0.984        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 240     |
|    iterations      | 878     |
|    time_elapsed    | 14934   |
|    total_timesteps | 3596288 |
--------------------------------
Eval num_timesteps=3600000, episode_reward=-274.96 +/- 136.10
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -275        |
| time/                   |             |
|    total_timesteps      | 3600000     |
| train/                  |             |
|    approx_kl            | 0.004001442 |
|    clip_fraction        | 0.0362      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.12        |
|    n_updates            | 8780        |
|    policy_gradient_loss | -0.00217    |
|    std                  | 0.82        |
|    value_loss           | 4.32        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 240     |
|    iterations      | 879     |
|    time_elapsed    | 14951   |
|    total_timesteps | 3600384 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 240          |
|    iterations           | 880          |
|    time_elapsed         | 14960        |
|    total_timesteps      | 3604480      |
| train/                  |              |
|    approx_kl            | 0.0057928325 |
|    clip_fraction        | 0.0515       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.13         |
|    n_updates            | 8790         |
|    policy_gradient_loss | -0.00116     |
|    std                  | 0.82         |
|    value_loss           | 0.912        |
------------------------------------------
Eval num_timesteps=3605000, episode_reward=-427.93 +/- 195.49
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -428      |
| time/                   |           |
|    total_timesteps      | 3605000   |
| train/                  |           |
|    approx_kl            | 0.0074768 |
|    clip_fraction        | 0.0718    |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.59     |
|    explained_variance   | 1         |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0163    |
|    n_updates            | 8800      |
|    policy_gradient_loss | -0.00468  |
|    std                  | 0.823     |
|    value_loss           | 0.0345    |
---------------------------------------
--------------------------------
| time/              |         |
|    fps             | 241     |
|    iterations      | 881     |
|    time_elapsed    | 14973   |
|    total_timesteps | 3608576 |
--------------------------------
Eval num_timesteps=3610000, episode_reward=-294.25 +/- 151.92
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -294       |
| time/                   |            |
|    total_timesteps      | 3610000    |
| train/                  |            |
|    approx_kl            | 0.00478753 |
|    clip_fraction        | 0.04       |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.59      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.252      |
|    n_updates            | 8810       |
|    policy_gradient_loss | -0.00182   |
|    std                  | 0.822      |
|    value_loss           | 3.01       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 241     |
|    iterations      | 882     |
|    time_elapsed    | 14982   |
|    total_timesteps | 3612672 |
--------------------------------
Eval num_timesteps=3615000, episode_reward=-156.61 +/- 45.46
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -157        |
| time/                   |             |
|    total_timesteps      | 3615000     |
| train/                  |             |
|    approx_kl            | 0.003107749 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00222     |
|    n_updates            | 8820        |
|    policy_gradient_loss | -0.00144    |
|    std                  | 0.824       |
|    value_loss           | 7.35        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 241     |
|    iterations      | 883     |
|    time_elapsed    | 14992   |
|    total_timesteps | 3616768 |
--------------------------------
Eval num_timesteps=3620000, episode_reward=-249.69 +/- 126.46
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -250        |
| time/                   |             |
|    total_timesteps      | 3620000     |
| train/                  |             |
|    approx_kl            | 0.005126157 |
|    clip_fraction        | 0.0422      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.52        |
|    n_updates            | 8830        |
|    policy_gradient_loss | -0.00193    |
|    std                  | 0.822       |
|    value_loss           | 0.64        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 241     |
|    iterations      | 884     |
|    time_elapsed    | 15002   |
|    total_timesteps | 3620864 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 885         |
|    time_elapsed         | 15009       |
|    total_timesteps      | 3624960     |
| train/                  |             |
|    approx_kl            | 0.005667033 |
|    clip_fraction        | 0.0344      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0118      |
|    n_updates            | 8840        |
|    policy_gradient_loss | -0.00158    |
|    std                  | 0.823       |
|    value_loss           | 3.65        |
-----------------------------------------
Eval num_timesteps=3625000, episode_reward=-196.16 +/- 150.93
Episode length: 812.80 +/- 376.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -196         |
| time/                   |              |
|    total_timesteps      | 3625000      |
| train/                  |              |
|    approx_kl            | 0.0044149836 |
|    clip_fraction        | 0.0471       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00676     |
|    n_updates            | 8850         |
|    policy_gradient_loss | -0.00253     |
|    std                  | 0.823        |
|    value_loss           | 0.0933       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 241     |
|    iterations      | 886     |
|    time_elapsed    | 15019   |
|    total_timesteps | 3629056 |
--------------------------------
Eval num_timesteps=3630000, episode_reward=-292.36 +/- 104.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -292         |
| time/                   |              |
|    total_timesteps      | 3630000      |
| train/                  |              |
|    approx_kl            | 0.0040537887 |
|    clip_fraction        | 0.0344       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.893        |
|    n_updates            | 8860         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 0.824        |
|    value_loss           | 4.73         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 241     |
|    iterations      | 887     |
|    time_elapsed    | 15030   |
|    total_timesteps | 3633152 |
--------------------------------
Eval num_timesteps=3635000, episode_reward=-372.07 +/- 197.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -372         |
| time/                   |              |
|    total_timesteps      | 3635000      |
| train/                  |              |
|    approx_kl            | 0.0061323214 |
|    clip_fraction        | 0.05         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0243       |
|    n_updates            | 8870         |
|    policy_gradient_loss | -0.00218     |
|    std                  | 0.823        |
|    value_loss           | 2.14         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 241     |
|    iterations      | 888     |
|    time_elapsed    | 15041   |
|    total_timesteps | 3637248 |
--------------------------------
Eval num_timesteps=3640000, episode_reward=-140.79 +/- 110.89
Episode length: 812.20 +/- 377.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 812         |
|    mean_reward          | -141        |
| time/                   |             |
|    total_timesteps      | 3640000     |
| train/                  |             |
|    approx_kl            | 0.005872651 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.19        |
|    n_updates            | 8880        |
|    policy_gradient_loss | -0.00377    |
|    std                  | 0.823       |
|    value_loss           | 1.93        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 241     |
|    iterations      | 889     |
|    time_elapsed    | 15053   |
|    total_timesteps | 3641344 |
--------------------------------
Eval num_timesteps=3645000, episode_reward=-307.08 +/- 131.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -307        |
| time/                   |             |
|    total_timesteps      | 3645000     |
| train/                  |             |
|    approx_kl            | 0.008129135 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0101      |
|    n_updates            | 8890        |
|    policy_gradient_loss | -0.00318    |
|    std                  | 0.819       |
|    value_loss           | 0.194       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 241     |
|    iterations      | 890     |
|    time_elapsed    | 15066   |
|    total_timesteps | 3645440 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 242          |
|    iterations           | 891          |
|    time_elapsed         | 15073        |
|    total_timesteps      | 3649536      |
| train/                  |              |
|    approx_kl            | 0.0038653065 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0181       |
|    n_updates            | 8900         |
|    policy_gradient_loss | -0.000615    |
|    std                  | 0.818        |
|    value_loss           | 1.16         |
------------------------------------------
Eval num_timesteps=3650000, episode_reward=-196.37 +/- 50.91
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -196         |
| time/                   |              |
|    total_timesteps      | 3650000      |
| train/                  |              |
|    approx_kl            | 0.0057204403 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00506      |
|    n_updates            | 8910         |
|    policy_gradient_loss | -0.000625    |
|    std                  | 0.815        |
|    value_loss           | 0.0806       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 242     |
|    iterations      | 892     |
|    time_elapsed    | 15083   |
|    total_timesteps | 3653632 |
--------------------------------
Eval num_timesteps=3655000, episode_reward=-323.62 +/- 78.59
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -324        |
| time/                   |             |
|    total_timesteps      | 3655000     |
| train/                  |             |
|    approx_kl            | 0.004324778 |
|    clip_fraction        | 0.0266      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0318      |
|    n_updates            | 8920        |
|    policy_gradient_loss | -0.000807   |
|    std                  | 0.811       |
|    value_loss           | 0.0837      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 242     |
|    iterations      | 893     |
|    time_elapsed    | 15095   |
|    total_timesteps | 3657728 |
--------------------------------
Eval num_timesteps=3660000, episode_reward=-252.53 +/- 71.55
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -253         |
| time/                   |              |
|    total_timesteps      | 3660000      |
| train/                  |              |
|    approx_kl            | 0.0050000297 |
|    clip_fraction        | 0.0328       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0151       |
|    n_updates            | 8930         |
|    policy_gradient_loss | -0.00219     |
|    std                  | 0.808        |
|    value_loss           | 0.27         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 242     |
|    iterations      | 894     |
|    time_elapsed    | 15108   |
|    total_timesteps | 3661824 |
--------------------------------
Eval num_timesteps=3665000, episode_reward=-405.68 +/- 188.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -406         |
| time/                   |              |
|    total_timesteps      | 3665000      |
| train/                  |              |
|    approx_kl            | 0.0038956737 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00478      |
|    n_updates            | 8940         |
|    policy_gradient_loss | -0.00208     |
|    std                  | 0.804        |
|    value_loss           | 1.33         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 242     |
|    iterations      | 895     |
|    time_elapsed    | 15119   |
|    total_timesteps | 3665920 |
--------------------------------
Eval num_timesteps=3670000, episode_reward=-206.83 +/- 204.07
Episode length: 806.40 +/- 389.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 806          |
|    mean_reward          | -207         |
| time/                   |              |
|    total_timesteps      | 3670000      |
| train/                  |              |
|    approx_kl            | 0.0038743485 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 11.9         |
|    n_updates            | 8950         |
|    policy_gradient_loss | -0.00038     |
|    std                  | 0.804        |
|    value_loss           | 3.97         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 242     |
|    iterations      | 896     |
|    time_elapsed    | 15129   |
|    total_timesteps | 3670016 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 897         |
|    time_elapsed         | 15138       |
|    total_timesteps      | 3674112     |
| train/                  |             |
|    approx_kl            | 0.006126604 |
|    clip_fraction        | 0.0539      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0323      |
|    n_updates            | 8960        |
|    policy_gradient_loss | -0.00358    |
|    std                  | 0.808       |
|    value_loss           | 1.85        |
-----------------------------------------
Eval num_timesteps=3675000, episode_reward=-371.53 +/- 135.24
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -372        |
| time/                   |             |
|    total_timesteps      | 3675000     |
| train/                  |             |
|    approx_kl            | 0.007225461 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00866    |
|    n_updates            | 8970        |
|    policy_gradient_loss | -0.00445    |
|    std                  | 0.808       |
|    value_loss           | 0.047       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 242     |
|    iterations      | 898     |
|    time_elapsed    | 15148   |
|    total_timesteps | 3678208 |
--------------------------------
Eval num_timesteps=3680000, episode_reward=-328.57 +/- 225.00
Episode length: 806.20 +/- 389.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 806          |
|    mean_reward          | -329         |
| time/                   |              |
|    total_timesteps      | 3680000      |
| train/                  |              |
|    approx_kl            | 0.0051610004 |
|    clip_fraction        | 0.0263       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 33           |
|    n_updates            | 8980         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.807        |
|    value_loss           | 9.69         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 242     |
|    iterations      | 899     |
|    time_elapsed    | 15159   |
|    total_timesteps | 3682304 |
--------------------------------
Eval num_timesteps=3685000, episode_reward=-397.59 +/- 50.22
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -398         |
| time/                   |              |
|    total_timesteps      | 3685000      |
| train/                  |              |
|    approx_kl            | 0.0068604834 |
|    clip_fraction        | 0.0731       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.29         |
|    n_updates            | 8990         |
|    policy_gradient_loss | -0.00319     |
|    std                  | 0.807        |
|    value_loss           | 0.964        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 243     |
|    iterations      | 900     |
|    time_elapsed    | 15169   |
|    total_timesteps | 3686400 |
--------------------------------
Eval num_timesteps=3690000, episode_reward=-337.16 +/- 105.15
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -337        |
| time/                   |             |
|    total_timesteps      | 3690000     |
| train/                  |             |
|    approx_kl            | 0.006942572 |
|    clip_fraction        | 0.0554      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00234     |
|    n_updates            | 9000        |
|    policy_gradient_loss | -0.00263    |
|    std                  | 0.803       |
|    value_loss           | 2.55        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 243     |
|    iterations      | 901     |
|    time_elapsed    | 15180   |
|    total_timesteps | 3690496 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 902         |
|    time_elapsed         | 15187       |
|    total_timesteps      | 3694592     |
| train/                  |             |
|    approx_kl            | 0.007710859 |
|    clip_fraction        | 0.0711      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.972       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0175      |
|    n_updates            | 9010        |
|    policy_gradient_loss | -0.00356    |
|    std                  | 0.804       |
|    value_loss           | 3.11        |
-----------------------------------------
Eval num_timesteps=3695000, episode_reward=-229.45 +/- 132.66
Episode length: 808.40 +/- 385.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 808         |
|    mean_reward          | -229        |
| time/                   |             |
|    total_timesteps      | 3695000     |
| train/                  |             |
|    approx_kl            | 0.006049825 |
|    clip_fraction        | 0.0458      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0651      |
|    n_updates            | 9020        |
|    policy_gradient_loss | -0.00162    |
|    std                  | 0.803       |
|    value_loss           | 1.12        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 243     |
|    iterations      | 903     |
|    time_elapsed    | 15196   |
|    total_timesteps | 3698688 |
--------------------------------
Eval num_timesteps=3700000, episode_reward=-304.39 +/- 172.29
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -304        |
| time/                   |             |
|    total_timesteps      | 3700000     |
| train/                  |             |
|    approx_kl            | 0.004580044 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 6.29        |
|    n_updates            | 9030        |
|    policy_gradient_loss | -0.00302    |
|    std                  | 0.805       |
|    value_loss           | 3.01        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 243     |
|    iterations      | 904     |
|    time_elapsed    | 15206   |
|    total_timesteps | 3702784 |
--------------------------------
Eval num_timesteps=3705000, episode_reward=-319.65 +/- 197.15
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -320         |
| time/                   |              |
|    total_timesteps      | 3705000      |
| train/                  |              |
|    approx_kl            | 0.0058966978 |
|    clip_fraction        | 0.0507       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0014       |
|    n_updates            | 9040         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 0.804        |
|    value_loss           | 0.061        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 243     |
|    iterations      | 905     |
|    time_elapsed    | 15216   |
|    total_timesteps | 3706880 |
--------------------------------
Eval num_timesteps=3710000, episode_reward=-243.04 +/- 116.91
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -243        |
| time/                   |             |
|    total_timesteps      | 3710000     |
| train/                  |             |
|    approx_kl            | 0.005647739 |
|    clip_fraction        | 0.0626      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0183      |
|    n_updates            | 9050        |
|    policy_gradient_loss | -0.00388    |
|    std                  | 0.804       |
|    value_loss           | 1.04        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 243     |
|    iterations      | 906     |
|    time_elapsed    | 15225   |
|    total_timesteps | 3710976 |
--------------------------------
Eval num_timesteps=3715000, episode_reward=-309.53 +/- 157.02
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -310        |
| time/                   |             |
|    total_timesteps      | 3715000     |
| train/                  |             |
|    approx_kl            | 0.006553329 |
|    clip_fraction        | 0.0671      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.158       |
|    n_updates            | 9060        |
|    policy_gradient_loss | -0.00297    |
|    std                  | 0.8         |
|    value_loss           | 0.268       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 243     |
|    iterations      | 907     |
|    time_elapsed    | 15234   |
|    total_timesteps | 3715072 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 244          |
|    iterations           | 908          |
|    time_elapsed         | 15241        |
|    total_timesteps      | 3719168      |
| train/                  |              |
|    approx_kl            | 0.0060574436 |
|    clip_fraction        | 0.0508       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00291      |
|    n_updates            | 9070         |
|    policy_gradient_loss | -0.00282     |
|    std                  | 0.805        |
|    value_loss           | 0.365        |
------------------------------------------
Eval num_timesteps=3720000, episode_reward=-286.90 +/- 127.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -287         |
| time/                   |              |
|    total_timesteps      | 3720000      |
| train/                  |              |
|    approx_kl            | 0.0030936007 |
|    clip_fraction        | 0.0221       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00325      |
|    n_updates            | 9080         |
|    policy_gradient_loss | 0.000271     |
|    std                  | 0.801        |
|    value_loss           | 0.041        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 244     |
|    iterations      | 909     |
|    time_elapsed    | 15251   |
|    total_timesteps | 3723264 |
--------------------------------
Eval num_timesteps=3725000, episode_reward=-207.38 +/- 58.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -207         |
| time/                   |              |
|    total_timesteps      | 3725000      |
| train/                  |              |
|    approx_kl            | 0.0035475504 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0246       |
|    n_updates            | 9090         |
|    policy_gradient_loss | -0.0015      |
|    std                  | 0.812        |
|    value_loss           | 0.133        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 244     |
|    iterations      | 910     |
|    time_elapsed    | 15261   |
|    total_timesteps | 3727360 |
--------------------------------
Eval num_timesteps=3730000, episode_reward=-229.14 +/- 83.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -229         |
| time/                   |              |
|    total_timesteps      | 3730000      |
| train/                  |              |
|    approx_kl            | 0.0055683586 |
|    clip_fraction        | 0.0553       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0425       |
|    n_updates            | 9100         |
|    policy_gradient_loss | -0.00338     |
|    std                  | 0.812        |
|    value_loss           | 1.16         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 244     |
|    iterations      | 911     |
|    time_elapsed    | 15271   |
|    total_timesteps | 3731456 |
--------------------------------
Eval num_timesteps=3735000, episode_reward=-452.70 +/- 244.05
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -453        |
| time/                   |             |
|    total_timesteps      | 3735000     |
| train/                  |             |
|    approx_kl            | 0.006227824 |
|    clip_fraction        | 0.042       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0169      |
|    n_updates            | 9110        |
|    policy_gradient_loss | -0.00152    |
|    std                  | 0.812       |
|    value_loss           | 0.237       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 244     |
|    iterations      | 912     |
|    time_elapsed    | 15280   |
|    total_timesteps | 3735552 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 244          |
|    iterations           | 913          |
|    time_elapsed         | 15287        |
|    total_timesteps      | 3739648      |
| train/                  |              |
|    approx_kl            | 0.0033311415 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0215       |
|    n_updates            | 9120         |
|    policy_gradient_loss | 0.000264     |
|    std                  | 0.811        |
|    value_loss           | 2.12         |
------------------------------------------
Eval num_timesteps=3740000, episode_reward=-306.53 +/- 116.97
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -307        |
| time/                   |             |
|    total_timesteps      | 3740000     |
| train/                  |             |
|    approx_kl            | 0.007369387 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00595    |
|    n_updates            | 9130        |
|    policy_gradient_loss | -0.00521    |
|    std                  | 0.816       |
|    value_loss           | 0.0104      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 244     |
|    iterations      | 914     |
|    time_elapsed    | 15298   |
|    total_timesteps | 3743744 |
--------------------------------
Eval num_timesteps=3745000, episode_reward=-337.54 +/- 186.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -338        |
| time/                   |             |
|    total_timesteps      | 3745000     |
| train/                  |             |
|    approx_kl            | 0.003610082 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00333     |
|    n_updates            | 9140        |
|    policy_gradient_loss | -0.000995   |
|    std                  | 0.818       |
|    value_loss           | 0.296       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 244     |
|    iterations      | 915     |
|    time_elapsed    | 15311   |
|    total_timesteps | 3747840 |
--------------------------------
Eval num_timesteps=3750000, episode_reward=-317.82 +/- 92.57
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -318        |
| time/                   |             |
|    total_timesteps      | 3750000     |
| train/                  |             |
|    approx_kl            | 0.004996499 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0154      |
|    n_updates            | 9150        |
|    policy_gradient_loss | -0.000648   |
|    std                  | 0.82        |
|    value_loss           | 2.82        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 244     |
|    iterations      | 916     |
|    time_elapsed    | 15324   |
|    total_timesteps | 3751936 |
--------------------------------
Eval num_timesteps=3755000, episode_reward=-249.45 +/- 155.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -249         |
| time/                   |              |
|    total_timesteps      | 3755000      |
| train/                  |              |
|    approx_kl            | 0.0043101474 |
|    clip_fraction        | 0.0236       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.59        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0199       |
|    n_updates            | 9160         |
|    policy_gradient_loss | -0.00128     |
|    std                  | 0.82         |
|    value_loss           | 1.59         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 244     |
|    iterations      | 917     |
|    time_elapsed    | 15338   |
|    total_timesteps | 3756032 |
--------------------------------
Eval num_timesteps=3760000, episode_reward=-166.54 +/- 62.94
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -167        |
| time/                   |             |
|    total_timesteps      | 3760000     |
| train/                  |             |
|    approx_kl            | 0.005054734 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 34.7        |
|    n_updates            | 9170        |
|    policy_gradient_loss | -0.00293    |
|    std                  | 0.818       |
|    value_loss           | 10.1        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 244     |
|    iterations      | 918     |
|    time_elapsed    | 15350   |
|    total_timesteps | 3760128 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 245          |
|    iterations           | 919          |
|    time_elapsed         | 15358        |
|    total_timesteps      | 3764224      |
| train/                  |              |
|    approx_kl            | 0.0063487617 |
|    clip_fraction        | 0.0562       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.458        |
|    n_updates            | 9180         |
|    policy_gradient_loss | -0.00194     |
|    std                  | 0.815        |
|    value_loss           | 16.7         |
------------------------------------------
Eval num_timesteps=3765000, episode_reward=-286.05 +/- 181.33
Episode length: 804.20 +/- 393.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 804          |
|    mean_reward          | -286         |
| time/                   |              |
|    total_timesteps      | 3765000      |
| train/                  |              |
|    approx_kl            | 0.0061433734 |
|    clip_fraction        | 0.0601       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0166       |
|    n_updates            | 9190         |
|    policy_gradient_loss | -0.00237     |
|    std                  | 0.81         |
|    value_loss           | 0.0484       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 245     |
|    iterations      | 920     |
|    time_elapsed    | 15370   |
|    total_timesteps | 3768320 |
--------------------------------
Eval num_timesteps=3770000, episode_reward=-268.30 +/- 192.50
Episode length: 810.20 +/- 381.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | -268        |
| time/                   |             |
|    total_timesteps      | 3770000     |
| train/                  |             |
|    approx_kl            | 0.005611336 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.56       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.94        |
|    n_updates            | 9200        |
|    policy_gradient_loss | -0.000548   |
|    std                  | 0.811       |
|    value_loss           | 1.95        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 245     |
|    iterations      | 921     |
|    time_elapsed    | 15379   |
|    total_timesteps | 3772416 |
--------------------------------
Eval num_timesteps=3775000, episode_reward=-277.70 +/- 26.37
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -278         |
| time/                   |              |
|    total_timesteps      | 3775000      |
| train/                  |              |
|    approx_kl            | 0.0031426395 |
|    clip_fraction        | 0.031        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0656       |
|    n_updates            | 9210         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 0.809        |
|    value_loss           | 2.13         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 245     |
|    iterations      | 922     |
|    time_elapsed    | 15391   |
|    total_timesteps | 3776512 |
--------------------------------
Eval num_timesteps=3780000, episode_reward=-153.20 +/- 177.07
Episode length: 637.80 +/- 444.84
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 638          |
|    mean_reward          | -153         |
| time/                   |              |
|    total_timesteps      | 3780000      |
| train/                  |              |
|    approx_kl            | 0.0051448196 |
|    clip_fraction        | 0.0404       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0814       |
|    n_updates            | 9220         |
|    policy_gradient_loss | -0.00161     |
|    std                  | 0.81         |
|    value_loss           | 2.4          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 245     |
|    iterations      | 923     |
|    time_elapsed    | 15400   |
|    total_timesteps | 3780608 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 245          |
|    iterations           | 924          |
|    time_elapsed         | 15407        |
|    total_timesteps      | 3784704      |
| train/                  |              |
|    approx_kl            | 0.0068021193 |
|    clip_fraction        | 0.0574       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.36         |
|    n_updates            | 9230         |
|    policy_gradient_loss | -0.00124     |
|    std                  | 0.807        |
|    value_loss           | 1.77         |
------------------------------------------
Eval num_timesteps=3785000, episode_reward=-278.60 +/- 189.57
Episode length: 807.20 +/- 387.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 807          |
|    mean_reward          | -279         |
| time/                   |              |
|    total_timesteps      | 3785000      |
| train/                  |              |
|    approx_kl            | 0.0052383468 |
|    clip_fraction        | 0.053        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0442       |
|    n_updates            | 9240         |
|    policy_gradient_loss | -0.00328     |
|    std                  | 0.811        |
|    value_loss           | 0.294        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 245     |
|    iterations      | 925     |
|    time_elapsed    | 15417   |
|    total_timesteps | 3788800 |
--------------------------------
Eval num_timesteps=3790000, episode_reward=-198.99 +/- 74.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -199         |
| time/                   |              |
|    total_timesteps      | 3790000      |
| train/                  |              |
|    approx_kl            | 0.0069816397 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.55        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.227        |
|    n_updates            | 9250         |
|    policy_gradient_loss | -0.000913    |
|    std                  | 0.81         |
|    value_loss           | 14.9         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 245     |
|    iterations      | 926     |
|    time_elapsed    | 15428   |
|    total_timesteps | 3792896 |
--------------------------------
Eval num_timesteps=3795000, episode_reward=-240.51 +/- 191.18
Episode length: 808.80 +/- 384.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 809          |
|    mean_reward          | -241         |
| time/                   |              |
|    total_timesteps      | 3795000      |
| train/                  |              |
|    approx_kl            | 0.0056020115 |
|    clip_fraction        | 0.0348       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.64         |
|    n_updates            | 9260         |
|    policy_gradient_loss | -0.00257     |
|    std                  | 0.807        |
|    value_loss           | 0.7          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 245     |
|    iterations      | 927     |
|    time_elapsed    | 15438   |
|    total_timesteps | 3796992 |
--------------------------------
Eval num_timesteps=3800000, episode_reward=-202.08 +/- 141.83
Episode length: 801.00 +/- 400.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 801          |
|    mean_reward          | -202         |
| time/                   |              |
|    total_timesteps      | 3800000      |
| train/                  |              |
|    approx_kl            | 0.0029647648 |
|    clip_fraction        | 0.0162       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.98         |
|    n_updates            | 9270         |
|    policy_gradient_loss | 0.000493     |
|    std                  | 0.808        |
|    value_loss           | 6.9          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 246     |
|    iterations      | 928     |
|    time_elapsed    | 15448   |
|    total_timesteps | 3801088 |
--------------------------------
Eval num_timesteps=3805000, episode_reward=-346.25 +/- 148.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -346        |
| time/                   |             |
|    total_timesteps      | 3805000     |
| train/                  |             |
|    approx_kl            | 0.005350668 |
|    clip_fraction        | 0.0535      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.54       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0234      |
|    n_updates            | 9280        |
|    policy_gradient_loss | -0.00243    |
|    std                  | 0.807       |
|    value_loss           | 1.09        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 246     |
|    iterations      | 929     |
|    time_elapsed    | 15459   |
|    total_timesteps | 3805184 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 246          |
|    iterations           | 930          |
|    time_elapsed         | 15466        |
|    total_timesteps      | 3809280      |
| train/                  |              |
|    approx_kl            | 0.0061619002 |
|    clip_fraction        | 0.054        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.37         |
|    n_updates            | 9290         |
|    policy_gradient_loss | -0.00154     |
|    std                  | 0.805        |
|    value_loss           | 1.02         |
------------------------------------------
Eval num_timesteps=3810000, episode_reward=-212.44 +/- 94.36
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -212       |
| time/                   |            |
|    total_timesteps      | 3810000    |
| train/                  |            |
|    approx_kl            | 0.00533314 |
|    clip_fraction        | 0.0473     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.54      |
|    explained_variance   | 0.999      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0173     |
|    n_updates            | 9300       |
|    policy_gradient_loss | -0.000591  |
|    std                  | 0.809      |
|    value_loss           | 0.0319     |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 246     |
|    iterations      | 931     |
|    time_elapsed    | 15478   |
|    total_timesteps | 3813376 |
--------------------------------
Eval num_timesteps=3815000, episode_reward=-438.41 +/- 196.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -438         |
| time/                   |              |
|    total_timesteps      | 3815000      |
| train/                  |              |
|    approx_kl            | 0.0050612865 |
|    clip_fraction        | 0.0362       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0181      |
|    n_updates            | 9310         |
|    policy_gradient_loss | -0.00166     |
|    std                  | 0.806        |
|    value_loss           | 1.05         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 246     |
|    iterations      | 932     |
|    time_elapsed    | 15488   |
|    total_timesteps | 3817472 |
--------------------------------
Eval num_timesteps=3820000, episode_reward=-472.71 +/- 220.11
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -473         |
| time/                   |              |
|    total_timesteps      | 3820000      |
| train/                  |              |
|    approx_kl            | 0.0049472777 |
|    clip_fraction        | 0.0314       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0186       |
|    n_updates            | 9320         |
|    policy_gradient_loss | -0.00174     |
|    std                  | 0.806        |
|    value_loss           | 0.17         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 246     |
|    iterations      | 933     |
|    time_elapsed    | 15499   |
|    total_timesteps | 3821568 |
--------------------------------
Eval num_timesteps=3825000, episode_reward=-312.49 +/- 128.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -312         |
| time/                   |              |
|    total_timesteps      | 3825000      |
| train/                  |              |
|    approx_kl            | 0.0056245388 |
|    clip_fraction        | 0.069        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0994       |
|    n_updates            | 9330         |
|    policy_gradient_loss | -0.00382     |
|    std                  | 0.804        |
|    value_loss           | 0.988        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 246     |
|    iterations      | 934     |
|    time_elapsed    | 15509   |
|    total_timesteps | 3825664 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 246          |
|    iterations           | 935          |
|    time_elapsed         | 15516        |
|    total_timesteps      | 3829760      |
| train/                  |              |
|    approx_kl            | 0.0042562247 |
|    clip_fraction        | 0.0233       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.539        |
|    n_updates            | 9340         |
|    policy_gradient_loss | -0.0012      |
|    std                  | 0.803        |
|    value_loss           | 13.1         |
------------------------------------------
Eval num_timesteps=3830000, episode_reward=-297.18 +/- 120.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -297        |
| time/                   |             |
|    total_timesteps      | 3830000     |
| train/                  |             |
|    approx_kl            | 0.006368368 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0155      |
|    n_updates            | 9350        |
|    policy_gradient_loss | -0.00183    |
|    std                  | 0.806       |
|    value_loss           | 0.0554      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 246     |
|    iterations      | 936     |
|    time_elapsed    | 15527   |
|    total_timesteps | 3833856 |
--------------------------------
Eval num_timesteps=3835000, episode_reward=-441.47 +/- 145.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -441         |
| time/                   |              |
|    total_timesteps      | 3835000      |
| train/                  |              |
|    approx_kl            | 0.0069798627 |
|    clip_fraction        | 0.0585       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0627       |
|    n_updates            | 9360         |
|    policy_gradient_loss | -0.00226     |
|    std                  | 0.806        |
|    value_loss           | 4.14         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 246     |
|    iterations      | 937     |
|    time_elapsed    | 15538   |
|    total_timesteps | 3837952 |
--------------------------------
Eval num_timesteps=3840000, episode_reward=-222.61 +/- 96.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -223         |
| time/                   |              |
|    total_timesteps      | 3840000      |
| train/                  |              |
|    approx_kl            | 0.0058226436 |
|    clip_fraction        | 0.0607       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0365       |
|    n_updates            | 9370         |
|    policy_gradient_loss | -0.00446     |
|    std                  | 0.806        |
|    value_loss           | 0.657        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 247     |
|    iterations      | 938     |
|    time_elapsed    | 15549   |
|    total_timesteps | 3842048 |
--------------------------------
Eval num_timesteps=3845000, episode_reward=-390.12 +/- 129.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -390        |
| time/                   |             |
|    total_timesteps      | 3845000     |
| train/                  |             |
|    approx_kl            | 0.004906133 |
|    clip_fraction        | 0.0292      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 10.4        |
|    n_updates            | 9380        |
|    policy_gradient_loss | -0.000873   |
|    std                  | 0.804       |
|    value_loss           | 10.4        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 247     |
|    iterations      | 939     |
|    time_elapsed    | 15560   |
|    total_timesteps | 3846144 |
--------------------------------
Eval num_timesteps=3850000, episode_reward=-350.43 +/- 139.39
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -350         |
| time/                   |              |
|    total_timesteps      | 3850000      |
| train/                  |              |
|    approx_kl            | 0.0035933852 |
|    clip_fraction        | 0.0446       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.245        |
|    n_updates            | 9390         |
|    policy_gradient_loss | -0.00175     |
|    std                  | 0.808        |
|    value_loss           | 18.1         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 247     |
|    iterations      | 940     |
|    time_elapsed    | 15572   |
|    total_timesteps | 3850240 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 247          |
|    iterations           | 941          |
|    time_elapsed         | 15581        |
|    total_timesteps      | 3854336      |
| train/                  |              |
|    approx_kl            | 0.0065219793 |
|    clip_fraction        | 0.0609       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.35         |
|    n_updates            | 9400         |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.808        |
|    value_loss           | 9.52         |
------------------------------------------
Eval num_timesteps=3855000, episode_reward=-207.53 +/- 223.47
Episode length: 808.40 +/- 385.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 808         |
|    mean_reward          | -208        |
| time/                   |             |
|    total_timesteps      | 3855000     |
| train/                  |             |
|    approx_kl            | 0.005886359 |
|    clip_fraction        | 0.0626      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.012       |
|    n_updates            | 9410        |
|    policy_gradient_loss | -0.00276    |
|    std                  | 0.816       |
|    value_loss           | 0.0494      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 247     |
|    iterations      | 942     |
|    time_elapsed    | 15595   |
|    total_timesteps | 3858432 |
--------------------------------
Eval num_timesteps=3860000, episode_reward=-450.91 +/- 149.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -451         |
| time/                   |              |
|    total_timesteps      | 3860000      |
| train/                  |              |
|    approx_kl            | 0.0043091536 |
|    clip_fraction        | 0.0232       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.57        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0374       |
|    n_updates            | 9420         |
|    policy_gradient_loss | -0.0016      |
|    std                  | 0.817        |
|    value_loss           | 4.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 247     |
|    iterations      | 943     |
|    time_elapsed    | 15609   |
|    total_timesteps | 3862528 |
--------------------------------
Eval num_timesteps=3865000, episode_reward=-157.13 +/- 86.21
Episode length: 805.60 +/- 390.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 806         |
|    mean_reward          | -157        |
| time/                   |             |
|    total_timesteps      | 3865000     |
| train/                  |             |
|    approx_kl            | 0.004531697 |
|    clip_fraction        | 0.049       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.608       |
|    n_updates            | 9430        |
|    policy_gradient_loss | -0.00321    |
|    std                  | 0.822       |
|    value_loss           | 2.96        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 247     |
|    iterations      | 944     |
|    time_elapsed    | 15622   |
|    total_timesteps | 3866624 |
--------------------------------
Eval num_timesteps=3870000, episode_reward=-311.61 +/- 258.52
Episode length: 818.20 +/- 365.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 818         |
|    mean_reward          | -312        |
| time/                   |             |
|    total_timesteps      | 3870000     |
| train/                  |             |
|    approx_kl            | 0.005489122 |
|    clip_fraction        | 0.0409      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.58       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.041       |
|    n_updates            | 9440        |
|    policy_gradient_loss | -0.00115    |
|    std                  | 0.818       |
|    value_loss           | 0.257       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 247     |
|    iterations      | 945     |
|    time_elapsed    | 15632   |
|    total_timesteps | 3870720 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 247          |
|    iterations           | 946          |
|    time_elapsed         | 15638        |
|    total_timesteps      | 3874816      |
| train/                  |              |
|    approx_kl            | 0.0048046755 |
|    clip_fraction        | 0.03         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.58        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.11         |
|    n_updates            | 9450         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 0.816        |
|    value_loss           | 12.4         |
------------------------------------------
Eval num_timesteps=3875000, episode_reward=-309.54 +/- 145.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -310         |
| time/                   |              |
|    total_timesteps      | 3875000      |
| train/                  |              |
|    approx_kl            | 0.0052880603 |
|    clip_fraction        | 0.0425       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.56        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0173       |
|    n_updates            | 9460         |
|    policy_gradient_loss | -0.000885    |
|    std                  | 0.809        |
|    value_loss           | 0.0948       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 247     |
|    iterations      | 947     |
|    time_elapsed    | 15648   |
|    total_timesteps | 3878912 |
--------------------------------
Eval num_timesteps=3880000, episode_reward=-273.74 +/- 227.03
Episode length: 617.60 +/- 469.59
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 618          |
|    mean_reward          | -274         |
| time/                   |              |
|    total_timesteps      | 3880000      |
| train/                  |              |
|    approx_kl            | 0.0059337774 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.54        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0105       |
|    n_updates            | 9470         |
|    policy_gradient_loss | -0.00299     |
|    std                  | 0.804        |
|    value_loss           | 0.168        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 247     |
|    iterations      | 948     |
|    time_elapsed    | 15657   |
|    total_timesteps | 3883008 |
--------------------------------
Eval num_timesteps=3885000, episode_reward=-448.39 +/- 187.66
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -448       |
| time/                   |            |
|    total_timesteps      | 3885000    |
| train/                  |            |
|    approx_kl            | 0.00355925 |
|    clip_fraction        | 0.0241     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.53      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0323     |
|    n_updates            | 9480       |
|    policy_gradient_loss | -0.00083   |
|    std                  | 0.805      |
|    value_loss           | 1.44       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 248     |
|    iterations      | 949     |
|    time_elapsed    | 15668   |
|    total_timesteps | 3887104 |
--------------------------------
Eval num_timesteps=3890000, episode_reward=-425.70 +/- 163.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -426         |
| time/                   |              |
|    total_timesteps      | 3890000      |
| train/                  |              |
|    approx_kl            | 0.0055223964 |
|    clip_fraction        | 0.0529       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.53         |
|    n_updates            | 9490         |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.805        |
|    value_loss           | 0.89         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 248     |
|    iterations      | 950     |
|    time_elapsed    | 15679   |
|    total_timesteps | 3891200 |
--------------------------------
Eval num_timesteps=3895000, episode_reward=-287.58 +/- 157.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -288         |
| time/                   |              |
|    total_timesteps      | 3895000      |
| train/                  |              |
|    approx_kl            | 0.0050668577 |
|    clip_fraction        | 0.0282       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0534       |
|    n_updates            | 9500         |
|    policy_gradient_loss | -0.00143     |
|    std                  | 0.804        |
|    value_loss           | 0.564        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 248     |
|    iterations      | 951     |
|    time_elapsed    | 15689   |
|    total_timesteps | 3895296 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 248          |
|    iterations           | 952          |
|    time_elapsed         | 15696        |
|    total_timesteps      | 3899392      |
| train/                  |              |
|    approx_kl            | 0.0058896937 |
|    clip_fraction        | 0.0293       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.53        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 33.7         |
|    n_updates            | 9510         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.801        |
|    value_loss           | 9.96         |
------------------------------------------
Eval num_timesteps=3900000, episode_reward=-373.05 +/- 99.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -373         |
| time/                   |              |
|    total_timesteps      | 3900000      |
| train/                  |              |
|    approx_kl            | 0.0049612857 |
|    clip_fraction        | 0.0521       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0152       |
|    n_updates            | 9520         |
|    policy_gradient_loss | -0.00164     |
|    std                  | 0.798        |
|    value_loss           | 0.0998       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 248     |
|    iterations      | 953     |
|    time_elapsed    | 15706   |
|    total_timesteps | 3903488 |
--------------------------------
Eval num_timesteps=3905000, episode_reward=-232.76 +/- 169.55
Episode length: 810.00 +/- 382.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | -233        |
| time/                   |             |
|    total_timesteps      | 3905000     |
| train/                  |             |
|    approx_kl            | 0.004506336 |
|    clip_fraction        | 0.0397      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0156      |
|    n_updates            | 9530        |
|    policy_gradient_loss | -0.00212    |
|    std                  | 0.8         |
|    value_loss           | 0.151       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 248     |
|    iterations      | 954     |
|    time_elapsed    | 15715   |
|    total_timesteps | 3907584 |
--------------------------------
Eval num_timesteps=3910000, episode_reward=-417.12 +/- 145.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -417        |
| time/                   |             |
|    total_timesteps      | 3910000     |
| train/                  |             |
|    approx_kl            | 0.005540092 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.463       |
|    n_updates            | 9540        |
|    policy_gradient_loss | -0.00199    |
|    std                  | 0.801       |
|    value_loss           | 2.18        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 248     |
|    iterations      | 955     |
|    time_elapsed    | 15726   |
|    total_timesteps | 3911680 |
--------------------------------
Eval num_timesteps=3915000, episode_reward=-317.11 +/- 59.51
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -317         |
| time/                   |              |
|    total_timesteps      | 3915000      |
| train/                  |              |
|    approx_kl            | 0.0048638214 |
|    clip_fraction        | 0.0417       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.52        |
|    explained_variance   | 0.985        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.173        |
|    n_updates            | 9550         |
|    policy_gradient_loss | -0.00242     |
|    std                  | 0.797        |
|    value_loss           | 10.2         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 248     |
|    iterations      | 956     |
|    time_elapsed    | 15738   |
|    total_timesteps | 3915776 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 957         |
|    time_elapsed         | 15746       |
|    total_timesteps      | 3919872     |
| train/                  |             |
|    approx_kl            | 0.007144432 |
|    clip_fraction        | 0.0591      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0596      |
|    n_updates            | 9560        |
|    policy_gradient_loss | -0.0025     |
|    std                  | 0.8         |
|    value_loss           | 2.54        |
-----------------------------------------
Eval num_timesteps=3920000, episode_reward=-244.29 +/- 73.81
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -244        |
| time/                   |             |
|    total_timesteps      | 3920000     |
| train/                  |             |
|    approx_kl            | 0.007272818 |
|    clip_fraction        | 0.0683      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.52       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0283      |
|    n_updates            | 9570        |
|    policy_gradient_loss | -0.00381    |
|    std                  | 0.796       |
|    value_loss           | 0.243       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 248     |
|    iterations      | 958     |
|    time_elapsed    | 15761   |
|    total_timesteps | 3923968 |
--------------------------------
Eval num_timesteps=3925000, episode_reward=-422.97 +/- 203.34
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -423         |
| time/                   |              |
|    total_timesteps      | 3925000      |
| train/                  |              |
|    approx_kl            | 0.0052240267 |
|    clip_fraction        | 0.0515       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.895        |
|    n_updates            | 9580         |
|    policy_gradient_loss | -0.00252     |
|    std                  | 0.796        |
|    value_loss           | 2.74         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 959     |
|    time_elapsed    | 15775   |
|    total_timesteps | 3928064 |
--------------------------------
Eval num_timesteps=3930000, episode_reward=-222.60 +/- 184.64
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -223        |
| time/                   |             |
|    total_timesteps      | 3930000     |
| train/                  |             |
|    approx_kl            | 0.004086788 |
|    clip_fraction        | 0.0271      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.197       |
|    n_updates            | 9590        |
|    policy_gradient_loss | -0.00122    |
|    std                  | 0.795       |
|    value_loss           | 15.2        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 960     |
|    time_elapsed    | 15785   |
|    total_timesteps | 3932160 |
--------------------------------
Eval num_timesteps=3935000, episode_reward=-334.74 +/- 123.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -335         |
| time/                   |              |
|    total_timesteps      | 3935000      |
| train/                  |              |
|    approx_kl            | 0.0068853907 |
|    clip_fraction        | 0.0554       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.976        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.05         |
|    n_updates            | 9600         |
|    policy_gradient_loss | -0.00131     |
|    std                  | 0.798        |
|    value_loss           | 7.93         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 961     |
|    time_elapsed    | 15797   |
|    total_timesteps | 3936256 |
--------------------------------
Eval num_timesteps=3940000, episode_reward=-393.57 +/- 61.18
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -394        |
| time/                   |             |
|    total_timesteps      | 3940000     |
| train/                  |             |
|    approx_kl            | 0.004364958 |
|    clip_fraction        | 0.0304      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.803       |
|    n_updates            | 9610        |
|    policy_gradient_loss | -0.00198    |
|    std                  | 0.794       |
|    value_loss           | 1.33        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 962     |
|    time_elapsed    | 15809   |
|    total_timesteps | 3940352 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 249          |
|    iterations           | 963          |
|    time_elapsed         | 15816        |
|    total_timesteps      | 3944448      |
| train/                  |              |
|    approx_kl            | 0.0059778225 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.51        |
|    explained_variance   | 0.971        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.24         |
|    n_updates            | 9620         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 0.799        |
|    value_loss           | 4.56         |
------------------------------------------
Eval num_timesteps=3945000, episode_reward=-255.86 +/- 163.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -256         |
| time/                   |              |
|    total_timesteps      | 3945000      |
| train/                  |              |
|    approx_kl            | 0.0035997839 |
|    clip_fraction        | 0.0214       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.5         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00811      |
|    n_updates            | 9630         |
|    policy_gradient_loss | -0.000125    |
|    std                  | 0.792        |
|    value_loss           | 0.118        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 964     |
|    time_elapsed    | 15831   |
|    total_timesteps | 3948544 |
--------------------------------
Eval num_timesteps=3950000, episode_reward=-204.56 +/- 115.68
Episode length: 813.00 +/- 376.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 813         |
|    mean_reward          | -205        |
| time/                   |             |
|    total_timesteps      | 3950000     |
| train/                  |             |
|    approx_kl            | 0.004263682 |
|    clip_fraction        | 0.0407      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.209       |
|    n_updates            | 9640        |
|    policy_gradient_loss | -0.0026     |
|    std                  | 0.791       |
|    value_loss           | 3.88        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 965     |
|    time_elapsed    | 15845   |
|    total_timesteps | 3952640 |
--------------------------------
Eval num_timesteps=3955000, episode_reward=-343.96 +/- 136.10
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -344        |
| time/                   |             |
|    total_timesteps      | 3955000     |
| train/                  |             |
|    approx_kl            | 0.004867922 |
|    clip_fraction        | 0.0509      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.49       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.147       |
|    n_updates            | 9650        |
|    policy_gradient_loss | -0.00198    |
|    std                  | 0.79        |
|    value_loss           | 13.2        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 966     |
|    time_elapsed    | 15855   |
|    total_timesteps | 3956736 |
--------------------------------
Eval num_timesteps=3960000, episode_reward=-334.85 +/- 167.97
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -335        |
| time/                   |             |
|    total_timesteps      | 3960000     |
| train/                  |             |
|    approx_kl            | 0.006955711 |
|    clip_fraction        | 0.0624      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.471       |
|    n_updates            | 9660        |
|    policy_gradient_loss | -0.00297    |
|    std                  | 0.787       |
|    value_loss           | 1.3         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 967     |
|    time_elapsed    | 15866   |
|    total_timesteps | 3960832 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 249         |
|    iterations           | 968         |
|    time_elapsed         | 15875       |
|    total_timesteps      | 3964928     |
| train/                  |             |
|    approx_kl            | 0.007990219 |
|    clip_fraction        | 0.0724      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00261     |
|    n_updates            | 9670        |
|    policy_gradient_loss | -0.00202    |
|    std                  | 0.791       |
|    value_loss           | 2.07        |
-----------------------------------------
Eval num_timesteps=3965000, episode_reward=-456.41 +/- 155.65
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -456        |
| time/                   |             |
|    total_timesteps      | 3965000     |
| train/                  |             |
|    approx_kl            | 0.008110573 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0088     |
|    n_updates            | 9680        |
|    policy_gradient_loss | -0.00493    |
|    std                  | 0.791       |
|    value_loss           | 0.0713      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 969     |
|    time_elapsed    | 15891   |
|    total_timesteps | 3969024 |
--------------------------------
Eval num_timesteps=3970000, episode_reward=-169.90 +/- 104.79
Episode length: 806.60 +/- 388.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 807         |
|    mean_reward          | -170        |
| time/                   |             |
|    total_timesteps      | 3970000     |
| train/                  |             |
|    approx_kl            | 0.003998528 |
|    clip_fraction        | 0.0287      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.48       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0182      |
|    n_updates            | 9690        |
|    policy_gradient_loss | -0.0015     |
|    std                  | 0.789       |
|    value_loss           | 1.09        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 970     |
|    time_elapsed    | 15902   |
|    total_timesteps | 3973120 |
--------------------------------
Eval num_timesteps=3975000, episode_reward=-332.26 +/- 227.29
Episode length: 806.60 +/- 388.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 807          |
|    mean_reward          | -332         |
| time/                   |              |
|    total_timesteps      | 3975000      |
| train/                  |              |
|    approx_kl            | 0.0059091086 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.48        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.05         |
|    n_updates            | 9700         |
|    policy_gradient_loss | -0.00151     |
|    std                  | 0.788        |
|    value_loss           | 3.12         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 249     |
|    iterations      | 971     |
|    time_elapsed    | 15912   |
|    total_timesteps | 3977216 |
--------------------------------
Eval num_timesteps=3980000, episode_reward=-324.48 +/- 121.34
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -324        |
| time/                   |             |
|    total_timesteps      | 3980000     |
| train/                  |             |
|    approx_kl            | 0.004626773 |
|    clip_fraction        | 0.0482      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.62        |
|    n_updates            | 9710        |
|    policy_gradient_loss | -0.00203    |
|    std                  | 0.786       |
|    value_loss           | 6.1         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 250     |
|    iterations      | 972     |
|    time_elapsed    | 15922   |
|    total_timesteps | 3981312 |
--------------------------------
Eval num_timesteps=3985000, episode_reward=-333.88 +/- 135.79
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -334        |
| time/                   |             |
|    total_timesteps      | 3985000     |
| train/                  |             |
|    approx_kl            | 0.007638489 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0001      |
|    loss                 | 9.74        |
|    n_updates            | 9720        |
|    policy_gradient_loss | -0.00297    |
|    std                  | 0.786       |
|    value_loss           | 11.5        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 250     |
|    iterations      | 973     |
|    time_elapsed    | 15932   |
|    total_timesteps | 3985408 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 974         |
|    time_elapsed         | 15939       |
|    total_timesteps      | 3989504     |
| train/                  |             |
|    approx_kl            | 0.007329755 |
|    clip_fraction        | 0.0682      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.46       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00321     |
|    n_updates            | 9730        |
|    policy_gradient_loss | -0.00347    |
|    std                  | 0.782       |
|    value_loss           | 0.435       |
-----------------------------------------
Eval num_timesteps=3990000, episode_reward=-285.33 +/- 156.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -285         |
| time/                   |              |
|    total_timesteps      | 3990000      |
| train/                  |              |
|    approx_kl            | 0.0044617373 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.45        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00243      |
|    n_updates            | 9740         |
|    policy_gradient_loss | -0.00157     |
|    std                  | 0.781        |
|    value_loss           | 0.0707       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 250     |
|    iterations      | 975     |
|    time_elapsed    | 15948   |
|    total_timesteps | 3993600 |
--------------------------------
Eval num_timesteps=3995000, episode_reward=-206.86 +/- 54.56
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -207         |
| time/                   |              |
|    total_timesteps      | 3995000      |
| train/                  |              |
|    approx_kl            | 0.0048561012 |
|    clip_fraction        | 0.0335       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.45        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0979       |
|    n_updates            | 9750         |
|    policy_gradient_loss | -0.0026      |
|    std                  | 0.779        |
|    value_loss           | 1.8          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 250     |
|    iterations      | 976     |
|    time_elapsed    | 15959   |
|    total_timesteps | 3997696 |
--------------------------------
Eval num_timesteps=4000000, episode_reward=-462.32 +/- 146.17
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -462        |
| time/                   |             |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.003624253 |
|    clip_fraction        | 0.0248      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0275      |
|    n_updates            | 9760        |
|    policy_gradient_loss | 0.000213    |
|    std                  | 0.78        |
|    value_loss           | 0.151       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 250     |
|    iterations      | 977     |
|    time_elapsed    | 15971   |
|    total_timesteps | 4001792 |
--------------------------------
Eval num_timesteps=4005000, episode_reward=-262.38 +/- 179.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -262        |
| time/                   |             |
|    total_timesteps      | 4005000     |
| train/                  |             |
|    approx_kl            | 0.005001495 |
|    clip_fraction        | 0.0472      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0499      |
|    n_updates            | 9770        |
|    policy_gradient_loss | -0.00289    |
|    std                  | 0.777       |
|    value_loss           | 1.28        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 250     |
|    iterations      | 978     |
|    time_elapsed    | 15985   |
|    total_timesteps | 4005888 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 250          |
|    iterations           | 979          |
|    time_elapsed         | 15992        |
|    total_timesteps      | 4009984      |
| train/                  |              |
|    approx_kl            | 0.0061436803 |
|    clip_fraction        | 0.0441       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.44        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.162        |
|    n_updates            | 9780         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 0.779        |
|    value_loss           | 3.52         |
------------------------------------------
Eval num_timesteps=4010000, episode_reward=-310.10 +/- 142.54
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -310        |
| time/                   |             |
|    total_timesteps      | 4010000     |
| train/                  |             |
|    approx_kl            | 0.006688455 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.45       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00498    |
|    n_updates            | 9790        |
|    policy_gradient_loss | -0.00257    |
|    std                  | 0.781       |
|    value_loss           | 0.0688      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 250     |
|    iterations      | 980     |
|    time_elapsed    | 16002   |
|    total_timesteps | 4014080 |
--------------------------------
Eval num_timesteps=4015000, episode_reward=-260.47 +/- 151.63
Episode length: 809.80 +/- 382.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 810          |
|    mean_reward          | -260         |
| time/                   |              |
|    total_timesteps      | 4015000      |
| train/                  |              |
|    approx_kl            | 0.0063812425 |
|    clip_fraction        | 0.0548       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.46        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0176       |
|    n_updates            | 9800         |
|    policy_gradient_loss | -0.00346     |
|    std                  | 0.782        |
|    value_loss           | 1.78         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 250     |
|    iterations      | 981     |
|    time_elapsed    | 16012   |
|    total_timesteps | 4018176 |
--------------------------------
Eval num_timesteps=4020000, episode_reward=-302.26 +/- 218.77
Episode length: 805.80 +/- 390.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 806          |
|    mean_reward          | -302         |
| time/                   |              |
|    total_timesteps      | 4020000      |
| train/                  |              |
|    approx_kl            | 0.0026929057 |
|    clip_fraction        | 0.0252       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.46        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.068        |
|    n_updates            | 9810         |
|    policy_gradient_loss | -0.00072     |
|    std                  | 0.781        |
|    value_loss           | 6.68         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 251     |
|    iterations      | 982     |
|    time_elapsed    | 16022   |
|    total_timesteps | 4022272 |
--------------------------------
Eval num_timesteps=4025000, episode_reward=-263.38 +/- 74.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -263         |
| time/                   |              |
|    total_timesteps      | 4025000      |
| train/                  |              |
|    approx_kl            | 0.0047434387 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.46        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.11         |
|    n_updates            | 9820         |
|    policy_gradient_loss | -0.00219     |
|    std                  | 0.783        |
|    value_loss           | 13.1         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 251     |
|    iterations      | 983     |
|    time_elapsed    | 16032   |
|    total_timesteps | 4026368 |
--------------------------------
Eval num_timesteps=4030000, episode_reward=-318.69 +/- 90.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 4030000      |
| train/                  |              |
|    approx_kl            | 0.0041571623 |
|    clip_fraction        | 0.0424       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.46        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00627      |
|    n_updates            | 9830         |
|    policy_gradient_loss | -0.000242    |
|    std                  | 0.781        |
|    value_loss           | 0.0552       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 251     |
|    iterations      | 984     |
|    time_elapsed    | 16042   |
|    total_timesteps | 4030464 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 251          |
|    iterations           | 985          |
|    time_elapsed         | 16049        |
|    total_timesteps      | 4034560      |
| train/                  |              |
|    approx_kl            | 0.0044681965 |
|    clip_fraction        | 0.0276       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.45        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.13         |
|    n_updates            | 9840         |
|    policy_gradient_loss | -0.000362    |
|    std                  | 0.779        |
|    value_loss           | 5.3          |
------------------------------------------
Eval num_timesteps=4035000, episode_reward=-370.02 +/- 135.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -370        |
| time/                   |             |
|    total_timesteps      | 4035000     |
| train/                  |             |
|    approx_kl            | 0.004601731 |
|    clip_fraction        | 0.0475      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.43       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0242      |
|    n_updates            | 9850        |
|    policy_gradient_loss | -0.00134    |
|    std                  | 0.771       |
|    value_loss           | 0.0712      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 251     |
|    iterations      | 986     |
|    time_elapsed    | 16060   |
|    total_timesteps | 4038656 |
--------------------------------
Eval num_timesteps=4040000, episode_reward=-169.72 +/- 89.66
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -170        |
| time/                   |             |
|    total_timesteps      | 4040000     |
| train/                  |             |
|    approx_kl            | 0.004815319 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0197      |
|    n_updates            | 9860        |
|    policy_gradient_loss | -0.00231    |
|    std                  | 0.774       |
|    value_loss           | 0.246       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 251     |
|    iterations      | 987     |
|    time_elapsed    | 16070   |
|    total_timesteps | 4042752 |
--------------------------------
Eval num_timesteps=4045000, episode_reward=-259.39 +/- 98.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -259         |
| time/                   |              |
|    total_timesteps      | 4045000      |
| train/                  |              |
|    approx_kl            | 0.0036512848 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.43        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.491        |
|    n_updates            | 9870         |
|    policy_gradient_loss | -0.00263     |
|    std                  | 0.775        |
|    value_loss           | 2.97         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 251     |
|    iterations      | 988     |
|    time_elapsed    | 16080   |
|    total_timesteps | 4046848 |
--------------------------------
Eval num_timesteps=4050000, episode_reward=-255.99 +/- 151.10
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -256         |
| time/                   |              |
|    total_timesteps      | 4050000      |
| train/                  |              |
|    approx_kl            | 0.0061899116 |
|    clip_fraction        | 0.0362       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.42        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 9.62         |
|    n_updates            | 9880         |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.774        |
|    value_loss           | 8.69         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 251     |
|    iterations      | 989     |
|    time_elapsed    | 16091   |
|    total_timesteps | 4050944 |
--------------------------------
Eval num_timesteps=4055000, episode_reward=-435.47 +/- 148.32
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -435         |
| time/                   |              |
|    total_timesteps      | 4055000      |
| train/                  |              |
|    approx_kl            | 0.0043511963 |
|    clip_fraction        | 0.0472       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.42        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0367       |
|    n_updates            | 9890         |
|    policy_gradient_loss | -0.00198     |
|    std                  | 0.776        |
|    value_loss           | 4.14         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 251     |
|    iterations      | 990     |
|    time_elapsed    | 16102   |
|    total_timesteps | 4055040 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 251          |
|    iterations           | 991          |
|    time_elapsed         | 16109        |
|    total_timesteps      | 4059136      |
| train/                  |              |
|    approx_kl            | 0.0061569717 |
|    clip_fraction        | 0.0523       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.43        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.678        |
|    n_updates            | 9900         |
|    policy_gradient_loss | -0.00424     |
|    std                  | 0.781        |
|    value_loss           | 3.15         |
------------------------------------------
Eval num_timesteps=4060000, episode_reward=-285.68 +/- 134.59
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -286        |
| time/                   |             |
|    total_timesteps      | 4060000     |
| train/                  |             |
|    approx_kl            | 0.006787177 |
|    clip_fraction        | 0.0496      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0172      |
|    n_updates            | 9910        |
|    policy_gradient_loss | -0.00165    |
|    std                  | 0.771       |
|    value_loss           | 0.0412      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 252     |
|    iterations      | 992     |
|    time_elapsed    | 16123   |
|    total_timesteps | 4063232 |
--------------------------------
Eval num_timesteps=4065000, episode_reward=-207.45 +/- 111.99
Episode length: 817.00 +/- 368.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 817         |
|    mean_reward          | -207        |
| time/                   |             |
|    total_timesteps      | 4065000     |
| train/                  |             |
|    approx_kl            | 0.006448216 |
|    clip_fraction        | 0.0312      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0001      |
|    loss                 | 5.16        |
|    n_updates            | 9920        |
|    policy_gradient_loss | -0.000749   |
|    std                  | 0.771       |
|    value_loss           | 23.5        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 252     |
|    iterations      | 993     |
|    time_elapsed    | 16136   |
|    total_timesteps | 4067328 |
--------------------------------
Eval num_timesteps=4070000, episode_reward=-440.98 +/- 222.55
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -441         |
| time/                   |              |
|    total_timesteps      | 4070000      |
| train/                  |              |
|    approx_kl            | 0.0056037083 |
|    clip_fraction        | 0.0531       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.42        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0217       |
|    n_updates            | 9930         |
|    policy_gradient_loss | -0.00247     |
|    std                  | 0.775        |
|    value_loss           | 1.27         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 252     |
|    iterations      | 994     |
|    time_elapsed    | 16147   |
|    total_timesteps | 4071424 |
--------------------------------
Eval num_timesteps=4075000, episode_reward=-304.18 +/- 205.97
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -304         |
| time/                   |              |
|    total_timesteps      | 4075000      |
| train/                  |              |
|    approx_kl            | 0.0052202214 |
|    clip_fraction        | 0.0213       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.43        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0243       |
|    n_updates            | 9940         |
|    policy_gradient_loss | -0.000323    |
|    std                  | 0.775        |
|    value_loss           | 4.33         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 252     |
|    iterations      | 995     |
|    time_elapsed    | 16156   |
|    total_timesteps | 4075520 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 252          |
|    iterations           | 996          |
|    time_elapsed         | 16163        |
|    total_timesteps      | 4079616      |
| train/                  |              |
|    approx_kl            | 0.0052744327 |
|    clip_fraction        | 0.0421       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.42        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0829       |
|    n_updates            | 9950         |
|    policy_gradient_loss | -0.000586    |
|    std                  | 0.77         |
|    value_loss           | 0.0442       |
------------------------------------------
Eval num_timesteps=4080000, episode_reward=-268.90 +/- 127.49
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -269      |
| time/                   |           |
|    total_timesteps      | 4080000   |
| train/                  |           |
|    approx_kl            | 0.0051299 |
|    clip_fraction        | 0.0446    |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.41     |
|    explained_variance   | 0.999     |
|    learning_rate        | 0.0001    |
|    loss                 | 0.0189    |
|    n_updates            | 9960      |
|    policy_gradient_loss | -0.000454 |
|    std                  | 0.77      |
|    value_loss           | 0.0447    |
---------------------------------------
--------------------------------
| time/              |         |
|    fps             | 252     |
|    iterations      | 997     |
|    time_elapsed    | 16173   |
|    total_timesteps | 4083712 |
--------------------------------
Eval num_timesteps=4085000, episode_reward=-240.20 +/- 165.90
Episode length: 810.60 +/- 380.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -240        |
| time/                   |             |
|    total_timesteps      | 4085000     |
| train/                  |             |
|    approx_kl            | 0.007941943 |
|    clip_fraction        | 0.0553      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0733      |
|    n_updates            | 9970        |
|    policy_gradient_loss | -0.0034     |
|    std                  | 0.768       |
|    value_loss           | 0.91        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 252     |
|    iterations      | 998     |
|    time_elapsed    | 16183   |
|    total_timesteps | 4087808 |
--------------------------------
Eval num_timesteps=4090000, episode_reward=-386.98 +/- 126.01
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -387        |
| time/                   |             |
|    total_timesteps      | 4090000     |
| train/                  |             |
|    approx_kl            | 0.004962169 |
|    clip_fraction        | 0.0315      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.41       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.107       |
|    n_updates            | 9980        |
|    policy_gradient_loss | -0.00207    |
|    std                  | 0.769       |
|    value_loss           | 6.23        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 252     |
|    iterations      | 999     |
|    time_elapsed    | 16194   |
|    total_timesteps | 4091904 |
--------------------------------
Eval num_timesteps=4095000, episode_reward=-247.01 +/- 189.58
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -247        |
| time/                   |             |
|    total_timesteps      | 4095000     |
| train/                  |             |
|    approx_kl            | 0.005876391 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00408    |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.00176    |
|    std                  | 0.773       |
|    value_loss           | 0.0533      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 252     |
|    iterations      | 1000    |
|    time_elapsed    | 16204   |
|    total_timesteps | 4096000 |
--------------------------------
Eval num_timesteps=4100000, episode_reward=-287.45 +/- 126.40
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -287         |
| time/                   |              |
|    total_timesteps      | 4100000      |
| train/                  |              |
|    approx_kl            | 0.0053227423 |
|    clip_fraction        | 0.042        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.43        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0988       |
|    n_updates            | 10000        |
|    policy_gradient_loss | -0.00188     |
|    std                  | 0.774        |
|    value_loss           | 8.84         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 252     |
|    iterations      | 1001    |
|    time_elapsed    | 16214   |
|    total_timesteps | 4100096 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 1002        |
|    time_elapsed         | 16222       |
|    total_timesteps      | 4104192     |
| train/                  |             |
|    approx_kl            | 0.003540683 |
|    clip_fraction        | 0.0165      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.43       |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.027       |
|    n_updates            | 10010       |
|    policy_gradient_loss | -0.000182   |
|    std                  | 0.775       |
|    value_loss           | 6.56        |
-----------------------------------------
Eval num_timesteps=4105000, episode_reward=-292.65 +/- 60.86
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -293         |
| time/                   |              |
|    total_timesteps      | 4105000      |
| train/                  |              |
|    approx_kl            | 0.0047909115 |
|    clip_fraction        | 0.0385       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.43        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0111       |
|    n_updates            | 10020        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 0.769        |
|    value_loss           | 0.129        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 253     |
|    iterations      | 1003    |
|    time_elapsed    | 16233   |
|    total_timesteps | 4108288 |
--------------------------------
Eval num_timesteps=4110000, episode_reward=-416.27 +/- 119.27
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -416       |
| time/                   |            |
|    total_timesteps      | 4110000    |
| train/                  |            |
|    approx_kl            | 0.00554163 |
|    clip_fraction        | 0.0212     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.41      |
|    explained_variance   | 0.982      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.149      |
|    n_updates            | 10030      |
|    policy_gradient_loss | -0.001     |
|    std                  | 0.768      |
|    value_loss           | 27.7       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 253     |
|    iterations      | 1004    |
|    time_elapsed    | 16246   |
|    total_timesteps | 4112384 |
--------------------------------
Eval num_timesteps=4115000, episode_reward=-323.80 +/- 143.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -324         |
| time/                   |              |
|    total_timesteps      | 4115000      |
| train/                  |              |
|    approx_kl            | 0.0053431345 |
|    clip_fraction        | 0.0545       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.41        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.114        |
|    n_updates            | 10040        |
|    policy_gradient_loss | -0.00328     |
|    std                  | 0.765        |
|    value_loss           | 0.285        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 253     |
|    iterations      | 1005    |
|    time_elapsed    | 16261   |
|    total_timesteps | 4116480 |
--------------------------------
Eval num_timesteps=4120000, episode_reward=-289.80 +/- 135.84
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -290        |
| time/                   |             |
|    total_timesteps      | 4120000     |
| train/                  |             |
|    approx_kl            | 0.004348093 |
|    clip_fraction        | 0.0415      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0273      |
|    n_updates            | 10050       |
|    policy_gradient_loss | -0.00225    |
|    std                  | 0.766       |
|    value_loss           | 9.71        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 253     |
|    iterations      | 1006    |
|    time_elapsed    | 16272   |
|    total_timesteps | 4120576 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 1007        |
|    time_elapsed         | 16278       |
|    total_timesteps      | 4124672     |
| train/                  |             |
|    approx_kl            | 0.005095408 |
|    clip_fraction        | 0.0458      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0452      |
|    n_updates            | 10060       |
|    policy_gradient_loss | -0.0018     |
|    std                  | 0.763       |
|    value_loss           | 1.75        |
-----------------------------------------
Eval num_timesteps=4125000, episode_reward=-256.48 +/- 170.97
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -256        |
| time/                   |             |
|    total_timesteps      | 4125000     |
| train/                  |             |
|    approx_kl            | 0.009382734 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00454    |
|    n_updates            | 10070       |
|    policy_gradient_loss | -0.00533    |
|    std                  | 0.766       |
|    value_loss           | 0.0506      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 253     |
|    iterations      | 1008    |
|    time_elapsed    | 16289   |
|    total_timesteps | 4128768 |
--------------------------------
Eval num_timesteps=4130000, episode_reward=-331.13 +/- 125.39
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -331        |
| time/                   |             |
|    total_timesteps      | 4130000     |
| train/                  |             |
|    approx_kl            | 0.004041204 |
|    clip_fraction        | 0.0316      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.96        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.12        |
|    n_updates            | 10080       |
|    policy_gradient_loss | -0.00103    |
|    std                  | 0.766       |
|    value_loss           | 10.3        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 253     |
|    iterations      | 1009    |
|    time_elapsed    | 16300   |
|    total_timesteps | 4132864 |
--------------------------------
Eval num_timesteps=4135000, episode_reward=-337.72 +/- 164.52
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -338        |
| time/                   |             |
|    total_timesteps      | 4135000     |
| train/                  |             |
|    approx_kl            | 0.005462882 |
|    clip_fraction        | 0.0325      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0858      |
|    n_updates            | 10090       |
|    policy_gradient_loss | -0.000497   |
|    std                  | 0.764       |
|    value_loss           | 5.47        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 253     |
|    iterations      | 1010    |
|    time_elapsed    | 16310   |
|    total_timesteps | 4136960 |
--------------------------------
Eval num_timesteps=4140000, episode_reward=-354.41 +/- 208.86
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -354        |
| time/                   |             |
|    total_timesteps      | 4140000     |
| train/                  |             |
|    approx_kl            | 0.004822515 |
|    clip_fraction        | 0.0258      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.205       |
|    n_updates            | 10100       |
|    policy_gradient_loss | -0.000998   |
|    std                  | 0.761       |
|    value_loss           | 2.6         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 253     |
|    iterations      | 1011    |
|    time_elapsed    | 16320   |
|    total_timesteps | 4141056 |
--------------------------------
Eval num_timesteps=4145000, episode_reward=-242.82 +/- 103.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -243        |
| time/                   |             |
|    total_timesteps      | 4145000     |
| train/                  |             |
|    approx_kl            | 0.004915043 |
|    clip_fraction        | 0.0381      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.39       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.000334   |
|    n_updates            | 10110       |
|    policy_gradient_loss | -0.00259    |
|    std                  | 0.764       |
|    value_loss           | 0.459       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 253     |
|    iterations      | 1012    |
|    time_elapsed    | 16331   |
|    total_timesteps | 4145152 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 253          |
|    iterations           | 1013         |
|    time_elapsed         | 16339        |
|    total_timesteps      | 4149248      |
| train/                  |              |
|    approx_kl            | 0.0042311563 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.39        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0249       |
|    n_updates            | 10120        |
|    policy_gradient_loss | -0.000717    |
|    std                  | 0.765        |
|    value_loss           | 8.24         |
------------------------------------------
Eval num_timesteps=4150000, episode_reward=-449.77 +/- 120.36
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -450      |
| time/                   |           |
|    total_timesteps      | 4150000   |
| train/                  |           |
|    approx_kl            | 0.0055358 |
|    clip_fraction        | 0.0521    |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.38     |
|    explained_variance   | 1         |
|    learning_rate        | 0.0001    |
|    loss                 | 0.032     |
|    n_updates            | 10130     |
|    policy_gradient_loss | -0.00147  |
|    std                  | 0.76      |
|    value_loss           | 0.0189    |
---------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1014    |
|    time_elapsed    | 16351   |
|    total_timesteps | 4153344 |
--------------------------------
Eval num_timesteps=4155000, episode_reward=-300.96 +/- 191.51
Episode length: 814.20 +/- 373.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 814         |
|    mean_reward          | -301        |
| time/                   |             |
|    total_timesteps      | 4155000     |
| train/                  |             |
|    approx_kl            | 0.005991147 |
|    clip_fraction        | 0.0392      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.38       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.028       |
|    n_updates            | 10140       |
|    policy_gradient_loss | -0.00289    |
|    std                  | 0.759       |
|    value_loss           | 0.406       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1015    |
|    time_elapsed    | 16363   |
|    total_timesteps | 4157440 |
--------------------------------
Eval num_timesteps=4160000, episode_reward=-258.05 +/- 190.42
Episode length: 814.00 +/- 374.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -258         |
| time/                   |              |
|    total_timesteps      | 4160000      |
| train/                  |              |
|    approx_kl            | 0.0035496037 |
|    clip_fraction        | 0.0296       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.37        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.373        |
|    n_updates            | 10150        |
|    policy_gradient_loss | -0.000284    |
|    std                  | 0.76         |
|    value_loss           | 3.37         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1016    |
|    time_elapsed    | 16375   |
|    total_timesteps | 4161536 |
--------------------------------
Eval num_timesteps=4165000, episode_reward=-307.21 +/- 221.61
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -307       |
| time/                   |            |
|    total_timesteps      | 4165000    |
| train/                  |            |
|    approx_kl            | 0.00446334 |
|    clip_fraction        | 0.0365     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.38      |
|    explained_variance   | 0.996      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0438     |
|    n_updates            | 10160      |
|    policy_gradient_loss | -0.00166   |
|    std                  | 0.761      |
|    value_loss           | 0.311      |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1017    |
|    time_elapsed    | 16390   |
|    total_timesteps | 4165632 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 254          |
|    iterations           | 1018         |
|    time_elapsed         | 16399        |
|    total_timesteps      | 4169728      |
| train/                  |              |
|    approx_kl            | 0.0073831854 |
|    clip_fraction        | 0.0798       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.38        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0241       |
|    n_updates            | 10170        |
|    policy_gradient_loss | -0.00174     |
|    std                  | 0.763        |
|    value_loss           | 4.92         |
------------------------------------------
Eval num_timesteps=4170000, episode_reward=-165.79 +/- 84.45
Episode length: 811.20 +/- 379.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 811          |
|    mean_reward          | -166         |
| time/                   |              |
|    total_timesteps      | 4170000      |
| train/                  |              |
|    approx_kl            | 0.0041922294 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.39        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0365       |
|    n_updates            | 10180        |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.766        |
|    value_loss           | 0.0368       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1019    |
|    time_elapsed    | 16408   |
|    total_timesteps | 4173824 |
--------------------------------
Eval num_timesteps=4175000, episode_reward=-380.81 +/- 139.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -381         |
| time/                   |              |
|    total_timesteps      | 4175000      |
| train/                  |              |
|    approx_kl            | 0.0028430296 |
|    clip_fraction        | 0.00808      |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.39        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 18.5         |
|    n_updates            | 10190        |
|    policy_gradient_loss | 0.000378     |
|    std                  | 0.764        |
|    value_loss           | 10.4         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1020    |
|    time_elapsed    | 16419   |
|    total_timesteps | 4177920 |
--------------------------------
Eval num_timesteps=4180000, episode_reward=-265.26 +/- 128.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -265         |
| time/                   |              |
|    total_timesteps      | 4180000      |
| train/                  |              |
|    approx_kl            | 0.0058628973 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.39        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.113        |
|    n_updates            | 10200        |
|    policy_gradient_loss | -0.00106     |
|    std                  | 0.764        |
|    value_loss           | 3.15         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1021    |
|    time_elapsed    | 16430   |
|    total_timesteps | 4182016 |
--------------------------------
Eval num_timesteps=4185000, episode_reward=-233.23 +/- 93.11
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -233         |
| time/                   |              |
|    total_timesteps      | 4185000      |
| train/                  |              |
|    approx_kl            | 0.0039613517 |
|    clip_fraction        | 0.0284       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.38        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.811        |
|    n_updates            | 10210        |
|    policy_gradient_loss | -0.000453    |
|    std                  | 0.761        |
|    value_loss           | 2.98         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1022    |
|    time_elapsed    | 16443   |
|    total_timesteps | 4186112 |
--------------------------------
Eval num_timesteps=4190000, episode_reward=-287.99 +/- 159.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -288         |
| time/                   |              |
|    total_timesteps      | 4190000      |
| train/                  |              |
|    approx_kl            | 0.0045927595 |
|    clip_fraction        | 0.0452       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.37        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.631        |
|    n_updates            | 10220        |
|    policy_gradient_loss | -0.00155     |
|    std                  | 0.759        |
|    value_loss           | 1.41         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1023    |
|    time_elapsed    | 16458   |
|    total_timesteps | 4190208 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 254          |
|    iterations           | 1024         |
|    time_elapsed         | 16465        |
|    total_timesteps      | 4194304      |
| train/                  |              |
|    approx_kl            | 0.0044063693 |
|    clip_fraction        | 0.0298       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.37        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.88         |
|    n_updates            | 10230        |
|    policy_gradient_loss | -0.000939    |
|    std                  | 0.758        |
|    value_loss           | 4.54         |
------------------------------------------
Eval num_timesteps=4195000, episode_reward=-467.60 +/- 161.84
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -468         |
| time/                   |              |
|    total_timesteps      | 4195000      |
| train/                  |              |
|    approx_kl            | 0.0045890333 |
|    clip_fraction        | 0.0524       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.02         |
|    n_updates            | 10240        |
|    policy_gradient_loss | -0.00222     |
|    std                  | 0.758        |
|    value_loss           | 0.0185       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1025    |
|    time_elapsed    | 16476   |
|    total_timesteps | 4198400 |
--------------------------------
Eval num_timesteps=4200000, episode_reward=-293.75 +/- 147.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -294         |
| time/                   |              |
|    total_timesteps      | 4200000      |
| train/                  |              |
|    approx_kl            | 0.0060502356 |
|    clip_fraction        | 0.0504       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0196       |
|    n_updates            | 10250        |
|    policy_gradient_loss | -0.0022      |
|    std                  | 0.757        |
|    value_loss           | 0.21         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1026    |
|    time_elapsed    | 16488   |
|    total_timesteps | 4202496 |
--------------------------------
Eval num_timesteps=4205000, episode_reward=-444.02 +/- 188.81
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -444         |
| time/                   |              |
|    total_timesteps      | 4205000      |
| train/                  |              |
|    approx_kl            | 0.0056718416 |
|    clip_fraction        | 0.0409       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.35        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0596       |
|    n_updates            | 10260        |
|    policy_gradient_loss | -0.00279     |
|    std                  | 0.754        |
|    value_loss           | 0.768        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1027    |
|    time_elapsed    | 16500   |
|    total_timesteps | 4206592 |
--------------------------------
Eval num_timesteps=4210000, episode_reward=-137.92 +/- 85.75
Episode length: 810.80 +/- 380.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -138        |
| time/                   |             |
|    total_timesteps      | 4210000     |
| train/                  |             |
|    approx_kl            | 0.004084314 |
|    clip_fraction        | 0.0354      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.18        |
|    n_updates            | 10270       |
|    policy_gradient_loss | -0.00161    |
|    std                  | 0.755       |
|    value_loss           | 1.14        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 254     |
|    iterations      | 1028    |
|    time_elapsed    | 16513   |
|    total_timesteps | 4210688 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 255          |
|    iterations           | 1029         |
|    time_elapsed         | 16523        |
|    total_timesteps      | 4214784      |
| train/                  |              |
|    approx_kl            | 0.0074114194 |
|    clip_fraction        | 0.0631       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.524        |
|    n_updates            | 10280        |
|    policy_gradient_loss | -0.00431     |
|    std                  | 0.758        |
|    value_loss           | 1.6          |
------------------------------------------
Eval num_timesteps=4215000, episode_reward=-304.78 +/- 29.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -305         |
| time/                   |              |
|    total_timesteps      | 4215000      |
| train/                  |              |
|    approx_kl            | 0.0042648995 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0117       |
|    n_updates            | 10290        |
|    policy_gradient_loss | -0.00144     |
|    std                  | 0.756        |
|    value_loss           | 0.0381       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 255     |
|    iterations      | 1030    |
|    time_elapsed    | 16534   |
|    total_timesteps | 4218880 |
--------------------------------
Eval num_timesteps=4220000, episode_reward=-222.44 +/- 98.83
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -222        |
| time/                   |             |
|    total_timesteps      | 4220000     |
| train/                  |             |
|    approx_kl            | 0.007214792 |
|    clip_fraction        | 0.0545      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.267       |
|    n_updates            | 10300       |
|    policy_gradient_loss | -0.00185    |
|    std                  | 0.757       |
|    value_loss           | 2.85        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 255     |
|    iterations      | 1031    |
|    time_elapsed    | 16544   |
|    total_timesteps | 4222976 |
--------------------------------
Eval num_timesteps=4225000, episode_reward=-260.85 +/- 88.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -261         |
| time/                   |              |
|    total_timesteps      | 4225000      |
| train/                  |              |
|    approx_kl            | 0.0050506475 |
|    clip_fraction        | 0.033        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.28         |
|    n_updates            | 10310        |
|    policy_gradient_loss | -0.00132     |
|    std                  | 0.756        |
|    value_loss           | 2.03         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 255     |
|    iterations      | 1032    |
|    time_elapsed    | 16554   |
|    total_timesteps | 4227072 |
--------------------------------
Eval num_timesteps=4230000, episode_reward=-307.76 +/- 180.75
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -308        |
| time/                   |             |
|    total_timesteps      | 4230000     |
| train/                  |             |
|    approx_kl            | 0.006005465 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0053     |
|    n_updates            | 10320       |
|    policy_gradient_loss | -0.00173    |
|    std                  | 0.751       |
|    value_loss           | 0.0885      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 255     |
|    iterations      | 1033    |
|    time_elapsed    | 16565   |
|    total_timesteps | 4231168 |
--------------------------------
Eval num_timesteps=4235000, episode_reward=-249.85 +/- 170.00
Episode length: 818.60 +/- 364.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 819          |
|    mean_reward          | -250         |
| time/                   |              |
|    total_timesteps      | 4235000      |
| train/                  |              |
|    approx_kl            | 0.0030680122 |
|    clip_fraction        | 0.0227       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.34        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0127       |
|    n_updates            | 10330        |
|    policy_gradient_loss | -8.54e-05    |
|    std                  | 0.75         |
|    value_loss           | 0.707        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 255     |
|    iterations      | 1034    |
|    time_elapsed    | 16575   |
|    total_timesteps | 4235264 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 255          |
|    iterations           | 1035         |
|    time_elapsed         | 16582        |
|    total_timesteps      | 4239360      |
| train/                  |              |
|    approx_kl            | 0.0049296804 |
|    clip_fraction        | 0.0476       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.34        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.37         |
|    n_updates            | 10340        |
|    policy_gradient_loss | -0.00404     |
|    std                  | 0.752        |
|    value_loss           | 0.861        |
------------------------------------------
Eval num_timesteps=4240000, episode_reward=-262.10 +/- 122.28
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -262         |
| time/                   |              |
|    total_timesteps      | 4240000      |
| train/                  |              |
|    approx_kl            | 0.0045378306 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0702       |
|    n_updates            | 10350        |
|    policy_gradient_loss | -0.000889    |
|    std                  | 0.755        |
|    value_loss           | 0.163        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 255     |
|    iterations      | 1036    |
|    time_elapsed    | 16592   |
|    total_timesteps | 4243456 |
--------------------------------
Eval num_timesteps=4245000, episode_reward=-270.99 +/- 165.28
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -271        |
| time/                   |             |
|    total_timesteps      | 4245000     |
| train/                  |             |
|    approx_kl            | 0.004640798 |
|    clip_fraction        | 0.033       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0698      |
|    n_updates            | 10360       |
|    policy_gradient_loss | -0.00225    |
|    std                  | 0.756       |
|    value_loss           | 6.78        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 255     |
|    iterations      | 1037    |
|    time_elapsed    | 16603   |
|    total_timesteps | 4247552 |
--------------------------------
Eval num_timesteps=4250000, episode_reward=-460.71 +/- 147.36
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -461         |
| time/                   |              |
|    total_timesteps      | 4250000      |
| train/                  |              |
|    approx_kl            | 0.0041699004 |
|    clip_fraction        | 0.0323       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.158        |
|    n_updates            | 10370        |
|    policy_gradient_loss | -0.00314     |
|    std                  | 0.753        |
|    value_loss           | 4.61         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 255     |
|    iterations      | 1038    |
|    time_elapsed    | 16618   |
|    total_timesteps | 4251648 |
--------------------------------
Eval num_timesteps=4255000, episode_reward=-333.79 +/- 166.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -334        |
| time/                   |             |
|    total_timesteps      | 4255000     |
| train/                  |             |
|    approx_kl            | 0.005620175 |
|    clip_fraction        | 0.0692      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00439     |
|    n_updates            | 10380       |
|    policy_gradient_loss | -0.00166    |
|    std                  | 0.753       |
|    value_loss           | 4.11        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 255     |
|    iterations      | 1039    |
|    time_elapsed    | 16630   |
|    total_timesteps | 4255744 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 256         |
|    iterations           | 1040        |
|    time_elapsed         | 16636       |
|    total_timesteps      | 4259840     |
| train/                  |             |
|    approx_kl            | 0.007333304 |
|    clip_fraction        | 0.0861      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.115       |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.00435    |
|    std                  | 0.753       |
|    value_loss           | 5.23        |
-----------------------------------------
Eval num_timesteps=4260000, episode_reward=-311.44 +/- 50.39
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -311        |
| time/                   |             |
|    total_timesteps      | 4260000     |
| train/                  |             |
|    approx_kl            | 0.006556309 |
|    clip_fraction        | 0.0602      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00141    |
|    n_updates            | 10400       |
|    policy_gradient_loss | -0.00361    |
|    std                  | 0.744       |
|    value_loss           | 0.0287      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 256     |
|    iterations      | 1041    |
|    time_elapsed    | 16647   |
|    total_timesteps | 4263936 |
--------------------------------
Eval num_timesteps=4265000, episode_reward=-189.04 +/- 173.91
Episode length: 813.80 +/- 374.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 814          |
|    mean_reward          | -189         |
| time/                   |              |
|    total_timesteps      | 4265000      |
| train/                  |              |
|    approx_kl            | 0.0062740315 |
|    clip_fraction        | 0.0331       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.32        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.23         |
|    n_updates            | 10410        |
|    policy_gradient_loss | -0.00177     |
|    std                  | 0.744        |
|    value_loss           | 4.8          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 256     |
|    iterations      | 1042    |
|    time_elapsed    | 16657   |
|    total_timesteps | 4268032 |
--------------------------------
Eval num_timesteps=4270000, episode_reward=-280.36 +/- 139.93
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -280        |
| time/                   |             |
|    total_timesteps      | 4270000     |
| train/                  |             |
|    approx_kl            | 0.008311344 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0997      |
|    n_updates            | 10420       |
|    policy_gradient_loss | -0.00376    |
|    std                  | 0.746       |
|    value_loss           | 12.6        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 256     |
|    iterations      | 1043    |
|    time_elapsed    | 16667   |
|    total_timesteps | 4272128 |
--------------------------------
Eval num_timesteps=4275000, episode_reward=-177.17 +/- 82.39
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -177         |
| time/                   |              |
|    total_timesteps      | 4275000      |
| train/                  |              |
|    approx_kl            | 0.0051568616 |
|    clip_fraction        | 0.0399       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.34        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.53         |
|    n_updates            | 10430        |
|    policy_gradient_loss | -0.00284     |
|    std                  | 0.747        |
|    value_loss           | 0.795        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 256     |
|    iterations      | 1044    |
|    time_elapsed    | 16678   |
|    total_timesteps | 4276224 |
--------------------------------
Eval num_timesteps=4280000, episode_reward=-271.97 +/- 60.00
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -272        |
| time/                   |             |
|    total_timesteps      | 4280000     |
| train/                  |             |
|    approx_kl            | 0.004141477 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.000123    |
|    n_updates            | 10440       |
|    policy_gradient_loss | -5.04e-05   |
|    std                  | 0.747       |
|    value_loss           | 2.11        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 256     |
|    iterations      | 1045    |
|    time_elapsed    | 16688   |
|    total_timesteps | 4280320 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 256          |
|    iterations           | 1046         |
|    time_elapsed         | 16694        |
|    total_timesteps      | 4284416      |
| train/                  |              |
|    approx_kl            | 0.0052033095 |
|    clip_fraction        | 0.0346       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.33        |
|    explained_variance   | 0.967        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.177        |
|    n_updates            | 10450        |
|    policy_gradient_loss | -0.00145     |
|    std                  | 0.745        |
|    value_loss           | 6.21         |
------------------------------------------
Eval num_timesteps=4285000, episode_reward=-429.27 +/- 151.93
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -429         |
| time/                   |              |
|    total_timesteps      | 4285000      |
| train/                  |              |
|    approx_kl            | 0.0049474384 |
|    clip_fraction        | 0.0589       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.33        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0285       |
|    n_updates            | 10460        |
|    policy_gradient_loss | -0.0025      |
|    std                  | 0.748        |
|    value_loss           | 0.062        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 256     |
|    iterations      | 1047    |
|    time_elapsed    | 16704   |
|    total_timesteps | 4288512 |
--------------------------------
Eval num_timesteps=4290000, episode_reward=-382.54 +/- 148.15
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -383        |
| time/                   |             |
|    total_timesteps      | 4290000     |
| train/                  |             |
|    approx_kl            | 0.005456085 |
|    clip_fraction        | 0.0476      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0177      |
|    n_updates            | 10470       |
|    policy_gradient_loss | -0.00265    |
|    std                  | 0.747       |
|    value_loss           | 0.085       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 256     |
|    iterations      | 1048    |
|    time_elapsed    | 16715   |
|    total_timesteps | 4292608 |
--------------------------------
Eval num_timesteps=4295000, episode_reward=-214.37 +/- 28.17
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -214        |
| time/                   |             |
|    total_timesteps      | 4295000     |
| train/                  |             |
|    approx_kl            | 0.004263363 |
|    clip_fraction        | 0.0373      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00675     |
|    n_updates            | 10480       |
|    policy_gradient_loss | -0.00221    |
|    std                  | 0.75        |
|    value_loss           | 0.257       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 256     |
|    iterations      | 1049    |
|    time_elapsed    | 16725   |
|    total_timesteps | 4296704 |
--------------------------------
Eval num_timesteps=4300000, episode_reward=-296.55 +/- 132.81
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -297        |
| time/                   |             |
|    total_timesteps      | 4300000     |
| train/                  |             |
|    approx_kl            | 0.005894283 |
|    clip_fraction        | 0.0485      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.9         |
|    n_updates            | 10490       |
|    policy_gradient_loss | -0.0026     |
|    std                  | 0.751       |
|    value_loss           | 3.22        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 256     |
|    iterations      | 1050    |
|    time_elapsed    | 16736   |
|    total_timesteps | 4300800 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 257          |
|    iterations           | 1051         |
|    time_elapsed         | 16743        |
|    total_timesteps      | 4304896      |
| train/                  |              |
|    approx_kl            | 0.0040859752 |
|    clip_fraction        | 0.0414       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.58e-06     |
|    n_updates            | 10500        |
|    policy_gradient_loss | -0.00121     |
|    std                  | 0.755        |
|    value_loss           | 0.597        |
------------------------------------------
Eval num_timesteps=4305000, episode_reward=-262.56 +/- 172.26
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -263        |
| time/                   |             |
|    total_timesteps      | 4305000     |
| train/                  |             |
|    approx_kl            | 0.005594467 |
|    clip_fraction        | 0.0439      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.019       |
|    n_updates            | 10510       |
|    policy_gradient_loss | -0.0016     |
|    std                  | 0.747       |
|    value_loss           | 0.0259      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 257     |
|    iterations      | 1052    |
|    time_elapsed    | 16755   |
|    total_timesteps | 4308992 |
--------------------------------
Eval num_timesteps=4310000, episode_reward=-279.21 +/- 152.25
Episode length: 815.20 +/- 371.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 815         |
|    mean_reward          | -279        |
| time/                   |             |
|    total_timesteps      | 4310000     |
| train/                  |             |
|    approx_kl            | 0.004440446 |
|    clip_fraction        | 0.0202      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.325       |
|    n_updates            | 10520       |
|    policy_gradient_loss | -0.00109    |
|    std                  | 0.748       |
|    value_loss           | 1.76        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 257     |
|    iterations      | 1053    |
|    time_elapsed    | 16766   |
|    total_timesteps | 4313088 |
--------------------------------
Eval num_timesteps=4315000, episode_reward=-375.61 +/- 152.12
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -376         |
| time/                   |              |
|    total_timesteps      | 4315000      |
| train/                  |              |
|    approx_kl            | 0.0054345555 |
|    clip_fraction        | 0.0421       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.33        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0835       |
|    n_updates            | 10530        |
|    policy_gradient_loss | -0.00187     |
|    std                  | 0.744        |
|    value_loss           | 0.655        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 257     |
|    iterations      | 1054    |
|    time_elapsed    | 16776   |
|    total_timesteps | 4317184 |
--------------------------------
Eval num_timesteps=4320000, episode_reward=-217.48 +/- 46.90
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -217        |
| time/                   |             |
|    total_timesteps      | 4320000     |
| train/                  |             |
|    approx_kl            | 0.005603645 |
|    clip_fraction        | 0.0551      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.36        |
|    n_updates            | 10540       |
|    policy_gradient_loss | -0.00238    |
|    std                  | 0.746       |
|    value_loss           | 0.516       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 257     |
|    iterations      | 1055    |
|    time_elapsed    | 16787   |
|    total_timesteps | 4321280 |
--------------------------------
Eval num_timesteps=4325000, episode_reward=-337.34 +/- 188.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -337         |
| time/                   |              |
|    total_timesteps      | 4325000      |
| train/                  |              |
|    approx_kl            | 0.0058919108 |
|    clip_fraction        | 0.0591       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.33        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0351       |
|    n_updates            | 10550        |
|    policy_gradient_loss | -0.00327     |
|    std                  | 0.745        |
|    value_loss           | 0.0806       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 257     |
|    iterations      | 1056    |
|    time_elapsed    | 16797   |
|    total_timesteps | 4325376 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 257         |
|    iterations           | 1057        |
|    time_elapsed         | 16803       |
|    total_timesteps      | 4329472     |
| train/                  |             |
|    approx_kl            | 0.009001451 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.129       |
|    n_updates            | 10560       |
|    policy_gradient_loss | -0.00487    |
|    std                  | 0.747       |
|    value_loss           | 3.16        |
-----------------------------------------
Eval num_timesteps=4330000, episode_reward=-352.35 +/- 135.98
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -352        |
| time/                   |             |
|    total_timesteps      | 4330000     |
| train/                  |             |
|    approx_kl            | 0.006116949 |
|    clip_fraction        | 0.0488      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00067    |
|    n_updates            | 10570       |
|    policy_gradient_loss | -0.00127    |
|    std                  | 0.753       |
|    value_loss           | 0.0237      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 257     |
|    iterations      | 1058    |
|    time_elapsed    | 16814   |
|    total_timesteps | 4333568 |
--------------------------------
Eval num_timesteps=4335000, episode_reward=-175.11 +/- 100.83
Episode length: 805.00 +/- 392.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 805          |
|    mean_reward          | -175         |
| time/                   |              |
|    total_timesteps      | 4335000      |
| train/                  |              |
|    approx_kl            | 0.0047970456 |
|    clip_fraction        | 0.0371       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.127        |
|    n_updates            | 10580        |
|    policy_gradient_loss | -0.00243     |
|    std                  | 0.753        |
|    value_loss           | 3.91         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 257     |
|    iterations      | 1059    |
|    time_elapsed    | 16824   |
|    total_timesteps | 4337664 |
--------------------------------
Eval num_timesteps=4340000, episode_reward=-234.96 +/- 129.21
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -235        |
| time/                   |             |
|    total_timesteps      | 4340000     |
| train/                  |             |
|    approx_kl            | 0.004857002 |
|    clip_fraction        | 0.0539      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.134       |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.00152    |
|    std                  | 0.751       |
|    value_loss           | 11.2        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 257     |
|    iterations      | 1060    |
|    time_elapsed    | 16837   |
|    total_timesteps | 4341760 |
--------------------------------
Eval num_timesteps=4345000, episode_reward=-282.04 +/- 165.67
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -282         |
| time/                   |              |
|    total_timesteps      | 4345000      |
| train/                  |              |
|    approx_kl            | 0.0055262106 |
|    clip_fraction        | 0.0477       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.000596     |
|    n_updates            | 10600        |
|    policy_gradient_loss | -0.00179     |
|    std                  | 0.753        |
|    value_loss           | 0.0253       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 257     |
|    iterations      | 1061    |
|    time_elapsed    | 16851   |
|    total_timesteps | 4345856 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 258          |
|    iterations           | 1062         |
|    time_elapsed         | 16857        |
|    total_timesteps      | 4349952      |
| train/                  |              |
|    approx_kl            | 0.0049527255 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00816      |
|    n_updates            | 10610        |
|    policy_gradient_loss | -0.000545    |
|    std                  | 0.751        |
|    value_loss           | 0.621        |
------------------------------------------
Eval num_timesteps=4350000, episode_reward=-133.09 +/- 70.19
Episode length: 807.80 +/- 386.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -133         |
| time/                   |              |
|    total_timesteps      | 4350000      |
| train/                  |              |
|    approx_kl            | 0.0060941856 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00749      |
|    n_updates            | 10620        |
|    policy_gradient_loss | -0.00308     |
|    std                  | 0.752        |
|    value_loss           | 0.0166       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1063    |
|    time_elapsed    | 16867   |
|    total_timesteps | 4354048 |
--------------------------------
Eval num_timesteps=4355000, episode_reward=-280.65 +/- 208.48
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -281       |
| time/                   |            |
|    total_timesteps      | 4355000    |
| train/                  |            |
|    approx_kl            | 0.00511832 |
|    clip_fraction        | 0.0466     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.37      |
|    explained_variance   | 0.985      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.157      |
|    n_updates            | 10630      |
|    policy_gradient_loss | -0.00223   |
|    std                  | 0.753      |
|    value_loss           | 0.612      |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1064    |
|    time_elapsed    | 16877   |
|    total_timesteps | 4358144 |
--------------------------------
Eval num_timesteps=4360000, episode_reward=-241.53 +/- 165.73
Episode length: 809.40 +/- 383.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 809          |
|    mean_reward          | -242         |
| time/                   |              |
|    total_timesteps      | 4360000      |
| train/                  |              |
|    approx_kl            | 0.0055248626 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.37        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.18         |
|    n_updates            | 10640        |
|    policy_gradient_loss | -0.00241     |
|    std                  | 0.755        |
|    value_loss           | 1.78         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1065    |
|    time_elapsed    | 16887   |
|    total_timesteps | 4362240 |
--------------------------------
Eval num_timesteps=4365000, episode_reward=-233.95 +/- 191.38
Episode length: 809.20 +/- 383.60
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 809         |
|    mean_reward          | -234        |
| time/                   |             |
|    total_timesteps      | 4365000     |
| train/                  |             |
|    approx_kl            | 0.003972695 |
|    clip_fraction        | 0.018       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.37       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0486      |
|    n_updates            | 10650       |
|    policy_gradient_loss | -0.000535   |
|    std                  | 0.751       |
|    value_loss           | 3.77        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1066    |
|    time_elapsed    | 16897   |
|    total_timesteps | 4366336 |
--------------------------------
Eval num_timesteps=4370000, episode_reward=-408.96 +/- 168.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -409        |
| time/                   |             |
|    total_timesteps      | 4370000     |
| train/                  |             |
|    approx_kl            | 0.007212593 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.533       |
|    n_updates            | 10660       |
|    policy_gradient_loss | -0.00227    |
|    std                  | 0.751       |
|    value_loss           | 1.57        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1067    |
|    time_elapsed    | 16908   |
|    total_timesteps | 4370432 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 258          |
|    iterations           | 1068         |
|    time_elapsed         | 16915        |
|    total_timesteps      | 4374528      |
| train/                  |              |
|    approx_kl            | 0.0050877486 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.972        |
|    learning_rate        | 0.0001       |
|    loss                 | 36.5         |
|    n_updates            | 10670        |
|    policy_gradient_loss | -0.00231     |
|    std                  | 0.75         |
|    value_loss           | 11.9         |
------------------------------------------
Eval num_timesteps=4375000, episode_reward=-469.62 +/- 149.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -470        |
| time/                   |             |
|    total_timesteps      | 4375000     |
| train/                  |             |
|    approx_kl            | 0.005876868 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00263    |
|    n_updates            | 10680       |
|    policy_gradient_loss | -0.00452    |
|    std                  | 0.746       |
|    value_loss           | 0.0347      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1069    |
|    time_elapsed    | 16927   |
|    total_timesteps | 4378624 |
--------------------------------
Eval num_timesteps=4380000, episode_reward=-371.04 +/- 168.94
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -371        |
| time/                   |             |
|    total_timesteps      | 4380000     |
| train/                  |             |
|    approx_kl            | 0.003170575 |
|    clip_fraction        | 0.0199      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.35       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.33        |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.000367   |
|    std                  | 0.747       |
|    value_loss           | 5.18        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1070    |
|    time_elapsed    | 16940   |
|    total_timesteps | 4382720 |
--------------------------------
Eval num_timesteps=4385000, episode_reward=-319.40 +/- 138.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -319         |
| time/                   |              |
|    total_timesteps      | 4385000      |
| train/                  |              |
|    approx_kl            | 0.0051245717 |
|    clip_fraction        | 0.0519       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.35        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 3.52         |
|    n_updates            | 10700        |
|    policy_gradient_loss | -0.00261     |
|    std                  | 0.745        |
|    value_loss           | 0.681        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1071    |
|    time_elapsed    | 16950   |
|    total_timesteps | 4386816 |
--------------------------------
Eval num_timesteps=4390000, episode_reward=-357.93 +/- 109.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -358         |
| time/                   |              |
|    total_timesteps      | 4390000      |
| train/                  |              |
|    approx_kl            | 0.0054607713 |
|    clip_fraction        | 0.04         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.33        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.01         |
|    n_updates            | 10710        |
|    policy_gradient_loss | -0.000883    |
|    std                  | 0.74         |
|    value_loss           | 1.2          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1072    |
|    time_elapsed    | 16961   |
|    total_timesteps | 4390912 |
--------------------------------
Eval num_timesteps=4395000, episode_reward=-289.28 +/- 172.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -289         |
| time/                   |              |
|    total_timesteps      | 4395000      |
| train/                  |              |
|    approx_kl            | 0.0050540026 |
|    clip_fraction        | 0.0267       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.33        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0541       |
|    n_updates            | 10720        |
|    policy_gradient_loss | -0.000664    |
|    std                  | 0.742        |
|    value_loss           | 5.08         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 258     |
|    iterations      | 1073    |
|    time_elapsed    | 16971   |
|    total_timesteps | 4395008 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 259         |
|    iterations           | 1074        |
|    time_elapsed         | 16977       |
|    total_timesteps      | 4399104     |
| train/                  |             |
|    approx_kl            | 0.006310284 |
|    clip_fraction        | 0.0695      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0542      |
|    n_updates            | 10730       |
|    policy_gradient_loss | -0.00358    |
|    std                  | 0.746       |
|    value_loss           | 1.24        |
-----------------------------------------
Eval num_timesteps=4400000, episode_reward=-370.59 +/- 153.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -371         |
| time/                   |              |
|    total_timesteps      | 4400000      |
| train/                  |              |
|    approx_kl            | 0.0039389348 |
|    clip_fraction        | 0.0302       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.35        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0205       |
|    n_updates            | 10740        |
|    policy_gradient_loss | 0.000272     |
|    std                  | 0.748        |
|    value_loss           | 0.0697       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 259     |
|    iterations      | 1075    |
|    time_elapsed    | 16988   |
|    total_timesteps | 4403200 |
--------------------------------
Eval num_timesteps=4405000, episode_reward=-212.59 +/- 96.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -213         |
| time/                   |              |
|    total_timesteps      | 4405000      |
| train/                  |              |
|    approx_kl            | 0.0059852023 |
|    clip_fraction        | 0.0737       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.35        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.528        |
|    n_updates            | 10750        |
|    policy_gradient_loss | -0.00326     |
|    std                  | 0.749        |
|    value_loss           | 0.471        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 259     |
|    iterations      | 1076    |
|    time_elapsed    | 16999   |
|    total_timesteps | 4407296 |
--------------------------------
Eval num_timesteps=4410000, episode_reward=-344.76 +/- 253.45
Episode length: 810.80 +/- 380.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -345        |
| time/                   |             |
|    total_timesteps      | 4410000     |
| train/                  |             |
|    approx_kl            | 0.007021397 |
|    clip_fraction        | 0.045       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.94        |
|    n_updates            | 10760       |
|    policy_gradient_loss | -0.00128    |
|    std                  | 0.749       |
|    value_loss           | 2.84        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 259     |
|    iterations      | 1077    |
|    time_elapsed    | 17010   |
|    total_timesteps | 4411392 |
--------------------------------
Eval num_timesteps=4415000, episode_reward=-253.90 +/- 160.69
Episode length: 810.00 +/- 382.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 810         |
|    mean_reward          | -254        |
| time/                   |             |
|    total_timesteps      | 4415000     |
| train/                  |             |
|    approx_kl            | 0.004247467 |
|    clip_fraction        | 0.0522      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.55        |
|    n_updates            | 10770       |
|    policy_gradient_loss | -0.00264    |
|    std                  | 0.749       |
|    value_loss           | 1.13        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 259     |
|    iterations      | 1078    |
|    time_elapsed    | 17022   |
|    total_timesteps | 4415488 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 259          |
|    iterations           | 1079         |
|    time_elapsed         | 17029        |
|    total_timesteps      | 4419584      |
| train/                  |              |
|    approx_kl            | 0.0071583334 |
|    clip_fraction        | 0.0799       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.36        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00761     |
|    n_updates            | 10780        |
|    policy_gradient_loss | -0.00374     |
|    std                  | 0.75         |
|    value_loss           | 0.867        |
------------------------------------------
Eval num_timesteps=4420000, episode_reward=-240.25 +/- 131.34
Episode length: 808.20 +/- 385.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -240         |
| time/                   |              |
|    total_timesteps      | 4420000      |
| train/                  |              |
|    approx_kl            | 0.0077334074 |
|    clip_fraction        | 0.0818       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.35        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00166      |
|    n_updates            | 10790        |
|    policy_gradient_loss | -0.00291     |
|    std                  | 0.746        |
|    value_loss           | 0.131        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 259     |
|    iterations      | 1080    |
|    time_elapsed    | 17039   |
|    total_timesteps | 4423680 |
--------------------------------
Eval num_timesteps=4425000, episode_reward=-305.84 +/- 186.06
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -306        |
| time/                   |             |
|    total_timesteps      | 4425000     |
| train/                  |             |
|    approx_kl            | 0.005909914 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0001      |
|    loss                 | 30.4        |
|    n_updates            | 10800       |
|    policy_gradient_loss | -0.00131    |
|    std                  | 0.745       |
|    value_loss           | 19.7        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 259     |
|    iterations      | 1081    |
|    time_elapsed    | 17050   |
|    total_timesteps | 4427776 |
--------------------------------
Eval num_timesteps=4430000, episode_reward=-257.44 +/- 157.42
Episode length: 805.80 +/- 390.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 806         |
|    mean_reward          | -257        |
| time/                   |             |
|    total_timesteps      | 4430000     |
| train/                  |             |
|    approx_kl            | 0.005686756 |
|    clip_fraction        | 0.0702      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.34       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.022       |
|    n_updates            | 10810       |
|    policy_gradient_loss | -0.00285    |
|    std                  | 0.744       |
|    value_loss           | 0.204       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 259     |
|    iterations      | 1082    |
|    time_elapsed    | 17060   |
|    total_timesteps | 4431872 |
--------------------------------
Eval num_timesteps=4435000, episode_reward=-197.43 +/- 185.48
Episode length: 624.40 +/- 461.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 624         |
|    mean_reward          | -197        |
| time/                   |             |
|    total_timesteps      | 4435000     |
| train/                  |             |
|    approx_kl            | 0.005401557 |
|    clip_fraction        | 0.0386      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.288       |
|    n_updates            | 10820       |
|    policy_gradient_loss | -0.00164    |
|    std                  | 0.742       |
|    value_loss           | 2.05        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 259     |
|    iterations      | 1083    |
|    time_elapsed    | 17069   |
|    total_timesteps | 4435968 |
--------------------------------
Eval num_timesteps=4440000, episode_reward=-216.20 +/- 163.62
Episode length: 812.00 +/- 378.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 812         |
|    mean_reward          | -216        |
| time/                   |             |
|    total_timesteps      | 4440000     |
| train/                  |             |
|    approx_kl            | 0.005283947 |
|    clip_fraction        | 0.0256      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.197       |
|    n_updates            | 10830       |
|    policy_gradient_loss | -0.00115    |
|    std                  | 0.743       |
|    value_loss           | 5.52        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 259     |
|    iterations      | 1084    |
|    time_elapsed    | 17080   |
|    total_timesteps | 4440064 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 260         |
|    iterations           | 1085        |
|    time_elapsed         | 17086       |
|    total_timesteps      | 4444160     |
| train/                  |             |
|    approx_kl            | 0.006388538 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00846    |
|    n_updates            | 10840       |
|    policy_gradient_loss | -0.0041     |
|    std                  | 0.739       |
|    value_loss           | 0.017       |
-----------------------------------------
Eval num_timesteps=4445000, episode_reward=-309.34 +/- 222.57
Episode length: 807.60 +/- 386.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 808          |
|    mean_reward          | -309         |
| time/                   |              |
|    total_timesteps      | 4445000      |
| train/                  |              |
|    approx_kl            | 0.0045656026 |
|    clip_fraction        | 0.0327       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.32        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00618      |
|    n_updates            | 10850        |
|    policy_gradient_loss | -0.00107     |
|    std                  | 0.736        |
|    value_loss           | 0.0725       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 260     |
|    iterations      | 1086    |
|    time_elapsed    | 17095   |
|    total_timesteps | 4448256 |
--------------------------------
Eval num_timesteps=4450000, episode_reward=-204.05 +/- 123.58
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -204         |
| time/                   |              |
|    total_timesteps      | 4450000      |
| train/                  |              |
|    approx_kl            | 0.0043087844 |
|    clip_fraction        | 0.0196       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.31        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0339       |
|    n_updates            | 10860        |
|    policy_gradient_loss | -0.00076     |
|    std                  | 0.734        |
|    value_loss           | 2.98         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 260     |
|    iterations      | 1087    |
|    time_elapsed    | 17106   |
|    total_timesteps | 4452352 |
--------------------------------
Eval num_timesteps=4455000, episode_reward=-268.79 +/- 131.16
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -269         |
| time/                   |              |
|    total_timesteps      | 4455000      |
| train/                  |              |
|    approx_kl            | 0.0051559163 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.31        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0116      |
|    n_updates            | 10870        |
|    policy_gradient_loss | -0.00249     |
|    std                  | 0.736        |
|    value_loss           | 0.171        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 260     |
|    iterations      | 1088    |
|    time_elapsed    | 17116   |
|    total_timesteps | 4456448 |
--------------------------------
Eval num_timesteps=4460000, episode_reward=-377.59 +/- 183.25
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -378        |
| time/                   |             |
|    total_timesteps      | 4460000     |
| train/                  |             |
|    approx_kl            | 0.005025494 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.31       |
|    explained_variance   | 0.981       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0621      |
|    n_updates            | 10880       |
|    policy_gradient_loss | -0.00237    |
|    std                  | 0.734       |
|    value_loss           | 3.48        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 260     |
|    iterations      | 1089    |
|    time_elapsed    | 17126   |
|    total_timesteps | 4460544 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 260          |
|    iterations           | 1090         |
|    time_elapsed         | 17133        |
|    total_timesteps      | 4464640      |
| train/                  |              |
|    approx_kl            | 0.0048115333 |
|    clip_fraction        | 0.0419       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.31        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.849        |
|    n_updates            | 10890        |
|    policy_gradient_loss | -0.00084     |
|    std                  | 0.735        |
|    value_loss           | 6.21         |
------------------------------------------
Eval num_timesteps=4465000, episode_reward=-414.42 +/- 134.61
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -414       |
| time/                   |            |
|    total_timesteps      | 4465000    |
| train/                  |            |
|    approx_kl            | 0.00546704 |
|    clip_fraction        | 0.0483     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.31      |
|    explained_variance   | 1          |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0183     |
|    n_updates            | 10900      |
|    policy_gradient_loss | -0.00119   |
|    std                  | 0.738      |
|    value_loss           | 0.0268     |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 260     |
|    iterations      | 1091    |
|    time_elapsed    | 17143   |
|    total_timesteps | 4468736 |
--------------------------------
Eval num_timesteps=4470000, episode_reward=-198.13 +/- 85.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -198        |
| time/                   |             |
|    total_timesteps      | 4470000     |
| train/                  |             |
|    approx_kl            | 0.003586994 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.32       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0111      |
|    n_updates            | 10910       |
|    policy_gradient_loss | -0.00172    |
|    std                  | 0.736       |
|    value_loss           | 1.64        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 260     |
|    iterations      | 1092    |
|    time_elapsed    | 17153   |
|    total_timesteps | 4472832 |
--------------------------------
Eval num_timesteps=4475000, episode_reward=-185.29 +/- 67.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -185         |
| time/                   |              |
|    total_timesteps      | 4475000      |
| train/                  |              |
|    approx_kl            | 0.0052748714 |
|    clip_fraction        | 0.0386       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.31        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.54         |
|    n_updates            | 10920        |
|    policy_gradient_loss | -0.00203     |
|    std                  | 0.737        |
|    value_loss           | 2.05         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 260     |
|    iterations      | 1093    |
|    time_elapsed    | 17164   |
|    total_timesteps | 4476928 |
--------------------------------
Eval num_timesteps=4480000, episode_reward=-360.34 +/- 89.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -360         |
| time/                   |              |
|    total_timesteps      | 4480000      |
| train/                  |              |
|    approx_kl            | 0.0039236634 |
|    clip_fraction        | 0.0494       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.31        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0567       |
|    n_updates            | 10930        |
|    policy_gradient_loss | -0.0034      |
|    std                  | 0.736        |
|    value_loss           | 9.79         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 260     |
|    iterations      | 1094    |
|    time_elapsed    | 17175   |
|    total_timesteps | 4481024 |
--------------------------------
Eval num_timesteps=4485000, episode_reward=-262.98 +/- 126.53
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -263         |
| time/                   |              |
|    total_timesteps      | 4485000      |
| train/                  |              |
|    approx_kl            | 0.0028467316 |
|    clip_fraction        | 0.0395       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.31        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 18           |
|    n_updates            | 10940        |
|    policy_gradient_loss | -0.00107     |
|    std                  | 0.735        |
|    value_loss           | 21           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 260     |
|    iterations      | 1095    |
|    time_elapsed    | 17185   |
|    total_timesteps | 4485120 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 1096        |
|    time_elapsed         | 17191       |
|    total_timesteps      | 4489216     |
| train/                  |             |
|    approx_kl            | 0.006201895 |
|    clip_fraction        | 0.0518      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.215       |
|    n_updates            | 10950       |
|    policy_gradient_loss | -0.00169    |
|    std                  | 0.732       |
|    value_loss           | 2.92        |
-----------------------------------------
Eval num_timesteps=4490000, episode_reward=-320.32 +/- 107.33
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -320        |
| time/                   |             |
|    total_timesteps      | 4490000     |
| train/                  |             |
|    approx_kl            | 0.005001945 |
|    clip_fraction        | 0.0424      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0265      |
|    n_updates            | 10960       |
|    policy_gradient_loss | -0.00285    |
|    std                  | 0.727       |
|    value_loss           | 0.0496      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 261     |
|    iterations      | 1097    |
|    time_elapsed    | 17201   |
|    total_timesteps | 4493312 |
--------------------------------
Eval num_timesteps=4495000, episode_reward=-227.08 +/- 100.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -227         |
| time/                   |              |
|    total_timesteps      | 4495000      |
| train/                  |              |
|    approx_kl            | 0.0052824086 |
|    clip_fraction        | 0.0495       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0253       |
|    n_updates            | 10970        |
|    policy_gradient_loss | -0.0024      |
|    std                  | 0.726        |
|    value_loss           | 0.0871       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 261     |
|    iterations      | 1098    |
|    time_elapsed    | 17212   |
|    total_timesteps | 4497408 |
--------------------------------
Eval num_timesteps=4500000, episode_reward=-383.98 +/- 190.82
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -384        |
| time/                   |             |
|    total_timesteps      | 4500000     |
| train/                  |             |
|    approx_kl            | 0.004324886 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0187      |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.00161    |
|    std                  | 0.723       |
|    value_loss           | 0.5         |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 261     |
|    iterations      | 1099    |
|    time_elapsed    | 17222   |
|    total_timesteps | 4501504 |
--------------------------------
Eval num_timesteps=4505000, episode_reward=-286.61 +/- 118.04
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -287        |
| time/                   |             |
|    total_timesteps      | 4505000     |
| train/                  |             |
|    approx_kl            | 0.004967754 |
|    clip_fraction        | 0.0396      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0133      |
|    n_updates            | 10990       |
|    policy_gradient_loss | -0.00203    |
|    std                  | 0.722       |
|    value_loss           | 0.412       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 261     |
|    iterations      | 1100    |
|    time_elapsed    | 17232   |
|    total_timesteps | 4505600 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 1101        |
|    time_elapsed         | 17238       |
|    total_timesteps      | 4509696     |
| train/                  |             |
|    approx_kl            | 0.003570153 |
|    clip_fraction        | 0.0329      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.65        |
|    n_updates            | 11000       |
|    policy_gradient_loss | -0.0025     |
|    std                  | 0.72        |
|    value_loss           | 16.3        |
-----------------------------------------
Eval num_timesteps=4510000, episode_reward=-254.41 +/- 98.84
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -254         |
| time/                   |              |
|    total_timesteps      | 4510000      |
| train/                  |              |
|    approx_kl            | 0.0049438127 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0237       |
|    n_updates            | 11010        |
|    policy_gradient_loss | -0.00117     |
|    std                  | 0.725        |
|    value_loss           | 0.0391       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 261     |
|    iterations      | 1102    |
|    time_elapsed    | 17248   |
|    total_timesteps | 4513792 |
--------------------------------
Eval num_timesteps=4515000, episode_reward=-454.55 +/- 65.33
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -455        |
| time/                   |             |
|    total_timesteps      | 4515000     |
| train/                  |             |
|    approx_kl            | 0.004858943 |
|    clip_fraction        | 0.0524      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00155     |
|    n_updates            | 11020       |
|    policy_gradient_loss | -0.00263    |
|    std                  | 0.725       |
|    value_loss           | 0.138       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 261     |
|    iterations      | 1103    |
|    time_elapsed    | 17259   |
|    total_timesteps | 4517888 |
--------------------------------
Eval num_timesteps=4520000, episode_reward=-247.31 +/- 37.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -247         |
| time/                   |              |
|    total_timesteps      | 4520000      |
| train/                  |              |
|    approx_kl            | 0.0045982273 |
|    clip_fraction        | 0.0545       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0612       |
|    n_updates            | 11030        |
|    policy_gradient_loss | -0.00298     |
|    std                  | 0.729        |
|    value_loss           | 0.196        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 261     |
|    iterations      | 1104    |
|    time_elapsed    | 17269   |
|    total_timesteps | 4521984 |
--------------------------------
Eval num_timesteps=4525000, episode_reward=-378.75 +/- 234.98
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -379        |
| time/                   |             |
|    total_timesteps      | 4525000     |
| train/                  |             |
|    approx_kl            | 0.005516019 |
|    clip_fraction        | 0.0433      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0412      |
|    n_updates            | 11040       |
|    policy_gradient_loss | -0.00333    |
|    std                  | 0.725       |
|    value_loss           | 1.85        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 261     |
|    iterations      | 1105    |
|    time_elapsed    | 17279   |
|    total_timesteps | 4526080 |
--------------------------------
Eval num_timesteps=4530000, episode_reward=-270.35 +/- 67.47
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -270       |
| time/                   |            |
|    total_timesteps      | 4530000    |
| train/                  |            |
|    approx_kl            | 0.00485631 |
|    clip_fraction        | 0.0401     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.26      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0532     |
|    n_updates            | 11050      |
|    policy_gradient_loss | -0.00129   |
|    std                  | 0.725      |
|    value_loss           | 3.74       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1106    |
|    time_elapsed    | 17289   |
|    total_timesteps | 4530176 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 262          |
|    iterations           | 1107         |
|    time_elapsed         | 17296        |
|    total_timesteps      | 4534272      |
| train/                  |              |
|    approx_kl            | 0.0055922344 |
|    clip_fraction        | 0.0583       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0879       |
|    n_updates            | 11060        |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.721        |
|    value_loss           | 0.0798       |
------------------------------------------
Eval num_timesteps=4535000, episode_reward=-421.33 +/- 165.95
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -421         |
| time/                   |              |
|    total_timesteps      | 4535000      |
| train/                  |              |
|    approx_kl            | 0.0042187436 |
|    clip_fraction        | 0.032        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.24        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0131       |
|    n_updates            | 11070        |
|    policy_gradient_loss | -0.000589    |
|    std                  | 0.716        |
|    value_loss           | 0.0363       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1108    |
|    time_elapsed    | 17306   |
|    total_timesteps | 4538368 |
--------------------------------
Eval num_timesteps=4540000, episode_reward=-331.24 +/- 191.68
Episode length: 801.00 +/- 400.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 801         |
|    mean_reward          | -331        |
| time/                   |             |
|    total_timesteps      | 4540000     |
| train/                  |             |
|    approx_kl            | 0.003899341 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0001      |
|    loss                 | 19.4        |
|    n_updates            | 11080       |
|    policy_gradient_loss | -0.000284   |
|    std                  | 0.718       |
|    value_loss           | 12.5        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1109    |
|    time_elapsed    | 17316   |
|    total_timesteps | 4542464 |
--------------------------------
Eval num_timesteps=4545000, episode_reward=-365.13 +/- 171.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 4545000      |
| train/                  |              |
|    approx_kl            | 0.0031719606 |
|    clip_fraction        | 0.0248       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.24        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00153     |
|    n_updates            | 11090        |
|    policy_gradient_loss | 0.000511     |
|    std                  | 0.717        |
|    value_loss           | 0.0578       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1110    |
|    time_elapsed    | 17326   |
|    total_timesteps | 4546560 |
--------------------------------
Eval num_timesteps=4550000, episode_reward=-328.72 +/- 144.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -329        |
| time/                   |             |
|    total_timesteps      | 4550000     |
| train/                  |             |
|    approx_kl            | 0.004097074 |
|    clip_fraction        | 0.0301      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.23       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0345      |
|    n_updates            | 11100       |
|    policy_gradient_loss | -0.0011     |
|    std                  | 0.718       |
|    value_loss           | 0.337       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1111    |
|    time_elapsed    | 17337   |
|    total_timesteps | 4550656 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 262          |
|    iterations           | 1112         |
|    time_elapsed         | 17344        |
|    total_timesteps      | 4554752      |
| train/                  |              |
|    approx_kl            | 0.0075576017 |
|    clip_fraction        | 0.0645       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.24        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00574     |
|    n_updates            | 11110        |
|    policy_gradient_loss | -0.0044      |
|    std                  | 0.721        |
|    value_loss           | 0.0891       |
------------------------------------------
Eval num_timesteps=4555000, episode_reward=-340.68 +/- 102.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -341         |
| time/                   |              |
|    total_timesteps      | 4555000      |
| train/                  |              |
|    approx_kl            | 0.0056823785 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0253      |
|    n_updates            | 11120        |
|    policy_gradient_loss | -0.0018      |
|    std                  | 0.721        |
|    value_loss           | 0.0273       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1113    |
|    time_elapsed    | 17354   |
|    total_timesteps | 4558848 |
--------------------------------
Eval num_timesteps=4560000, episode_reward=-448.92 +/- 126.84
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -449         |
| time/                   |              |
|    total_timesteps      | 4560000      |
| train/                  |              |
|    approx_kl            | 0.0039455164 |
|    clip_fraction        | 0.0316       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.45         |
|    n_updates            | 11130        |
|    policy_gradient_loss | -0.00171     |
|    std                  | 0.722        |
|    value_loss           | 2.65         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1114    |
|    time_elapsed    | 17365   |
|    total_timesteps | 4562944 |
--------------------------------
Eval num_timesteps=4565000, episode_reward=-321.65 +/- 166.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -322         |
| time/                   |              |
|    total_timesteps      | 4565000      |
| train/                  |              |
|    approx_kl            | 0.0069957953 |
|    clip_fraction        | 0.048        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0371       |
|    n_updates            | 11140        |
|    policy_gradient_loss | -0.00133     |
|    std                  | 0.719        |
|    value_loss           | 0.801        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1115    |
|    time_elapsed    | 17375   |
|    total_timesteps | 4567040 |
--------------------------------
Eval num_timesteps=4570000, episode_reward=-309.68 +/- 113.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -310         |
| time/                   |              |
|    total_timesteps      | 4570000      |
| train/                  |              |
|    approx_kl            | 0.0048171887 |
|    clip_fraction        | 0.0352       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.24        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 12.8         |
|    n_updates            | 11150        |
|    policy_gradient_loss | -0.00407     |
|    std                  | 0.719        |
|    value_loss           | 10           |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1116    |
|    time_elapsed    | 17386   |
|    total_timesteps | 4571136 |
--------------------------------
Eval num_timesteps=4575000, episode_reward=-322.50 +/- 122.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -323         |
| time/                   |              |
|    total_timesteps      | 4575000      |
| train/                  |              |
|    approx_kl            | 0.0077368966 |
|    clip_fraction        | 0.0724       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.24        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0304       |
|    n_updates            | 11160        |
|    policy_gradient_loss | -0.00322     |
|    std                  | 0.72         |
|    value_loss           | 8.7          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 262     |
|    iterations      | 1117    |
|    time_elapsed    | 17396   |
|    total_timesteps | 4575232 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 263          |
|    iterations           | 1118         |
|    time_elapsed         | 17403        |
|    total_timesteps      | 4579328      |
| train/                  |              |
|    approx_kl            | 0.0057258513 |
|    clip_fraction        | 0.051        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.26        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.043        |
|    n_updates            | 11170        |
|    policy_gradient_loss | -0.00129     |
|    std                  | 0.728        |
|    value_loss           | 0.463        |
------------------------------------------
Eval num_timesteps=4580000, episode_reward=-325.82 +/- 177.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -326         |
| time/                   |              |
|    total_timesteps      | 4580000      |
| train/                  |              |
|    approx_kl            | 0.0043244674 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00109      |
|    n_updates            | 11180        |
|    policy_gradient_loss | -0.000483    |
|    std                  | 0.729        |
|    value_loss           | 0.0319       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 263     |
|    iterations      | 1119    |
|    time_elapsed    | 17413   |
|    total_timesteps | 4583424 |
--------------------------------
Eval num_timesteps=4585000, episode_reward=-299.46 +/- 88.53
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -299         |
| time/                   |              |
|    total_timesteps      | 4585000      |
| train/                  |              |
|    approx_kl            | 0.0054878737 |
|    clip_fraction        | 0.0482       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.139        |
|    n_updates            | 11190        |
|    policy_gradient_loss | -0.00296     |
|    std                  | 0.729        |
|    value_loss           | 0.849        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 263     |
|    iterations      | 1120    |
|    time_elapsed    | 17424   |
|    total_timesteps | 4587520 |
--------------------------------
Eval num_timesteps=4590000, episode_reward=-272.37 +/- 210.47
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -272         |
| time/                   |              |
|    total_timesteps      | 4590000      |
| train/                  |              |
|    approx_kl            | 0.0049721845 |
|    clip_fraction        | 0.0225       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.38         |
|    n_updates            | 11200        |
|    policy_gradient_loss | -0.000558    |
|    std                  | 0.731        |
|    value_loss           | 3.55         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 263     |
|    iterations      | 1121    |
|    time_elapsed    | 17434   |
|    total_timesteps | 4591616 |
--------------------------------
Eval num_timesteps=4595000, episode_reward=-290.53 +/- 67.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -291         |
| time/                   |              |
|    total_timesteps      | 4595000      |
| train/                  |              |
|    approx_kl            | 0.0053842845 |
|    clip_fraction        | 0.0519       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 6.67         |
|    n_updates            | 11210        |
|    policy_gradient_loss | -0.00192     |
|    std                  | 0.73         |
|    value_loss           | 4.29         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 263     |
|    iterations      | 1122    |
|    time_elapsed    | 17444   |
|    total_timesteps | 4595712 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 263          |
|    iterations           | 1123         |
|    time_elapsed         | 17451        |
|    total_timesteps      | 4599808      |
| train/                  |              |
|    approx_kl            | 0.0065880166 |
|    clip_fraction        | 0.0442       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0295       |
|    n_updates            | 11220        |
|    policy_gradient_loss | -0.00129     |
|    std                  | 0.727        |
|    value_loss           | 5.8          |
------------------------------------------
Eval num_timesteps=4600000, episode_reward=-374.65 +/- 147.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -375         |
| time/                   |              |
|    total_timesteps      | 4600000      |
| train/                  |              |
|    approx_kl            | 0.0068259053 |
|    clip_fraction        | 0.0587       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0119       |
|    n_updates            | 11230        |
|    policy_gradient_loss | -0.0031      |
|    std                  | 0.725        |
|    value_loss           | 0.0179       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 263     |
|    iterations      | 1124    |
|    time_elapsed    | 17461   |
|    total_timesteps | 4603904 |
--------------------------------
Eval num_timesteps=4605000, episode_reward=-370.24 +/- 134.68
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -370        |
| time/                   |             |
|    total_timesteps      | 4605000     |
| train/                  |             |
|    approx_kl            | 0.005356814 |
|    clip_fraction        | 0.0426      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0142     |
|    n_updates            | 11240       |
|    policy_gradient_loss | -0.00233    |
|    std                  | 0.725       |
|    value_loss           | 0.0744      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 263     |
|    iterations      | 1125    |
|    time_elapsed    | 17471   |
|    total_timesteps | 4608000 |
--------------------------------
Eval num_timesteps=4610000, episode_reward=-418.74 +/- 234.08
Episode length: 812.80 +/- 376.40
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 813          |
|    mean_reward          | -419         |
| time/                   |              |
|    total_timesteps      | 4610000      |
| train/                  |              |
|    approx_kl            | 0.0054196985 |
|    clip_fraction        | 0.0484       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.26        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0282       |
|    n_updates            | 11250        |
|    policy_gradient_loss | -0.00234     |
|    std                  | 0.724        |
|    value_loss           | 3.03         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 263     |
|    iterations      | 1126    |
|    time_elapsed    | 17481   |
|    total_timesteps | 4612096 |
--------------------------------
Eval num_timesteps=4615000, episode_reward=-407.07 +/- 186.59
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -407        |
| time/                   |             |
|    total_timesteps      | 4615000     |
| train/                  |             |
|    approx_kl            | 0.005836445 |
|    clip_fraction        | 0.0573      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.972       |
|    n_updates            | 11260       |
|    policy_gradient_loss | -0.00391    |
|    std                  | 0.722       |
|    value_loss           | 1.79        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 263     |
|    iterations      | 1127    |
|    time_elapsed    | 17492   |
|    total_timesteps | 4616192 |
--------------------------------
Eval num_timesteps=4620000, episode_reward=-306.59 +/- 216.05
Episode length: 817.60 +/- 366.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 818         |
|    mean_reward          | -307        |
| time/                   |             |
|    total_timesteps      | 4620000     |
| train/                  |             |
|    approx_kl            | 0.006839294 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0102      |
|    n_updates            | 11270       |
|    policy_gradient_loss | -0.00261    |
|    std                  | 0.728       |
|    value_loss           | 2.03        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 263     |
|    iterations      | 1128    |
|    time_elapsed    | 17501   |
|    total_timesteps | 4620288 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 264          |
|    iterations           | 1129         |
|    time_elapsed         | 17508        |
|    total_timesteps      | 4624384      |
| train/                  |              |
|    approx_kl            | 0.0046433453 |
|    clip_fraction        | 0.0339       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.62         |
|    n_updates            | 11280        |
|    policy_gradient_loss | 0.000171     |
|    std                  | 0.727        |
|    value_loss           | 10.8         |
------------------------------------------
Eval num_timesteps=4625000, episode_reward=-149.78 +/- 37.11
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -150         |
| time/                   |              |
|    total_timesteps      | 4625000      |
| train/                  |              |
|    approx_kl            | 0.0065386416 |
|    clip_fraction        | 0.0593       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0978       |
|    n_updates            | 11290        |
|    policy_gradient_loss | -0.00269     |
|    std                  | 0.728        |
|    value_loss           | 0.192        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 264     |
|    iterations      | 1130    |
|    time_elapsed    | 17519   |
|    total_timesteps | 4628480 |
--------------------------------
Eval num_timesteps=4630000, episode_reward=-290.32 +/- 98.80
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -290        |
| time/                   |             |
|    total_timesteps      | 4630000     |
| train/                  |             |
|    approx_kl            | 0.004099733 |
|    clip_fraction        | 0.0407      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0476      |
|    n_updates            | 11300       |
|    policy_gradient_loss | -0.000789   |
|    std                  | 0.731       |
|    value_loss           | 0.384       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 264     |
|    iterations      | 1131    |
|    time_elapsed    | 17529   |
|    total_timesteps | 4632576 |
--------------------------------
Eval num_timesteps=4635000, episode_reward=-379.20 +/- 87.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -379         |
| time/                   |              |
|    total_timesteps      | 4635000      |
| train/                  |              |
|    approx_kl            | 0.0056759203 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.3         |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0105       |
|    n_updates            | 11310        |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.731        |
|    value_loss           | 0.0615       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 264     |
|    iterations      | 1132    |
|    time_elapsed    | 17539   |
|    total_timesteps | 4636672 |
--------------------------------
Eval num_timesteps=4640000, episode_reward=-218.50 +/- 169.38
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -219         |
| time/                   |              |
|    total_timesteps      | 4640000      |
| train/                  |              |
|    approx_kl            | 0.0047374833 |
|    clip_fraction        | 0.0274       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.71         |
|    n_updates            | 11320        |
|    policy_gradient_loss | -0.00222     |
|    std                  | 0.73         |
|    value_loss           | 5.21         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 264     |
|    iterations      | 1133    |
|    time_elapsed    | 17551   |
|    total_timesteps | 4640768 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 264          |
|    iterations           | 1134         |
|    time_elapsed         | 17557        |
|    total_timesteps      | 4644864      |
| train/                  |              |
|    approx_kl            | 0.0056510023 |
|    clip_fraction        | 0.0498       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0175      |
|    n_updates            | 11330        |
|    policy_gradient_loss | -0.00212     |
|    std                  | 0.728        |
|    value_loss           | 0.054        |
------------------------------------------
Eval num_timesteps=4645000, episode_reward=-278.60 +/- 140.48
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -279         |
| time/                   |              |
|    total_timesteps      | 4645000      |
| train/                  |              |
|    approx_kl            | 0.0060618245 |
|    clip_fraction        | 0.0326       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0345       |
|    n_updates            | 11340        |
|    policy_gradient_loss | -0.00081     |
|    std                  | 0.725        |
|    value_loss           | 0.0392       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 264     |
|    iterations      | 1135    |
|    time_elapsed    | 17567   |
|    total_timesteps | 4648960 |
--------------------------------
Eval num_timesteps=4650000, episode_reward=-302.88 +/- 186.21
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -303        |
| time/                   |             |
|    total_timesteps      | 4650000     |
| train/                  |             |
|    approx_kl            | 0.005652461 |
|    clip_fraction        | 0.0606      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0617      |
|    n_updates            | 11350       |
|    policy_gradient_loss | -0.00284    |
|    std                  | 0.725       |
|    value_loss           | 0.0604      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 264     |
|    iterations      | 1136    |
|    time_elapsed    | 17578   |
|    total_timesteps | 4653056 |
--------------------------------
Eval num_timesteps=4655000, episode_reward=-209.18 +/- 50.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -209         |
| time/                   |              |
|    total_timesteps      | 4655000      |
| train/                  |              |
|    approx_kl            | 0.0044326354 |
|    clip_fraction        | 0.02         |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.21         |
|    n_updates            | 11360        |
|    policy_gradient_loss | -0.000763    |
|    std                  | 0.727        |
|    value_loss           | 9.11         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 264     |
|    iterations      | 1137    |
|    time_elapsed    | 17589   |
|    total_timesteps | 4657152 |
--------------------------------
Eval num_timesteps=4660000, episode_reward=-245.91 +/- 191.35
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -246         |
| time/                   |              |
|    total_timesteps      | 4660000      |
| train/                  |              |
|    approx_kl            | 0.0049074274 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00124      |
|    n_updates            | 11370        |
|    policy_gradient_loss | -0.00228     |
|    std                  | 0.729        |
|    value_loss           | 0.0571       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 264     |
|    iterations      | 1138    |
|    time_elapsed    | 17600   |
|    total_timesteps | 4661248 |
--------------------------------
Eval num_timesteps=4665000, episode_reward=-289.25 +/- 131.20
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -289        |
| time/                   |             |
|    total_timesteps      | 4665000     |
| train/                  |             |
|    approx_kl            | 0.004407416 |
|    clip_fraction        | 0.0443      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0126      |
|    n_updates            | 11380       |
|    policy_gradient_loss | -0.00256    |
|    std                  | 0.729       |
|    value_loss           | 0.846       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 264     |
|    iterations      | 1139    |
|    time_elapsed    | 17610   |
|    total_timesteps | 4665344 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 265          |
|    iterations           | 1140         |
|    time_elapsed         | 17617        |
|    total_timesteps      | 4669440      |
| train/                  |              |
|    approx_kl            | 0.0039500743 |
|    clip_fraction        | 0.0209       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0276       |
|    n_updates            | 11390        |
|    policy_gradient_loss | -0.000773    |
|    std                  | 0.729        |
|    value_loss           | 4.95         |
------------------------------------------
Eval num_timesteps=4670000, episode_reward=-253.12 +/- 156.36
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -253        |
| time/                   |             |
|    total_timesteps      | 4670000     |
| train/                  |             |
|    approx_kl            | 0.005408151 |
|    clip_fraction        | 0.0454      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0167     |
|    n_updates            | 11400       |
|    policy_gradient_loss | -0.00323    |
|    std                  | 0.717       |
|    value_loss           | 0.0254      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1141    |
|    time_elapsed    | 17628   |
|    total_timesteps | 4673536 |
--------------------------------
Eval num_timesteps=4675000, episode_reward=-319.10 +/- 117.66
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -319        |
| time/                   |             |
|    total_timesteps      | 4675000     |
| train/                  |             |
|    approx_kl            | 0.004539543 |
|    clip_fraction        | 0.0341      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 7.37        |
|    n_updates            | 11410       |
|    policy_gradient_loss | -0.00155    |
|    std                  | 0.716       |
|    value_loss           | 9.15        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1142    |
|    time_elapsed    | 17638   |
|    total_timesteps | 4677632 |
--------------------------------
Eval num_timesteps=4680000, episode_reward=-380.96 +/- 256.92
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -381         |
| time/                   |              |
|    total_timesteps      | 4680000      |
| train/                  |              |
|    approx_kl            | 0.0052923076 |
|    clip_fraction        | 0.0376       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.24        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0523       |
|    n_updates            | 11420        |
|    policy_gradient_loss | -0.000589    |
|    std                  | 0.719        |
|    value_loss           | 4.59         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1143    |
|    time_elapsed    | 17648   |
|    total_timesteps | 4681728 |
--------------------------------
Eval num_timesteps=4685000, episode_reward=-220.41 +/- 116.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -220         |
| time/                   |              |
|    total_timesteps      | 4685000      |
| train/                  |              |
|    approx_kl            | 0.0045677973 |
|    clip_fraction        | 0.0463       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.993        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0303       |
|    n_updates            | 11430        |
|    policy_gradient_loss | -0.00118     |
|    std                  | 0.717        |
|    value_loss           | 2.61         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1144    |
|    time_elapsed    | 17659   |
|    total_timesteps | 4685824 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 265         |
|    iterations           | 1145        |
|    time_elapsed         | 17666       |
|    total_timesteps      | 4689920     |
| train/                  |             |
|    approx_kl            | 0.008773217 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0116      |
|    n_updates            | 11440       |
|    policy_gradient_loss | -0.00213    |
|    std                  | 0.722       |
|    value_loss           | 3.46        |
-----------------------------------------
Eval num_timesteps=4690000, episode_reward=-344.51 +/- 199.62
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -345        |
| time/                   |             |
|    total_timesteps      | 4690000     |
| train/                  |             |
|    approx_kl            | 0.006472934 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0181      |
|    n_updates            | 11450       |
|    policy_gradient_loss | -0.00313    |
|    std                  | 0.726       |
|    value_loss           | 0.0163      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1146    |
|    time_elapsed    | 17677   |
|    total_timesteps | 4694016 |
--------------------------------
Eval num_timesteps=4695000, episode_reward=-312.61 +/- 211.46
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -313        |
| time/                   |             |
|    total_timesteps      | 4695000     |
| train/                  |             |
|    approx_kl            | 0.005925537 |
|    clip_fraction        | 0.0382      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.11        |
|    n_updates            | 11460       |
|    policy_gradient_loss | -0.00123    |
|    std                  | 0.727       |
|    value_loss           | 0.917       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1147    |
|    time_elapsed    | 17689   |
|    total_timesteps | 4698112 |
--------------------------------
Eval num_timesteps=4700000, episode_reward=-367.61 +/- 162.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -368         |
| time/                   |              |
|    total_timesteps      | 4700000      |
| train/                  |              |
|    approx_kl            | 0.0044382336 |
|    clip_fraction        | 0.0179       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0965       |
|    n_updates            | 11470        |
|    policy_gradient_loss | -0.000389    |
|    std                  | 0.727        |
|    value_loss           | 5.26         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1148    |
|    time_elapsed    | 17700   |
|    total_timesteps | 4702208 |
--------------------------------
Eval num_timesteps=4705000, episode_reward=-230.96 +/- 149.59
Episode length: 809.00 +/- 384.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 809         |
|    mean_reward          | -231        |
| time/                   |             |
|    total_timesteps      | 4705000     |
| train/                  |             |
|    approx_kl            | 0.004712101 |
|    clip_fraction        | 0.0466      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0119      |
|    n_updates            | 11480       |
|    policy_gradient_loss | -0.00271    |
|    std                  | 0.726       |
|    value_loss           | 4.71        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1149    |
|    time_elapsed    | 17712   |
|    total_timesteps | 4706304 |
--------------------------------
Eval num_timesteps=4710000, episode_reward=-351.41 +/- 146.53
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -351         |
| time/                   |              |
|    total_timesteps      | 4710000      |
| train/                  |              |
|    approx_kl            | 0.0054597016 |
|    clip_fraction        | 0.0384       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0664       |
|    n_updates            | 11490        |
|    policy_gradient_loss | -0.000638    |
|    std                  | 0.728        |
|    value_loss           | 0.0432       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1150    |
|    time_elapsed    | 17727   |
|    total_timesteps | 4710400 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 265          |
|    iterations           | 1151         |
|    time_elapsed         | 17736        |
|    total_timesteps      | 4714496      |
| train/                  |              |
|    approx_kl            | 0.0069791097 |
|    clip_fraction        | 0.0408       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.3         |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.418        |
|    n_updates            | 11500        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 0.731        |
|    value_loss           | 0.155        |
------------------------------------------
Eval num_timesteps=4715000, episode_reward=-377.28 +/- 194.93
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -377        |
| time/                   |             |
|    total_timesteps      | 4715000     |
| train/                  |             |
|    approx_kl            | 0.005476754 |
|    clip_fraction        | 0.0419      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.022       |
|    n_updates            | 11510       |
|    policy_gradient_loss | -0.00157    |
|    std                  | 0.729       |
|    value_loss           | 0.07        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1152    |
|    time_elapsed    | 17748   |
|    total_timesteps | 4718592 |
--------------------------------
Eval num_timesteps=4720000, episode_reward=-367.38 +/- 198.84
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -367         |
| time/                   |              |
|    total_timesteps      | 4720000      |
| train/                  |              |
|    approx_kl            | 0.0049747746 |
|    clip_fraction        | 0.0387       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.3         |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00527     |
|    n_updates            | 11520        |
|    policy_gradient_loss | -0.0024      |
|    std                  | 0.73         |
|    value_loss           | 0.3          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1153    |
|    time_elapsed    | 17762   |
|    total_timesteps | 4722688 |
--------------------------------
Eval num_timesteps=4725000, episode_reward=-246.25 +/- 128.85
Episode length: 808.60 +/- 384.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 809         |
|    mean_reward          | -246        |
| time/                   |             |
|    total_timesteps      | 4725000     |
| train/                  |             |
|    approx_kl            | 0.004473868 |
|    clip_fraction        | 0.0227      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00561    |
|    n_updates            | 11530       |
|    policy_gradient_loss | -0.000339   |
|    std                  | 0.728       |
|    value_loss           | 0.771       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1154    |
|    time_elapsed    | 17774   |
|    total_timesteps | 4726784 |
--------------------------------
Eval num_timesteps=4730000, episode_reward=-206.97 +/- 49.90
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -207        |
| time/                   |             |
|    total_timesteps      | 4730000     |
| train/                  |             |
|    approx_kl            | 0.005367139 |
|    clip_fraction        | 0.0461      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.3        |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.53        |
|    n_updates            | 11540       |
|    policy_gradient_loss | -0.0022     |
|    std                  | 0.728       |
|    value_loss           | 4.31        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 265     |
|    iterations      | 1155    |
|    time_elapsed    | 17785   |
|    total_timesteps | 4730880 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 266          |
|    iterations           | 1156         |
|    time_elapsed         | 17793        |
|    total_timesteps      | 4734976      |
| train/                  |              |
|    approx_kl            | 0.0029213824 |
|    clip_fraction        | 0.0329       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.488        |
|    n_updates            | 11550        |
|    policy_gradient_loss | -0.000929    |
|    std                  | 0.725        |
|    value_loss           | 13.7         |
------------------------------------------
Eval num_timesteps=4735000, episode_reward=-210.81 +/- 92.39
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -211        |
| time/                   |             |
|    total_timesteps      | 4735000     |
| train/                  |             |
|    approx_kl            | 0.007263081 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0298     |
|    n_updates            | 11560       |
|    policy_gradient_loss | -0.00688    |
|    std                  | 0.73        |
|    value_loss           | 0.0218      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1157    |
|    time_elapsed    | 17804   |
|    total_timesteps | 4739072 |
--------------------------------
Eval num_timesteps=4740000, episode_reward=-458.27 +/- 104.97
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -458        |
| time/                   |             |
|    total_timesteps      | 4740000     |
| train/                  |             |
|    approx_kl            | 0.005066244 |
|    clip_fraction        | 0.0359      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.119       |
|    n_updates            | 11570       |
|    policy_gradient_loss | -0.00157    |
|    std                  | 0.725       |
|    value_loss           | 0.0801      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1158    |
|    time_elapsed    | 17814   |
|    total_timesteps | 4743168 |
--------------------------------
Eval num_timesteps=4745000, episode_reward=-286.56 +/- 160.22
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -287        |
| time/                   |             |
|    total_timesteps      | 4745000     |
| train/                  |             |
|    approx_kl            | 0.005047341 |
|    clip_fraction        | 0.0356      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.97        |
|    learning_rate        | 0.0001      |
|    loss                 | 27          |
|    n_updates            | 11580       |
|    policy_gradient_loss | -0.00126    |
|    std                  | 0.724       |
|    value_loss           | 26.1        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1159    |
|    time_elapsed    | 17825   |
|    total_timesteps | 4747264 |
--------------------------------
Eval num_timesteps=4750000, episode_reward=-244.75 +/- 144.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -245         |
| time/                   |              |
|    total_timesteps      | 4750000      |
| train/                  |              |
|    approx_kl            | 0.0038525246 |
|    clip_fraction        | 0.0467       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.85         |
|    n_updates            | 11590        |
|    policy_gradient_loss | -0.00372     |
|    std                  | 0.724        |
|    value_loss           | 0.71         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1160    |
|    time_elapsed    | 17835   |
|    total_timesteps | 4751360 |
--------------------------------
Eval num_timesteps=4755000, episode_reward=-352.99 +/- 105.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -353         |
| time/                   |              |
|    total_timesteps      | 4755000      |
| train/                  |              |
|    approx_kl            | 0.0040494623 |
|    clip_fraction        | 0.0396       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0137       |
|    n_updates            | 11600        |
|    policy_gradient_loss | -0.000969    |
|    std                  | 0.726        |
|    value_loss           | 0.0335       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1161    |
|    time_elapsed    | 17846   |
|    total_timesteps | 4755456 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 266         |
|    iterations           | 1162        |
|    time_elapsed         | 17852       |
|    total_timesteps      | 4759552     |
| train/                  |             |
|    approx_kl            | 0.006484513 |
|    clip_fraction        | 0.0539      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0794      |
|    n_updates            | 11610       |
|    policy_gradient_loss | -0.00137    |
|    std                  | 0.724       |
|    value_loss           | 1.42        |
-----------------------------------------
Eval num_timesteps=4760000, episode_reward=-460.71 +/- 188.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -461        |
| time/                   |             |
|    total_timesteps      | 4760000     |
| train/                  |             |
|    approx_kl            | 0.004808292 |
|    clip_fraction        | 0.0417      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0191      |
|    n_updates            | 11620       |
|    policy_gradient_loss | -0.00171    |
|    std                  | 0.726       |
|    value_loss           | 0.0173      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1163    |
|    time_elapsed    | 17862   |
|    total_timesteps | 4763648 |
--------------------------------
Eval num_timesteps=4765000, episode_reward=-300.92 +/- 79.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -301        |
| time/                   |             |
|    total_timesteps      | 4765000     |
| train/                  |             |
|    approx_kl            | 0.004399156 |
|    clip_fraction        | 0.0286      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0387      |
|    n_updates            | 11630       |
|    policy_gradient_loss | -0.000871   |
|    std                  | 0.727       |
|    value_loss           | 2.52        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1164    |
|    time_elapsed    | 17873   |
|    total_timesteps | 4767744 |
--------------------------------
Eval num_timesteps=4770000, episode_reward=-332.63 +/- 159.31
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -333        |
| time/                   |             |
|    total_timesteps      | 4770000     |
| train/                  |             |
|    approx_kl            | 0.005706518 |
|    clip_fraction        | 0.0389      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0623      |
|    n_updates            | 11640       |
|    policy_gradient_loss | -0.00129    |
|    std                  | 0.728       |
|    value_loss           | 8.92        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1165    |
|    time_elapsed    | 17883   |
|    total_timesteps | 4771840 |
--------------------------------
Eval num_timesteps=4775000, episode_reward=-248.09 +/- 90.05
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -248         |
| time/                   |              |
|    total_timesteps      | 4775000      |
| train/                  |              |
|    approx_kl            | 0.0064555733 |
|    clip_fraction        | 0.0664       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.589        |
|    n_updates            | 11650        |
|    policy_gradient_loss | -0.00158     |
|    std                  | 0.73         |
|    value_loss           | 6.47         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1166    |
|    time_elapsed    | 17894   |
|    total_timesteps | 4775936 |
--------------------------------
Eval num_timesteps=4780000, episode_reward=-263.29 +/- 110.62
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -263        |
| time/                   |             |
|    total_timesteps      | 4780000     |
| train/                  |             |
|    approx_kl            | 0.003857281 |
|    clip_fraction        | 0.0349      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.29       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0294      |
|    n_updates            | 11660       |
|    policy_gradient_loss | -0.00186    |
|    std                  | 0.726       |
|    value_loss           | 1.13        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 266     |
|    iterations      | 1167    |
|    time_elapsed    | 17905   |
|    total_timesteps | 4780032 |
--------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 267        |
|    iterations           | 1168       |
|    time_elapsed         | 17913      |
|    total_timesteps      | 4784128    |
| train/                  |            |
|    approx_kl            | 0.00668092 |
|    clip_fraction        | 0.0517     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.27      |
|    explained_variance   | 0.987      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.98       |
|    n_updates            | 11670      |
|    policy_gradient_loss | -0.00162   |
|    std                  | 0.72       |
|    value_loss           | 0.726      |
----------------------------------------
Eval num_timesteps=4785000, episode_reward=-523.12 +/- 137.38
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -523        |
| time/                   |             |
|    total_timesteps      | 4785000     |
| train/                  |             |
|    approx_kl            | 0.006908752 |
|    clip_fraction        | 0.0638      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 1           |
|    learning_rate        | 0.0001      |
|    loss                 | 0.00535     |
|    n_updates            | 11680       |
|    policy_gradient_loss | -0.00205    |
|    std                  | 0.72        |
|    value_loss           | 0.0179      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1169    |
|    time_elapsed    | 17926   |
|    total_timesteps | 4788224 |
--------------------------------
Eval num_timesteps=4790000, episode_reward=-293.93 +/- 71.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -294        |
| time/                   |             |
|    total_timesteps      | 4790000     |
| train/                  |             |
|    approx_kl            | 0.004715001 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00457    |
|    n_updates            | 11690       |
|    policy_gradient_loss | -0.00289    |
|    std                  | 0.721       |
|    value_loss           | 0.267       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1170    |
|    time_elapsed    | 17937   |
|    total_timesteps | 4792320 |
--------------------------------
Eval num_timesteps=4795000, episode_reward=-214.37 +/- 196.11
Episode length: 620.40 +/- 466.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 620         |
|    mean_reward          | -214        |
| time/                   |             |
|    total_timesteps      | 4795000     |
| train/                  |             |
|    approx_kl            | 0.008387478 |
|    clip_fraction        | 0.0583      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.92        |
|    n_updates            | 11700       |
|    policy_gradient_loss | -0.00181    |
|    std                  | 0.721       |
|    value_loss           | 4.65        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1171    |
|    time_elapsed    | 17946   |
|    total_timesteps | 4796416 |
--------------------------------
Eval num_timesteps=4800000, episode_reward=-271.13 +/- 103.30
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -271        |
| time/                   |             |
|    total_timesteps      | 4800000     |
| train/                  |             |
|    approx_kl            | 0.004281988 |
|    clip_fraction        | 0.0378      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0382      |
|    n_updates            | 11710       |
|    policy_gradient_loss | -0.00171    |
|    std                  | 0.719       |
|    value_loss           | 0.868       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1172    |
|    time_elapsed    | 17956   |
|    total_timesteps | 4800512 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 267          |
|    iterations           | 1173         |
|    time_elapsed         | 17963        |
|    total_timesteps      | 4804608      |
| train/                  |              |
|    approx_kl            | 0.0032464904 |
|    clip_fraction        | 0.0365       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.26        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0207       |
|    n_updates            | 11720        |
|    policy_gradient_loss | -0.00175     |
|    std                  | 0.72         |
|    value_loss           | 4.3          |
------------------------------------------
Eval num_timesteps=4805000, episode_reward=-281.56 +/- 100.57
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -282         |
| time/                   |              |
|    total_timesteps      | 4805000      |
| train/                  |              |
|    approx_kl            | 0.0060538496 |
|    clip_fraction        | 0.0683       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0126      |
|    n_updates            | 11730        |
|    policy_gradient_loss | -0.00315     |
|    std                  | 0.724        |
|    value_loss           | 0.107        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1174    |
|    time_elapsed    | 17974   |
|    total_timesteps | 4808704 |
--------------------------------
Eval num_timesteps=4810000, episode_reward=-305.24 +/- 167.31
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -305        |
| time/                   |             |
|    total_timesteps      | 4810000     |
| train/                  |             |
|    approx_kl            | 0.004845107 |
|    clip_fraction        | 0.0393      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.398       |
|    n_updates            | 11740       |
|    policy_gradient_loss | -0.0014     |
|    std                  | 0.72        |
|    value_loss           | 0.45        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1175    |
|    time_elapsed    | 17985   |
|    total_timesteps | 4812800 |
--------------------------------
Eval num_timesteps=4815000, episode_reward=-263.71 +/- 142.51
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -264        |
| time/                   |             |
|    total_timesteps      | 4815000     |
| train/                  |             |
|    approx_kl            | 0.006156668 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0986      |
|    n_updates            | 11750       |
|    policy_gradient_loss | -0.00262    |
|    std                  | 0.723       |
|    value_loss           | 2.36        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1176    |
|    time_elapsed    | 17996   |
|    total_timesteps | 4816896 |
--------------------------------
Eval num_timesteps=4820000, episode_reward=-349.06 +/- 160.62
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -349         |
| time/                   |              |
|    total_timesteps      | 4820000      |
| train/                  |              |
|    approx_kl            | 0.0047389986 |
|    clip_fraction        | 0.0294       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.207        |
|    n_updates            | 11760        |
|    policy_gradient_loss | -0.000277    |
|    std                  | 0.723        |
|    value_loss           | 1.37         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1177    |
|    time_elapsed    | 18007   |
|    total_timesteps | 4820992 |
--------------------------------
Eval num_timesteps=4825000, episode_reward=-244.21 +/- 203.43
Episode length: 622.40 +/- 463.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 622         |
|    mean_reward          | -244        |
| time/                   |             |
|    total_timesteps      | 4825000     |
| train/                  |             |
|    approx_kl            | 0.005926761 |
|    clip_fraction        | 0.0404      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0539      |
|    n_updates            | 11770       |
|    policy_gradient_loss | -0.00139    |
|    std                  | 0.721       |
|    value_loss           | 0.192       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1178    |
|    time_elapsed    | 18017   |
|    total_timesteps | 4825088 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 267         |
|    iterations           | 1179        |
|    time_elapsed         | 18024       |
|    total_timesteps      | 4829184     |
| train/                  |             |
|    approx_kl            | 0.006297541 |
|    clip_fraction        | 0.0628      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0466      |
|    n_updates            | 11780       |
|    policy_gradient_loss | -0.00384    |
|    std                  | 0.719       |
|    value_loss           | 1.24        |
-----------------------------------------
Eval num_timesteps=4830000, episode_reward=-392.97 +/- 140.70
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -393        |
| time/                   |             |
|    total_timesteps      | 4830000     |
| train/                  |             |
|    approx_kl            | 0.006247228 |
|    clip_fraction        | 0.044       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0432      |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.000192   |
|    std                  | 0.719       |
|    value_loss           | 0.0559      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1180    |
|    time_elapsed    | 18037   |
|    total_timesteps | 4833280 |
--------------------------------
Eval num_timesteps=4835000, episode_reward=-400.54 +/- 141.92
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -401       |
| time/                   |            |
|    total_timesteps      | 4835000    |
| train/                  |            |
|    approx_kl            | 0.00435784 |
|    clip_fraction        | 0.0374     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.25      |
|    explained_variance   | 0.977      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.072      |
|    n_updates            | 11800      |
|    policy_gradient_loss | -0.00101   |
|    std                  | 0.72       |
|    value_loss           | 8.65       |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 267     |
|    iterations      | 1181    |
|    time_elapsed    | 18050   |
|    total_timesteps | 4837376 |
--------------------------------
Eval num_timesteps=4840000, episode_reward=-371.83 +/- 181.36
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -372         |
| time/                   |              |
|    total_timesteps      | 4840000      |
| train/                  |              |
|    approx_kl            | 0.0045626806 |
|    clip_fraction        | 0.0363       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0621       |
|    n_updates            | 11810        |
|    policy_gradient_loss | -0.00176     |
|    std                  | 0.716        |
|    value_loss           | 0.96         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1182    |
|    time_elapsed    | 18061   |
|    total_timesteps | 4841472 |
--------------------------------
Eval num_timesteps=4845000, episode_reward=-256.39 +/- 91.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -256         |
| time/                   |              |
|    total_timesteps      | 4845000      |
| train/                  |              |
|    approx_kl            | 0.0054690856 |
|    clip_fraction        | 0.0437       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.24        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.21         |
|    n_updates            | 11820        |
|    policy_gradient_loss | -0.000438    |
|    std                  | 0.718        |
|    value_loss           | 1.96         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1183    |
|    time_elapsed    | 18072   |
|    total_timesteps | 4845568 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 268          |
|    iterations           | 1184         |
|    time_elapsed         | 18079        |
|    total_timesteps      | 4849664      |
| train/                  |              |
|    approx_kl            | 0.0039065387 |
|    clip_fraction        | 0.0372       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.24        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00348      |
|    n_updates            | 11830        |
|    policy_gradient_loss | 0.000239     |
|    std                  | 0.715        |
|    value_loss           | 0.0637       |
------------------------------------------
Eval num_timesteps=4850000, episode_reward=-297.62 +/- 59.35
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -298         |
| time/                   |              |
|    total_timesteps      | 4850000      |
| train/                  |              |
|    approx_kl            | 0.0037548174 |
|    clip_fraction        | 0.0378       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.24        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | -0.00754     |
|    n_updates            | 11840        |
|    policy_gradient_loss | -0.00151     |
|    std                  | 0.719        |
|    value_loss           | 0.0156       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1185    |
|    time_elapsed    | 18091   |
|    total_timesteps | 4853760 |
--------------------------------
Eval num_timesteps=4855000, episode_reward=-162.79 +/- 56.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -163         |
| time/                   |              |
|    total_timesteps      | 4855000      |
| train/                  |              |
|    approx_kl            | 0.0043958174 |
|    clip_fraction        | 0.0422       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.987        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0786       |
|    n_updates            | 11850        |
|    policy_gradient_loss | -0.00336     |
|    std                  | 0.72         |
|    value_loss           | 2.66         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1186    |
|    time_elapsed    | 18103   |
|    total_timesteps | 4857856 |
--------------------------------
Eval num_timesteps=4860000, episode_reward=-369.96 +/- 216.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -370         |
| time/                   |              |
|    total_timesteps      | 4860000      |
| train/                  |              |
|    approx_kl            | 0.0038352243 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.26        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.596        |
|    n_updates            | 11860        |
|    policy_gradient_loss | -0.00274     |
|    std                  | 0.722        |
|    value_loss           | 1.78         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1187    |
|    time_elapsed    | 18114   |
|    total_timesteps | 4861952 |
--------------------------------
Eval num_timesteps=4865000, episode_reward=-318.61 +/- 152.61
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -319        |
| time/                   |             |
|    total_timesteps      | 4865000     |
| train/                  |             |
|    approx_kl            | 0.004752242 |
|    clip_fraction        | 0.0322      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0193      |
|    n_updates            | 11870       |
|    policy_gradient_loss | -0.000203   |
|    std                  | 0.721       |
|    value_loss           | 1.46        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1188    |
|    time_elapsed    | 18125   |
|    total_timesteps | 4866048 |
--------------------------------
Eval num_timesteps=4870000, episode_reward=-324.41 +/- 139.88
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -324        |
| time/                   |             |
|    total_timesteps      | 4870000     |
| train/                  |             |
|    approx_kl            | 0.005942896 |
|    clip_fraction        | 0.0612      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0231      |
|    n_updates            | 11880       |
|    policy_gradient_loss | -0.00269    |
|    std                  | 0.717       |
|    value_loss           | 0.907       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1189    |
|    time_elapsed    | 18136   |
|    total_timesteps | 4870144 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 268         |
|    iterations           | 1190        |
|    time_elapsed         | 18143       |
|    total_timesteps      | 4874240     |
| train/                  |             |
|    approx_kl            | 0.006881138 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.24       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0612      |
|    n_updates            | 11890       |
|    policy_gradient_loss | -0.00498    |
|    std                  | 0.715       |
|    value_loss           | 4.19        |
-----------------------------------------
Eval num_timesteps=4875000, episode_reward=-279.69 +/- 192.47
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -280        |
| time/                   |             |
|    total_timesteps      | 4875000     |
| train/                  |             |
|    approx_kl            | 0.004865678 |
|    clip_fraction        | 0.041       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0243      |
|    n_updates            | 11900       |
|    policy_gradient_loss | -0.00114    |
|    std                  | 0.721       |
|    value_loss           | 0.0204      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1191    |
|    time_elapsed    | 18154   |
|    total_timesteps | 4878336 |
--------------------------------
Eval num_timesteps=4880000, episode_reward=-147.12 +/- 122.45
Episode length: 622.40 +/- 463.69
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 622          |
|    mean_reward          | -147         |
| time/                   |              |
|    total_timesteps      | 4880000      |
| train/                  |              |
|    approx_kl            | 0.0057144426 |
|    clip_fraction        | 0.0499       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.26        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00797      |
|    n_updates            | 11910        |
|    policy_gradient_loss | -0.0024      |
|    std                  | 0.723        |
|    value_loss           | 0.512        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1192    |
|    time_elapsed    | 18163   |
|    total_timesteps | 4882432 |
--------------------------------
Eval num_timesteps=4885000, episode_reward=-243.53 +/- 99.99
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -244         |
| time/                   |              |
|    total_timesteps      | 4885000      |
| train/                  |              |
|    approx_kl            | 0.0035257707 |
|    clip_fraction        | 0.0426       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0111      |
|    n_updates            | 11920        |
|    policy_gradient_loss | -0.00172     |
|    std                  | 0.725        |
|    value_loss           | 0.344        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1193    |
|    time_elapsed    | 18174   |
|    total_timesteps | 4886528 |
--------------------------------
Eval num_timesteps=4890000, episode_reward=-298.81 +/- 88.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -299         |
| time/                   |              |
|    total_timesteps      | 4890000      |
| train/                  |              |
|    approx_kl            | 0.0060105827 |
|    clip_fraction        | 0.0367       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0302       |
|    n_updates            | 11930        |
|    policy_gradient_loss | -0.00195     |
|    std                  | 0.728        |
|    value_loss           | 9.33         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 268     |
|    iterations      | 1194    |
|    time_elapsed    | 18185   |
|    total_timesteps | 4890624 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 269          |
|    iterations           | 1195         |
|    time_elapsed         | 18192        |
|    total_timesteps      | 4894720      |
| train/                  |              |
|    approx_kl            | 0.0049361098 |
|    clip_fraction        | 0.0388       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.29        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.72         |
|    n_updates            | 11940        |
|    policy_gradient_loss | -0.00122     |
|    std                  | 0.731        |
|    value_loss           | 4.34         |
------------------------------------------
Eval num_timesteps=4895000, episode_reward=-234.05 +/- 155.52
Episode length: 815.40 +/- 371.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -234         |
| time/                   |              |
|    total_timesteps      | 4895000      |
| train/                  |              |
|    approx_kl            | 0.0052585034 |
|    clip_fraction        | 0.0434       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0094      |
|    n_updates            | 11950        |
|    policy_gradient_loss | -0.00109     |
|    std                  | 0.725        |
|    value_loss           | 0.0297       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 269     |
|    iterations      | 1196    |
|    time_elapsed    | 18202   |
|    total_timesteps | 4898816 |
--------------------------------
Eval num_timesteps=4900000, episode_reward=-276.04 +/- 250.96
Episode length: 627.00 +/- 458.06
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 627          |
|    mean_reward          | -276         |
| time/                   |              |
|    total_timesteps      | 4900000      |
| train/                  |              |
|    approx_kl            | 0.0040127574 |
|    clip_fraction        | 0.0198       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.32         |
|    n_updates            | 11960        |
|    policy_gradient_loss | -0.000233    |
|    std                  | 0.726        |
|    value_loss           | 4.95         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 269     |
|    iterations      | 1197    |
|    time_elapsed    | 18212   |
|    total_timesteps | 4902912 |
--------------------------------
Eval num_timesteps=4905000, episode_reward=-398.69 +/- 221.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -399         |
| time/                   |              |
|    total_timesteps      | 4905000      |
| train/                  |              |
|    approx_kl            | 0.0072433758 |
|    clip_fraction        | 0.0691       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0191       |
|    n_updates            | 11970        |
|    policy_gradient_loss | -0.00346     |
|    std                  | 0.726        |
|    value_loss           | 0.92         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 269     |
|    iterations      | 1198    |
|    time_elapsed    | 18222   |
|    total_timesteps | 4907008 |
--------------------------------
Eval num_timesteps=4910000, episode_reward=-188.24 +/- 119.68
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -188         |
| time/                   |              |
|    total_timesteps      | 4910000      |
| train/                  |              |
|    approx_kl            | 0.0039273803 |
|    clip_fraction        | 0.0347       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0653       |
|    n_updates            | 11980        |
|    policy_gradient_loss | -0.00125     |
|    std                  | 0.724        |
|    value_loss           | 5.29         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 269     |
|    iterations      | 1199    |
|    time_elapsed    | 18232   |
|    total_timesteps | 4911104 |
--------------------------------
Eval num_timesteps=4915000, episode_reward=-278.63 +/- 158.55
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -279        |
| time/                   |             |
|    total_timesteps      | 4915000     |
| train/                  |             |
|    approx_kl            | 0.003987269 |
|    clip_fraction        | 0.0198      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.985       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.26        |
|    n_updates            | 11990       |
|    policy_gradient_loss | 8.62e-05    |
|    std                  | 0.724       |
|    value_loss           | 3.48        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 269     |
|    iterations      | 1200    |
|    time_elapsed    | 18243   |
|    total_timesteps | 4915200 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 1201        |
|    time_elapsed         | 18249       |
|    total_timesteps      | 4919296     |
| train/                  |             |
|    approx_kl            | 0.004521845 |
|    clip_fraction        | 0.0494      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0087      |
|    n_updates            | 12000       |
|    policy_gradient_loss | -0.000842   |
|    std                  | 0.722       |
|    value_loss           | 7.43        |
-----------------------------------------
Eval num_timesteps=4920000, episode_reward=-301.17 +/- 139.65
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -301         |
| time/                   |              |
|    total_timesteps      | 4920000      |
| train/                  |              |
|    approx_kl            | 0.0067342757 |
|    clip_fraction        | 0.056        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.28        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0534       |
|    n_updates            | 12010        |
|    policy_gradient_loss | -0.00262     |
|    std                  | 0.729        |
|    value_loss           | 0.109        |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 269     |
|    iterations      | 1202    |
|    time_elapsed    | 18260   |
|    total_timesteps | 4923392 |
--------------------------------
Eval num_timesteps=4925000, episode_reward=-296.69 +/- 169.82
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -297        |
| time/                   |             |
|    total_timesteps      | 4925000     |
| train/                  |             |
|    approx_kl            | 0.005775083 |
|    clip_fraction        | 0.0307      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.198       |
|    n_updates            | 12020       |
|    policy_gradient_loss | -0.00142    |
|    std                  | 0.725       |
|    value_loss           | 0.785       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 269     |
|    iterations      | 1203    |
|    time_elapsed    | 18270   |
|    total_timesteps | 4927488 |
--------------------------------
Eval num_timesteps=4930000, episode_reward=-326.39 +/- 191.16
Episode length: 811.40 +/- 379.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 811         |
|    mean_reward          | -326        |
| time/                   |             |
|    total_timesteps      | 4930000     |
| train/                  |             |
|    approx_kl            | 0.006529182 |
|    clip_fraction        | 0.0447      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.0134     |
|    n_updates            | 12030       |
|    policy_gradient_loss | -0.00104    |
|    std                  | 0.724       |
|    value_loss           | 2.27        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 269     |
|    iterations      | 1204    |
|    time_elapsed    | 18281   |
|    total_timesteps | 4931584 |
--------------------------------
Eval num_timesteps=4935000, episode_reward=-260.45 +/- 99.09
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -260        |
| time/                   |             |
|    total_timesteps      | 4935000     |
| train/                  |             |
|    approx_kl            | 0.003919552 |
|    clip_fraction        | 0.0431      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.984       |
|    learning_rate        | 0.0001      |
|    loss                 | 10.5        |
|    n_updates            | 12040       |
|    policy_gradient_loss | -0.000946   |
|    std                  | 0.724       |
|    value_loss           | 13.2        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 269     |
|    iterations      | 1205    |
|    time_elapsed    | 18292   |
|    total_timesteps | 4935680 |
--------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 269         |
|    iterations           | 1206        |
|    time_elapsed         | 18299       |
|    total_timesteps      | 4939776     |
| train/                  |             |
|    approx_kl            | 0.007160022 |
|    clip_fraction        | 0.0549      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 1.7         |
|    n_updates            | 12050       |
|    policy_gradient_loss | -0.00184    |
|    std                  | 0.725       |
|    value_loss           | 0.807       |
-----------------------------------------
Eval num_timesteps=4940000, episode_reward=-346.07 +/- 213.28
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -346       |
| time/                   |            |
|    total_timesteps      | 4940000    |
| train/                  |            |
|    approx_kl            | 0.00460727 |
|    clip_fraction        | 0.0409     |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.27      |
|    explained_variance   | 0.992      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.101      |
|    n_updates            | 12060      |
|    policy_gradient_loss | -0.00127   |
|    std                  | 0.726      |
|    value_loss           | 0.319      |
----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1207    |
|    time_elapsed    | 18310   |
|    total_timesteps | 4943872 |
--------------------------------
Eval num_timesteps=4945000, episode_reward=-267.14 +/- 210.05
Episode length: 813.00 +/- 376.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 813         |
|    mean_reward          | -267        |
| time/                   |             |
|    total_timesteps      | 4945000     |
| train/                  |             |
|    approx_kl            | 0.005737199 |
|    clip_fraction        | 0.0481      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.27       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0371      |
|    n_updates            | 12070       |
|    policy_gradient_loss | -0.00257    |
|    std                  | 0.723       |
|    value_loss           | 0.118       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1208    |
|    time_elapsed    | 18320   |
|    total_timesteps | 4947968 |
--------------------------------
Eval num_timesteps=4950000, episode_reward=-405.73 +/- 111.33
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -406         |
| time/                   |              |
|    total_timesteps      | 4950000      |
| train/                  |              |
|    approx_kl            | 0.0021565098 |
|    clip_fraction        | 0.0153       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.26        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.00971      |
|    n_updates            | 12080        |
|    policy_gradient_loss | -0.00073     |
|    std                  | 0.722        |
|    value_loss           | 1.06         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1209    |
|    time_elapsed    | 18331   |
|    total_timesteps | 4952064 |
--------------------------------
Eval num_timesteps=4955000, episode_reward=-95.14 +/- 87.28
Episode length: 616.40 +/- 471.15
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 616         |
|    mean_reward          | -95.1       |
| time/                   |             |
|    total_timesteps      | 4955000     |
| train/                  |             |
|    approx_kl            | 0.005521981 |
|    clip_fraction        | 0.045       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.995       |
|    learning_rate        | 0.0001      |
|    loss                 | 13.8        |
|    n_updates            | 12090       |
|    policy_gradient_loss | -0.00191    |
|    std                  | 0.722       |
|    value_loss           | 6.32        |
-----------------------------------------
New best mean reward!
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1210    |
|    time_elapsed    | 18341   |
|    total_timesteps | 4956160 |
--------------------------------
Eval num_timesteps=4960000, episode_reward=-186.95 +/- 83.44
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -187        |
| time/                   |             |
|    total_timesteps      | 4960000     |
| train/                  |             |
|    approx_kl            | 0.005278471 |
|    clip_fraction        | 0.0454      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.37        |
|    n_updates            | 12100       |
|    policy_gradient_loss | 8.17e-05    |
|    std                  | 0.722       |
|    value_loss           | 20.9        |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1211    |
|    time_elapsed    | 18351   |
|    total_timesteps | 4960256 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 1212         |
|    time_elapsed         | 18358        |
|    total_timesteps      | 4964352      |
| train/                  |              |
|    approx_kl            | 0.0055230726 |
|    clip_fraction        | 0.0473       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0501       |
|    n_updates            | 12110        |
|    policy_gradient_loss | -0.00265     |
|    std                  | 0.72         |
|    value_loss           | 2.95         |
------------------------------------------
Eval num_timesteps=4965000, episode_reward=-398.06 +/- 153.90
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -398        |
| time/                   |             |
|    total_timesteps      | 4965000     |
| train/                  |             |
|    approx_kl            | 0.005679716 |
|    clip_fraction        | 0.0628      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | -0.00309    |
|    n_updates            | 12120       |
|    policy_gradient_loss | -0.00296    |
|    std                  | 0.726       |
|    value_loss           | 0.0386      |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1213    |
|    time_elapsed    | 18369   |
|    total_timesteps | 4968448 |
--------------------------------
Eval num_timesteps=4970000, episode_reward=-314.77 +/- 155.89
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -315         |
| time/                   |              |
|    total_timesteps      | 4970000      |
| train/                  |              |
|    approx_kl            | 0.0056687715 |
|    clip_fraction        | 0.035        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.227        |
|    n_updates            | 12130        |
|    policy_gradient_loss | -0.00173     |
|    std                  | 0.724        |
|    value_loss           | 1.4          |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1214    |
|    time_elapsed    | 18379   |
|    total_timesteps | 4972544 |
--------------------------------
Eval num_timesteps=4975000, episode_reward=-232.54 +/- 106.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -233         |
| time/                   |              |
|    total_timesteps      | 4975000      |
| train/                  |              |
|    approx_kl            | 0.0037516383 |
|    clip_fraction        | 0.024        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.66         |
|    n_updates            | 12140        |
|    policy_gradient_loss | -0.000464    |
|    std                  | 0.724        |
|    value_loss           | 7.74         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1215    |
|    time_elapsed    | 18390   |
|    total_timesteps | 4976640 |
--------------------------------
Eval num_timesteps=4980000, episode_reward=-245.27 +/- 131.93
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -245         |
| time/                   |              |
|    total_timesteps      | 4980000      |
| train/                  |              |
|    approx_kl            | 0.0033112147 |
|    clip_fraction        | 0.027        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.27        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.274        |
|    n_updates            | 12150        |
|    policy_gradient_loss | 0.000181     |
|    std                  | 0.725        |
|    value_loss           | 3.52         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1216    |
|    time_elapsed    | 18400   |
|    total_timesteps | 4980736 |
--------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 270          |
|    iterations           | 1217         |
|    time_elapsed         | 18407        |
|    total_timesteps      | 4984832      |
| train/                  |              |
|    approx_kl            | 0.0050796336 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.26        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0244       |
|    n_updates            | 12160        |
|    policy_gradient_loss | -0.000529    |
|    std                  | 0.721        |
|    value_loss           | 0.215        |
------------------------------------------
Eval num_timesteps=4985000, episode_reward=-354.96 +/- 181.77
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -355         |
| time/                   |              |
|    total_timesteps      | 4985000      |
| train/                  |              |
|    approx_kl            | 0.0051409043 |
|    clip_fraction        | 0.0497       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 1            |
|    learning_rate        | 0.0001       |
|    loss                 | -0.0124      |
|    n_updates            | 12170        |
|    policy_gradient_loss | -0.00121     |
|    std                  | 0.719        |
|    value_loss           | 0.0211       |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1218    |
|    time_elapsed    | 18417   |
|    total_timesteps | 4988928 |
--------------------------------
Eval num_timesteps=4990000, episode_reward=-236.86 +/- 87.29
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -237         |
| time/                   |              |
|    total_timesteps      | 4990000      |
| train/                  |              |
|    approx_kl            | 0.0031274692 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.25        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.171        |
|    n_updates            | 12180        |
|    policy_gradient_loss | -0.00108     |
|    std                  | 0.719        |
|    value_loss           | 11.7         |
------------------------------------------
--------------------------------
| time/              |         |
|    fps             | 270     |
|    iterations      | 1219    |
|    time_elapsed    | 18427   |
|    total_timesteps | 4993024 |
--------------------------------
Eval num_timesteps=4995000, episode_reward=-317.00 +/- 213.82
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -317        |
| time/                   |             |
|    total_timesteps      | 4995000     |
| train/                  |             |
|    approx_kl            | 0.002449911 |
|    clip_fraction        | 0.0125      |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.25       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0335      |
|    n_updates            | 12190       |
|    policy_gradient_loss | 0.000806    |
|    std                  | 0.722       |
|    value_loss           | 0.353       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 271     |
|    iterations      | 1220    |
|    time_elapsed    | 18438   |
|    total_timesteps | 4997120 |
--------------------------------
Eval num_timesteps=5000000, episode_reward=-211.32 +/- 148.71
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -211        |
| time/                   |             |
|    total_timesteps      | 5000000     |
| train/                  |             |
|    approx_kl            | 0.004088087 |
|    clip_fraction        | 0.038       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.999       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.356       |
|    n_updates            | 12200       |
|    policy_gradient_loss | -0.000484   |
|    std                  | 0.724       |
|    value_loss           | 0.474       |
-----------------------------------------
--------------------------------
| time/              |         |
|    fps             | 271     |
|    iterations      | 1221    |
|    time_elapsed    | 18448   |
|    total_timesteps | 5001216 |
--------------------------------
Training completed in 18451.68 seconds (~5.13 hours).
Model saved successfully!
