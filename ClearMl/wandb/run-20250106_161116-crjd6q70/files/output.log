Using cpu device
-----------------------------
| time/              |      |
|    fps             | 1371 |
|    iterations      | 1    |
|    time_elapsed    | 2    |
|    total_timesteps | 4096 |
-----------------------------
C:\Users\jimal\anaconda3\envs\Y2B\lib\site-packages\stable_baselines3\common\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=5000, episode_reward=-560.24 +/- 163.73
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -560         |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0022768588 |
|    clip_fraction        | 0.00229      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | -0.00644     |
|    learning_rate        | 0.0001       |
|    loss                 | 11.6         |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00149     |
|    std                  | 0.999        |
|    value_loss           | 77.3         |
------------------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 703  |
|    iterations      | 2    |
|    time_elapsed    | 11   |
|    total_timesteps | 8192 |
-----------------------------
Eval num_timesteps=10000, episode_reward=-535.01 +/- 71.80
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -535        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.004089876 |
|    clip_fraction        | 0.0194      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.324       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.697       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00265    |
|    std                  | 0.995       |
|    value_loss           | 17.9        |
-----------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 601   |
|    iterations      | 3     |
|    time_elapsed    | 20    |
|    total_timesteps | 12288 |
------------------------------
Eval num_timesteps=15000, episode_reward=-521.37 +/- 75.77
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -521          |
| time/                   |               |
|    total_timesteps      | 15000         |
| train/                  |               |
|    approx_kl            | 0.00032711375 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.24         |
|    explained_variance   | 0.721         |
|    learning_rate        | 0.0001        |
|    loss                 | 0.827         |
|    n_updates            | 30            |
|    policy_gradient_loss | -0.00049      |
|    std                  | 0.996         |
|    value_loss           | 13.1          |
-------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 560   |
|    iterations      | 4     |
|    time_elapsed    | 29    |
|    total_timesteps | 16384 |
------------------------------
Eval num_timesteps=20000, episode_reward=-592.43 +/- 64.88
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -592         |
| time/                   |              |
|    total_timesteps      | 20000        |
| train/                  |              |
|    approx_kl            | 0.0003942725 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.876        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.95         |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.000331    |
|    std                  | 0.995        |
|    value_loss           | 18.8         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 544   |
|    iterations      | 5     |
|    time_elapsed    | 37    |
|    total_timesteps | 20480 |
------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 564           |
|    iterations           | 6             |
|    time_elapsed         | 43            |
|    total_timesteps      | 24576         |
| train/                  |               |
|    approx_kl            | 0.00077471806 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.24         |
|    explained_variance   | 0.0853        |
|    learning_rate        | 0.0001        |
|    loss                 | 10.3          |
|    n_updates            | 50            |
|    policy_gradient_loss | -0.00102      |
|    std                  | 0.994         |
|    value_loss           | 44.5          |
-------------------------------------------
Eval num_timesteps=25000, episode_reward=-361.67 +/- 105.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -362         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0027484987 |
|    clip_fraction        | 0.00247      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | -1.2         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.913        |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.000909    |
|    std                  | 0.992        |
|    value_loss           | 8.64         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 550   |
|    iterations      | 7     |
|    time_elapsed    | 52    |
|    total_timesteps | 28672 |
------------------------------
Eval num_timesteps=30000, episode_reward=-482.55 +/- 161.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -483         |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0007271297 |
|    clip_fraction        | 2.44e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.0229       |
|    learning_rate        | 0.0001       |
|    loss                 | 6.98         |
|    n_updates            | 70           |
|    policy_gradient_loss | -0.000399    |
|    std                  | 0.99         |
|    value_loss           | 22           |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 538   |
|    iterations      | 8     |
|    time_elapsed    | 60    |
|    total_timesteps | 32768 |
------------------------------
Eval num_timesteps=35000, episode_reward=-399.12 +/- 77.66
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -399          |
| time/                   |               |
|    total_timesteps      | 35000         |
| train/                  |               |
|    approx_kl            | 8.2531726e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.23         |
|    explained_variance   | 0.13          |
|    learning_rate        | 0.0001        |
|    loss                 | 9.58          |
|    n_updates            | 80            |
|    policy_gradient_loss | -0.000263     |
|    std                  | 0.99          |
|    value_loss           | 35.6          |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 527   |
|    iterations      | 9     |
|    time_elapsed    | 69    |
|    total_timesteps | 36864 |
------------------------------
Eval num_timesteps=40000, episode_reward=-300.78 +/- 94.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -301         |
| time/                   |              |
|    total_timesteps      | 40000        |
| train/                  |              |
|    approx_kl            | 0.0024868078 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.0159       |
|    learning_rate        | 0.0001       |
|    loss                 | 2.73         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000426    |
|    std                  | 0.99         |
|    value_loss           | 10.3         |
------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 499   |
|    iterations      | 10    |
|    time_elapsed    | 81    |
|    total_timesteps | 40960 |
------------------------------
Eval num_timesteps=45000, episode_reward=-434.84 +/- 106.02
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -435         |
| time/                   |              |
|    total_timesteps      | 45000        |
| train/                  |              |
|    approx_kl            | 0.0022148425 |
|    clip_fraction        | 0.00083      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.025        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.853        |
|    n_updates            | 100          |
|    policy_gradient_loss | -0.000616    |
|    std                  | 0.99         |
|    value_loss           | 8.07         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 482   |
|    iterations      | 11    |
|    time_elapsed    | 93    |
|    total_timesteps | 45056 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 498         |
|    iterations           | 12          |
|    time_elapsed         | 98          |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.002810827 |
|    clip_fraction        | 0.0124      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | -0.0455     |
|    learning_rate        | 0.0001      |
|    loss                 | 2.97        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00208    |
|    std                  | 0.994       |
|    value_loss           | 6.23        |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=-485.93 +/- 149.30
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -486        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.000315997 |
|    clip_fraction        | 0           |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0001      |
|    loss                 | 5.66        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.000466   |
|    std                  | 0.994       |
|    value_loss           | 23.3        |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 498   |
|    iterations      | 13    |
|    time_elapsed    | 106   |
|    total_timesteps | 53248 |
------------------------------
Eval num_timesteps=55000, episode_reward=-449.35 +/- 208.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -449        |
| time/                   |             |
|    total_timesteps      | 55000       |
| train/                  |             |
|    approx_kl            | 0.003121149 |
|    clip_fraction        | 0.0154      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.0116      |
|    learning_rate        | 0.0001      |
|    loss                 | 1.36        |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0021     |
|    std                  | 0.991       |
|    value_loss           | 7.27        |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 497   |
|    iterations      | 14    |
|    time_elapsed    | 115   |
|    total_timesteps | 57344 |
------------------------------
Eval num_timesteps=60000, episode_reward=-455.55 +/- 101.82
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -456         |
| time/                   |              |
|    total_timesteps      | 60000        |
| train/                  |              |
|    approx_kl            | 0.0022489289 |
|    clip_fraction        | 0.000806     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.86         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.34         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.000607    |
|    std                  | 0.991        |
|    value_loss           | 6.86         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 498   |
|    iterations      | 15    |
|    time_elapsed    | 123   |
|    total_timesteps | 61440 |
------------------------------
Eval num_timesteps=65000, episode_reward=-460.49 +/- 144.59
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -460          |
| time/                   |               |
|    total_timesteps      | 65000         |
| train/                  |               |
|    approx_kl            | 0.00045942166 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.23         |
|    explained_variance   | 0.695         |
|    learning_rate        | 0.0001        |
|    loss                 | 6.13          |
|    n_updates            | 150           |
|    policy_gradient_loss | -0.000242     |
|    std                  | 0.991         |
|    value_loss           | 30.9          |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 496   |
|    iterations      | 16    |
|    time_elapsed    | 131   |
|    total_timesteps | 65536 |
------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 505         |
|    iterations           | 17          |
|    time_elapsed         | 137         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.004020579 |
|    clip_fraction        | 0.0176      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.977       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.758       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00337    |
|    std                  | 0.995       |
|    value_loss           | 3.67        |
-----------------------------------------
Eval num_timesteps=70000, episode_reward=-487.19 +/- 155.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -487         |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0035621305 |
|    clip_fraction        | 0.013        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.707        |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00179     |
|    std                  | 0.996        |
|    value_loss           | 2.69         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 502   |
|    iterations      | 18    |
|    time_elapsed    | 146   |
|    total_timesteps | 73728 |
------------------------------
Eval num_timesteps=75000, episode_reward=-549.03 +/- 97.22
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -549         |
| time/                   |              |
|    total_timesteps      | 75000        |
| train/                  |              |
|    approx_kl            | 0.0010922579 |
|    clip_fraction        | 0.000269     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.954        |
|    learning_rate        | 0.0001       |
|    loss                 | 5.1          |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.000552    |
|    std                  | 0.996        |
|    value_loss           | 15.8         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 501   |
|    iterations      | 19    |
|    time_elapsed    | 155   |
|    total_timesteps | 77824 |
------------------------------
Eval num_timesteps=80000, episode_reward=-576.40 +/- 73.77
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -576         |
| time/                   |              |
|    total_timesteps      | 80000        |
| train/                  |              |
|    approx_kl            | 0.0018530712 |
|    clip_fraction        | 0.000757     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.25         |
|    n_updates            | 190          |
|    policy_gradient_loss | -0.000451    |
|    std                  | 0.994        |
|    value_loss           | 14.9         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 495   |
|    iterations      | 20    |
|    time_elapsed    | 165   |
|    total_timesteps | 81920 |
------------------------------
Eval num_timesteps=85000, episode_reward=-427.62 +/- 84.42
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -428        |
| time/                   |             |
|    total_timesteps      | 85000       |
| train/                  |             |
|    approx_kl            | 0.003136484 |
|    clip_fraction        | 0.00654     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.971       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.628       |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.000901   |
|    std                  | 0.999       |
|    value_loss           | 7.18        |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 492   |
|    iterations      | 21    |
|    time_elapsed    | 174   |
|    total_timesteps | 86016 |
------------------------------
Eval num_timesteps=90000, episode_reward=-584.86 +/- 145.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -585         |
| time/                   |              |
|    total_timesteps      | 90000        |
| train/                  |              |
|    approx_kl            | 0.0042252103 |
|    clip_fraction        | 0.0278       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.884        |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00285     |
|    std                  | 1            |
|    value_loss           | 3.7          |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 487   |
|    iterations      | 22    |
|    time_elapsed    | 184   |
|    total_timesteps | 90112 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 494          |
|    iterations           | 23           |
|    time_elapsed         | 190          |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0025841051 |
|    clip_fraction        | 0.00554      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.766        |
|    learning_rate        | 0.0001       |
|    loss                 | 2            |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00162     |
|    std                  | 0.998        |
|    value_loss           | 12.9         |
------------------------------------------
Eval num_timesteps=95000, episode_reward=-479.07 +/- 161.49
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -479        |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.004162172 |
|    clip_fraction        | 0.0162      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.936       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.287       |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.000734   |
|    std                  | 1           |
|    value_loss           | 1.42        |
-----------------------------------------
------------------------------
| time/              |       |
|    fps             | 492   |
|    iterations      | 24    |
|    time_elapsed    | 199   |
|    total_timesteps | 98304 |
------------------------------
Eval num_timesteps=100000, episode_reward=-417.52 +/- 129.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -418         |
| time/                   |              |
|    total_timesteps      | 100000       |
| train/                  |              |
|    approx_kl            | 0.0034929172 |
|    clip_fraction        | 0.0072       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.977        |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00139     |
|    std                  | 1            |
|    value_loss           | 5.08         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 489    |
|    iterations      | 25     |
|    time_elapsed    | 209    |
|    total_timesteps | 102400 |
-------------------------------
Eval num_timesteps=105000, episode_reward=-459.90 +/- 156.30
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -460         |
| time/                   |              |
|    total_timesteps      | 105000       |
| train/                  |              |
|    approx_kl            | 0.0038619768 |
|    clip_fraction        | 0.023        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.387        |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00199     |
|    std                  | 0.994        |
|    value_loss           | 1.87         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 488    |
|    iterations      | 26     |
|    time_elapsed    | 218    |
|    total_timesteps | 106496 |
-------------------------------
Eval num_timesteps=110000, episode_reward=-530.13 +/- 149.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -530         |
| time/                   |              |
|    total_timesteps      | 110000       |
| train/                  |              |
|    approx_kl            | 0.0040380764 |
|    clip_fraction        | 0.00806      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.45         |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.00111     |
|    std                  | 0.994        |
|    value_loss           | 5.92         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 486    |
|    iterations      | 27     |
|    time_elapsed    | 227    |
|    total_timesteps | 110592 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 492         |
|    iterations           | 28          |
|    time_elapsed         | 232         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.004441726 |
|    clip_fraction        | 0.0183      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.202       |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.0022     |
|    std                  | 0.99        |
|    value_loss           | 4.36        |
-----------------------------------------
Eval num_timesteps=115000, episode_reward=-364.31 +/- 85.25
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -364        |
| time/                   |             |
|    total_timesteps      | 115000      |
| train/                  |             |
|    approx_kl            | 0.003922196 |
|    clip_fraction        | 0.00667     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.988       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.293       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00212    |
|    std                  | 0.991       |
|    value_loss           | 4.79        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 489    |
|    iterations      | 29     |
|    time_elapsed    | 242    |
|    total_timesteps | 118784 |
-------------------------------
Eval num_timesteps=120000, episode_reward=-460.88 +/- 126.54
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -461        |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.003700243 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.98        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.612       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0016     |
|    std                  | 0.992       |
|    value_loss           | 2.28        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 487    |
|    iterations      | 30     |
|    time_elapsed    | 252    |
|    total_timesteps | 122880 |
-------------------------------
Eval num_timesteps=125000, episode_reward=-415.73 +/- 95.61
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -416        |
| time/                   |             |
|    total_timesteps      | 125000      |
| train/                  |             |
|    approx_kl            | 0.004042409 |
|    clip_fraction        | 0.00657     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0001      |
|    loss                 | 3.45        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00164    |
|    std                  | 0.993       |
|    value_loss           | 24          |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 485    |
|    iterations      | 31     |
|    time_elapsed    | 261    |
|    total_timesteps | 126976 |
-------------------------------
Eval num_timesteps=130000, episode_reward=-411.42 +/- 177.90
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -411         |
| time/                   |              |
|    total_timesteps      | 130000       |
| train/                  |              |
|    approx_kl            | 0.0045350217 |
|    clip_fraction        | 0.0336       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.996        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.28         |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00294     |
|    std                  | 0.994        |
|    value_loss           | 1.44         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 483    |
|    iterations      | 32     |
|    time_elapsed    | 271    |
|    total_timesteps | 131072 |
-------------------------------
Eval num_timesteps=135000, episode_reward=-518.47 +/- 191.27
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -518         |
| time/                   |              |
|    total_timesteps      | 135000       |
| train/                  |              |
|    approx_kl            | 0.0012932024 |
|    clip_fraction        | 9.77e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.975        |
|    learning_rate        | 0.0001       |
|    loss                 | 1.57         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.000703    |
|    std                  | 0.995        |
|    value_loss           | 21.3         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 478    |
|    iterations      | 33     |
|    time_elapsed    | 282    |
|    total_timesteps | 135168 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 481         |
|    iterations           | 34          |
|    time_elapsed         | 289         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.004274771 |
|    clip_fraction        | 0.0193      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.17        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00179    |
|    std                  | 0.993       |
|    value_loss           | 1.66        |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=-338.22 +/- 83.70
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -338       |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.00416848 |
|    clip_fraction        | 0.0143     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.24      |
|    explained_variance   | 0.997      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.112      |
|    n_updates            | 340        |
|    policy_gradient_loss | -0.00149   |
|    std                  | 0.994      |
|    value_loss           | 0.445      |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 479    |
|    iterations      | 35     |
|    time_elapsed    | 298    |
|    total_timesteps | 143360 |
-------------------------------
Eval num_timesteps=145000, episode_reward=-341.13 +/- 102.71
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -341         |
| time/                   |              |
|    total_timesteps      | 145000       |
| train/                  |              |
|    approx_kl            | 0.0029146178 |
|    clip_fraction        | 0.00144      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.809        |
|    n_updates            | 350          |
|    policy_gradient_loss | -0.00092     |
|    std                  | 0.994        |
|    value_loss           | 3.59         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 476    |
|    iterations      | 36     |
|    time_elapsed    | 309    |
|    total_timesteps | 147456 |
-------------------------------
Eval num_timesteps=150000, episode_reward=-416.59 +/- 130.47
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -417         |
| time/                   |              |
|    total_timesteps      | 150000       |
| train/                  |              |
|    approx_kl            | 0.0035557435 |
|    clip_fraction        | 0.0106       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.172        |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00102     |
|    std                  | 0.995        |
|    value_loss           | 10.2         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 37     |
|    time_elapsed    | 320    |
|    total_timesteps | 151552 |
-------------------------------
Eval num_timesteps=155000, episode_reward=-424.35 +/- 103.46
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -424        |
| time/                   |             |
|    total_timesteps      | 155000      |
| train/                  |             |
|    approx_kl            | 0.003679025 |
|    clip_fraction        | 0.0108      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.978       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.273       |
|    n_updates            | 370         |
|    policy_gradient_loss | -0.00247    |
|    std                  | 0.989       |
|    value_loss           | 3.74        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 38     |
|    time_elapsed    | 330    |
|    total_timesteps | 155648 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 475         |
|    iterations           | 39          |
|    time_elapsed         | 335         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.002968331 |
|    clip_fraction        | 0.012       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.975       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.685       |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00202    |
|    std                  | 0.99        |
|    value_loss           | 2.74        |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=-554.87 +/- 151.62
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -555         |
| time/                   |              |
|    total_timesteps      | 160000       |
| train/                  |              |
|    approx_kl            | 0.0050714426 |
|    clip_fraction        | 0.0279       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0312       |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00269     |
|    std                  | 0.988        |
|    value_loss           | 0.499        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 40     |
|    time_elapsed    | 347    |
|    total_timesteps | 163840 |
-------------------------------
Eval num_timesteps=165000, episode_reward=-469.79 +/- 163.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -470         |
| time/                   |              |
|    total_timesteps      | 165000       |
| train/                  |              |
|    approx_kl            | 0.0053399657 |
|    clip_fraction        | 0.0256       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.212        |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.00168     |
|    std                  | 0.986        |
|    value_loss           | 3.49         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 41     |
|    time_elapsed    | 357    |
|    total_timesteps | 167936 |
-------------------------------
Eval num_timesteps=170000, episode_reward=-430.54 +/- 162.72
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -431         |
| time/                   |              |
|    total_timesteps      | 170000       |
| train/                  |              |
|    approx_kl            | 0.0018262339 |
|    clip_fraction        | 0.00605      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.718        |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.000691    |
|    std                  | 0.985        |
|    value_loss           | 1.59         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 469    |
|    iterations      | 42     |
|    time_elapsed    | 366    |
|    total_timesteps | 172032 |
-------------------------------
Eval num_timesteps=175000, episode_reward=-393.85 +/- 73.67
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -394       |
| time/                   |            |
|    total_timesteps      | 175000     |
| train/                  |            |
|    approx_kl            | 0.00315893 |
|    clip_fraction        | 0.00862    |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.21      |
|    explained_variance   | 0.993      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.518      |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.00338   |
|    std                  | 0.986      |
|    value_loss           | 8.5        |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 469    |
|    iterations      | 43     |
|    time_elapsed    | 375    |
|    total_timesteps | 176128 |
-------------------------------
Eval num_timesteps=180000, episode_reward=-353.48 +/- 176.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -353         |
| time/                   |              |
|    total_timesteps      | 180000       |
| train/                  |              |
|    approx_kl            | 0.0039618914 |
|    clip_fraction        | 0.0173       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.968        |
|    learning_rate        | 0.0001       |
|    loss                 | 12.3         |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00235     |
|    std                  | 0.986        |
|    value_loss           | 12           |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 469    |
|    iterations      | 44     |
|    time_elapsed    | 383    |
|    total_timesteps | 180224 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 45           |
|    time_elapsed         | 389          |
|    total_timesteps      | 184320       |
| train/                  |              |
|    approx_kl            | 0.0038430602 |
|    clip_fraction        | 0.0129       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.226        |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.00169     |
|    std                  | 0.986        |
|    value_loss           | 8.74         |
------------------------------------------
Eval num_timesteps=185000, episode_reward=-441.50 +/- 105.93
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -441         |
| time/                   |              |
|    total_timesteps      | 185000       |
| train/                  |              |
|    approx_kl            | 0.0030716334 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.139        |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00129     |
|    std                  | 0.989        |
|    value_loss           | 0.524        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 46     |
|    time_elapsed    | 399    |
|    total_timesteps | 188416 |
-------------------------------
Eval num_timesteps=190000, episode_reward=-329.29 +/- 113.12
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -329         |
| time/                   |              |
|    total_timesteps      | 190000       |
| train/                  |              |
|    approx_kl            | 0.0027153736 |
|    clip_fraction        | 0.00293      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.292        |
|    n_updates            | 460          |
|    policy_gradient_loss | -0.000668    |
|    std                  | 0.986        |
|    value_loss           | 1.44         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 47     |
|    time_elapsed    | 407    |
|    total_timesteps | 192512 |
-------------------------------
Eval num_timesteps=195000, episode_reward=-242.07 +/- 51.46
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -242         |
| time/                   |              |
|    total_timesteps      | 195000       |
| train/                  |              |
|    approx_kl            | 0.0023202556 |
|    clip_fraction        | 0.00786      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.169        |
|    n_updates            | 470          |
|    policy_gradient_loss | -0.000445    |
|    std                  | 0.987        |
|    value_loss           | 10.6         |
------------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 48     |
|    time_elapsed    | 416    |
|    total_timesteps | 196608 |
-------------------------------
Eval num_timesteps=200000, episode_reward=-446.84 +/- 111.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -447        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.004206136 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.982       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.191       |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00283    |
|    std                  | 0.984       |
|    value_loss           | 3.13        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 49     |
|    time_elapsed    | 426    |
|    total_timesteps | 200704 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 50           |
|    time_elapsed         | 432          |
|    total_timesteps      | 204800       |
| train/                  |              |
|    approx_kl            | 0.0023905071 |
|    clip_fraction        | 0.00698      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.2         |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0966       |
|    n_updates            | 490          |
|    policy_gradient_loss | -0.000711    |
|    std                  | 0.982        |
|    value_loss           | 5.94         |
------------------------------------------
Eval num_timesteps=205000, episode_reward=-469.53 +/- 180.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -470        |
| time/                   |             |
|    total_timesteps      | 205000      |
| train/                  |             |
|    approx_kl            | 0.004444319 |
|    clip_fraction        | 0.026       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.979       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0718      |
|    n_updates            | 500         |
|    policy_gradient_loss | -0.00196    |
|    std                  | 0.987       |
|    value_loss           | 1.08        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 51     |
|    time_elapsed    | 441    |
|    total_timesteps | 208896 |
-------------------------------
Eval num_timesteps=210000, episode_reward=-381.90 +/- 73.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -382         |
| time/                   |              |
|    total_timesteps      | 210000       |
| train/                  |              |
|    approx_kl            | 0.0029024186 |
|    clip_fraction        | 0.0105       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 1.02         |
|    n_updates            | 510          |
|    policy_gradient_loss | -0.0013      |
|    std                  | 0.987        |
|    value_loss           | 10.8         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 52     |
|    time_elapsed    | 450    |
|    total_timesteps | 212992 |
-------------------------------
Eval num_timesteps=215000, episode_reward=-334.53 +/- 164.96
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -335         |
| time/                   |              |
|    total_timesteps      | 215000       |
| train/                  |              |
|    approx_kl            | 0.0058792746 |
|    clip_fraction        | 0.0459       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.057        |
|    n_updates            | 520          |
|    policy_gradient_loss | -0.00393     |
|    std                  | 0.987        |
|    value_loss           | 0.322        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 53     |
|    time_elapsed    | 459    |
|    total_timesteps | 217088 |
-------------------------------
Eval num_timesteps=220000, episode_reward=-548.28 +/- 68.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -548         |
| time/                   |              |
|    total_timesteps      | 220000       |
| train/                  |              |
|    approx_kl            | 0.0027879744 |
|    clip_fraction        | 0.00559      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.757        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.178        |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00138     |
|    std                  | 0.984        |
|    value_loss           | 3.23         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 54     |
|    time_elapsed    | 468    |
|    total_timesteps | 221184 |
-------------------------------
Eval num_timesteps=225000, episode_reward=-507.17 +/- 107.36
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -507        |
| time/                   |             |
|    total_timesteps      | 225000      |
| train/                  |             |
|    approx_kl            | 0.004148078 |
|    clip_fraction        | 0.0126      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.938       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.869       |
|    n_updates            | 540         |
|    policy_gradient_loss | -0.00113    |
|    std                  | 0.988       |
|    value_loss           | 1.96        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 55     |
|    time_elapsed    | 477    |
|    total_timesteps | 225280 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 474          |
|    iterations           | 56           |
|    time_elapsed         | 483          |
|    total_timesteps      | 229376       |
| train/                  |              |
|    approx_kl            | 0.0038361116 |
|    clip_fraction        | 0.00427      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.97         |
|    learning_rate        | 0.0001       |
|    loss                 | 2.01         |
|    n_updates            | 550          |
|    policy_gradient_loss | -0.00156     |
|    std                  | 0.986        |
|    value_loss           | 2.26         |
------------------------------------------
Eval num_timesteps=230000, episode_reward=-445.40 +/- 150.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -445         |
| time/                   |              |
|    total_timesteps      | 230000       |
| train/                  |              |
|    approx_kl            | 0.0039801607 |
|    clip_fraction        | 0.0142       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.943        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.14         |
|    n_updates            | 560          |
|    policy_gradient_loss | -0.00222     |
|    std                  | 0.981        |
|    value_loss           | 1.31         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 57     |
|    time_elapsed    | 493    |
|    total_timesteps | 233472 |
-------------------------------
Eval num_timesteps=235000, episode_reward=-428.24 +/- 205.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -428         |
| time/                   |              |
|    total_timesteps      | 235000       |
| train/                  |              |
|    approx_kl            | 0.0024177874 |
|    clip_fraction        | 0.00276      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.19        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0672       |
|    n_updates            | 570          |
|    policy_gradient_loss | -0.00104     |
|    std                  | 0.972        |
|    value_loss           | 2.01         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 58     |
|    time_elapsed    | 503    |
|    total_timesteps | 237568 |
-------------------------------
Eval num_timesteps=240000, episode_reward=-317.64 +/- 118.20
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -318         |
| time/                   |              |
|    total_timesteps      | 240000       |
| train/                  |              |
|    approx_kl            | 0.0034736542 |
|    clip_fraction        | 0.0063       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.977        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.217        |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.000993    |
|    std                  | 0.971        |
|    value_loss           | 10.9         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 59     |
|    time_elapsed    | 513    |
|    total_timesteps | 241664 |
-------------------------------
Eval num_timesteps=245000, episode_reward=-412.89 +/- 168.60
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -413       |
| time/                   |            |
|    total_timesteps      | 245000     |
| train/                  |            |
|    approx_kl            | 0.00462221 |
|    clip_fraction        | 0.0295     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.17      |
|    explained_variance   | 0.99       |
|    learning_rate        | 0.0001     |
|    loss                 | 0.0966     |
|    n_updates            | 590        |
|    policy_gradient_loss | -0.0021    |
|    std                  | 0.976      |
|    value_loss           | 1.61       |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 60     |
|    time_elapsed    | 522    |
|    total_timesteps | 245760 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 472          |
|    iterations           | 61           |
|    time_elapsed         | 528          |
|    total_timesteps      | 249856       |
| train/                  |              |
|    approx_kl            | 0.0041698846 |
|    clip_fraction        | 0.0188       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.18        |
|    explained_variance   | 0.922        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.123        |
|    n_updates            | 600          |
|    policy_gradient_loss | -0.002       |
|    std                  | 0.974        |
|    value_loss           | 2.55         |
------------------------------------------
Eval num_timesteps=250000, episode_reward=-339.55 +/- 49.33
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -340        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.004242366 |
|    clip_fraction        | 0.025       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.932       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.282       |
|    n_updates            | 610         |
|    policy_gradient_loss | -0.00293    |
|    std                  | 0.974       |
|    value_loss           | 1.1         |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 62     |
|    time_elapsed    | 538    |
|    total_timesteps | 253952 |
-------------------------------
Eval num_timesteps=255000, episode_reward=-419.27 +/- 132.98
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -419        |
| time/                   |             |
|    total_timesteps      | 255000      |
| train/                  |             |
|    approx_kl            | 0.004957848 |
|    clip_fraction        | 0.0216      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.991       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0742      |
|    n_updates            | 620         |
|    policy_gradient_loss | -0.00241    |
|    std                  | 0.972       |
|    value_loss           | 1.19        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 63     |
|    time_elapsed    | 547    |
|    total_timesteps | 258048 |
-------------------------------
Eval num_timesteps=260000, episode_reward=-399.13 +/- 129.04
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -399         |
| time/                   |              |
|    total_timesteps      | 260000       |
| train/                  |              |
|    approx_kl            | 0.0048176856 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.113        |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00237     |
|    std                  | 0.972        |
|    value_loss           | 6.43         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 64     |
|    time_elapsed    | 556    |
|    total_timesteps | 262144 |
-------------------------------
Eval num_timesteps=265000, episode_reward=-511.51 +/- 148.93
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -512        |
| time/                   |             |
|    total_timesteps      | 265000      |
| train/                  |             |
|    approx_kl            | 0.003290521 |
|    clip_fraction        | 0.00833     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.994       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0295      |
|    n_updates            | 640         |
|    policy_gradient_loss | -0.00101    |
|    std                  | 0.97        |
|    value_loss           | 0.771       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 65     |
|    time_elapsed    | 566    |
|    total_timesteps | 266240 |
-------------------------------
Eval num_timesteps=270000, episode_reward=-321.23 +/- 124.72
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -321         |
| time/                   |              |
|    total_timesteps      | 270000       |
| train/                  |              |
|    approx_kl            | 0.0051283357 |
|    clip_fraction        | 0.0325       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.981        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.127        |
|    n_updates            | 650          |
|    policy_gradient_loss | -0.00211     |
|    std                  | 0.971        |
|    value_loss           | 0.51         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 468    |
|    iterations      | 66     |
|    time_elapsed    | 577    |
|    total_timesteps | 270336 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 471          |
|    iterations           | 67           |
|    time_elapsed         | 582          |
|    total_timesteps      | 274432       |
| train/                  |              |
|    approx_kl            | 0.0062515764 |
|    clip_fraction        | 0.0432       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0336       |
|    n_updates            | 660          |
|    policy_gradient_loss | -0.00405     |
|    std                  | 0.971        |
|    value_loss           | 0.559        |
------------------------------------------
Eval num_timesteps=275000, episode_reward=-371.91 +/- 116.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -372         |
| time/                   |              |
|    total_timesteps      | 275000       |
| train/                  |              |
|    approx_kl            | 0.0057732468 |
|    clip_fraction        | 0.0443       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0821       |
|    n_updates            | 670          |
|    policy_gradient_loss | -0.0042      |
|    std                  | 0.966        |
|    value_loss           | 0.235        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 68     |
|    time_elapsed    | 591    |
|    total_timesteps | 278528 |
-------------------------------
Eval num_timesteps=280000, episode_reward=-444.24 +/- 63.75
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -444        |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.005070146 |
|    clip_fraction        | 0.0264      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0738      |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00276    |
|    std                  | 0.962       |
|    value_loss           | 0.986       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 69     |
|    time_elapsed    | 600    |
|    total_timesteps | 282624 |
-------------------------------
Eval num_timesteps=285000, episode_reward=-363.72 +/- 86.65
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -364         |
| time/                   |              |
|    total_timesteps      | 285000       |
| train/                  |              |
|    approx_kl            | 0.0035513947 |
|    clip_fraction        | 0.0108       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.14        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0885       |
|    n_updates            | 690          |
|    policy_gradient_loss | -0.00063     |
|    std                  | 0.965        |
|    value_loss           | 2.69         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 70     |
|    time_elapsed    | 608    |
|    total_timesteps | 286720 |
-------------------------------
Eval num_timesteps=290000, episode_reward=-421.61 +/- 107.45
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -422         |
| time/                   |              |
|    total_timesteps      | 290000       |
| train/                  |              |
|    approx_kl            | 0.0051189866 |
|    clip_fraction        | 0.0506       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.15        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0644       |
|    n_updates            | 700          |
|    policy_gradient_loss | -0.00448     |
|    std                  | 0.969        |
|    value_loss           | 0.278        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 470    |
|    iterations      | 71     |
|    time_elapsed    | 617    |
|    total_timesteps | 290816 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 72           |
|    time_elapsed         | 623          |
|    total_timesteps      | 294912       |
| train/                  |              |
|    approx_kl            | 0.0033077719 |
|    clip_fraction        | 0.0164       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.992        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0314       |
|    n_updates            | 710          |
|    policy_gradient_loss | -0.00163     |
|    std                  | 0.969        |
|    value_loss           | 0.247        |
------------------------------------------
Eval num_timesteps=295000, episode_reward=-450.82 +/- 90.74
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -451         |
| time/                   |              |
|    total_timesteps      | 295000       |
| train/                  |              |
|    approx_kl            | 0.0036462643 |
|    clip_fraction        | 0.00911      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.989        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.146        |
|    n_updates            | 720          |
|    policy_gradient_loss | -0.000593    |
|    std                  | 0.972        |
|    value_loss           | 0.611        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 73     |
|    time_elapsed    | 631    |
|    total_timesteps | 299008 |
-------------------------------
Eval num_timesteps=300000, episode_reward=-413.41 +/- 175.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -413         |
| time/                   |              |
|    total_timesteps      | 300000       |
| train/                  |              |
|    approx_kl            | 0.0033734466 |
|    clip_fraction        | 0.0161       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.99         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.191        |
|    n_updates            | 730          |
|    policy_gradient_loss | -0.00113     |
|    std                  | 0.97         |
|    value_loss           | 2.46         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 74     |
|    time_elapsed    | 640    |
|    total_timesteps | 303104 |
-------------------------------
Eval num_timesteps=305000, episode_reward=-400.31 +/- 134.78
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -400         |
| time/                   |              |
|    total_timesteps      | 305000       |
| train/                  |              |
|    approx_kl            | 0.0046231057 |
|    clip_fraction        | 0.0199       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0916       |
|    n_updates            | 740          |
|    policy_gradient_loss | -0.000849    |
|    std                  | 0.975        |
|    value_loss           | 0.48         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 75     |
|    time_elapsed    | 649    |
|    total_timesteps | 307200 |
-------------------------------
Eval num_timesteps=310000, episode_reward=-362.66 +/- 63.59
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -363        |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.004625496 |
|    clip_fraction        | 0.0136      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.989       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.329       |
|    n_updates            | 750         |
|    policy_gradient_loss | -0.00179    |
|    std                  | 0.976       |
|    value_loss           | 4.42        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 76     |
|    time_elapsed    | 658    |
|    total_timesteps | 311296 |
-------------------------------
Eval num_timesteps=315000, episode_reward=-314.43 +/- 111.07
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -314        |
| time/                   |             |
|    total_timesteps      | 315000      |
| train/                  |             |
|    approx_kl            | 0.005480832 |
|    clip_fraction        | 0.0346      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.18       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 6           |
|    n_updates            | 760         |
|    policy_gradient_loss | -0.00447    |
|    std                  | 0.978       |
|    value_loss           | 4.85        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 77     |
|    time_elapsed    | 668    |
|    total_timesteps | 315392 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 473         |
|    iterations           | 78          |
|    time_elapsed         | 674         |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.003634302 |
|    clip_fraction        | 0.0279      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.983       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.224       |
|    n_updates            | 770         |
|    policy_gradient_loss | -0.00236    |
|    std                  | 0.982       |
|    value_loss           | 0.671       |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=-400.45 +/- 117.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -400        |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.004614005 |
|    clip_fraction        | 0.0218      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.19       |
|    explained_variance   | 0.996       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0471      |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00299    |
|    std                  | 0.979       |
|    value_loss           | 0.251       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 79     |
|    time_elapsed    | 683    |
|    total_timesteps | 323584 |
-------------------------------
Eval num_timesteps=325000, episode_reward=-449.29 +/- 111.51
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -449       |
| time/                   |            |
|    total_timesteps      | 325000     |
| train/                  |            |
|    approx_kl            | 0.00405935 |
|    clip_fraction        | 0.0183     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.18      |
|    explained_variance   | 0.989      |
|    learning_rate        | 0.0001     |
|    loss                 | 0.336      |
|    n_updates            | 790        |
|    policy_gradient_loss | -0.00333   |
|    std                  | 0.977      |
|    value_loss           | 6.74       |
----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 80     |
|    time_elapsed    | 693    |
|    total_timesteps | 327680 |
-------------------------------
Eval num_timesteps=330000, episode_reward=-422.77 +/- 117.58
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -423         |
| time/                   |              |
|    total_timesteps      | 330000       |
| train/                  |              |
|    approx_kl            | 0.0037958012 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.969        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.104        |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.00182     |
|    std                  | 0.971        |
|    value_loss           | 4.35         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 81     |
|    time_elapsed    | 702    |
|    total_timesteps | 331776 |
-------------------------------
Eval num_timesteps=335000, episode_reward=-393.66 +/- 136.03
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -394         |
| time/                   |              |
|    total_timesteps      | 335000       |
| train/                  |              |
|    approx_kl            | 0.0058499156 |
|    clip_fraction        | 0.0324       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.245        |
|    n_updates            | 810          |
|    policy_gradient_loss | -0.000642    |
|    std                  | 0.977        |
|    value_loss           | 1.25         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 82     |
|    time_elapsed    | 711    |
|    total_timesteps | 335872 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 473         |
|    iterations           | 83          |
|    time_elapsed         | 717         |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.005819852 |
|    clip_fraction        | 0.0479      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.834       |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.00321    |
|    std                  | 0.971       |
|    value_loss           | 0.746       |
-----------------------------------------
Eval num_timesteps=340000, episode_reward=-334.82 +/- 148.55
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -335         |
| time/                   |              |
|    total_timesteps      | 340000       |
| train/                  |              |
|    approx_kl            | 0.0043005827 |
|    clip_fraction        | 0.0258       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0252       |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00152     |
|    std                  | 0.971        |
|    value_loss           | 0.34         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 84     |
|    time_elapsed    | 726    |
|    total_timesteps | 344064 |
-------------------------------
Eval num_timesteps=345000, episode_reward=-341.95 +/- 122.68
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -342         |
| time/                   |              |
|    total_timesteps      | 345000       |
| train/                  |              |
|    approx_kl            | 0.0016963901 |
|    clip_fraction        | 0.00461      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.126        |
|    n_updates            | 840          |
|    policy_gradient_loss | -0.00105     |
|    std                  | 0.971        |
|    value_loss           | 4.41         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 85     |
|    time_elapsed    | 737    |
|    total_timesteps | 348160 |
-------------------------------
Eval num_timesteps=350000, episode_reward=-419.38 +/- 135.25
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -419         |
| time/                   |              |
|    total_timesteps      | 350000       |
| train/                  |              |
|    approx_kl            | 0.0025760096 |
|    clip_fraction        | 0.0157       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.17        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.197        |
|    n_updates            | 850          |
|    policy_gradient_loss | -0.00026     |
|    std                  | 0.976        |
|    value_loss           | 0.521        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 86     |
|    time_elapsed    | 746    |
|    total_timesteps | 352256 |
-------------------------------
Eval num_timesteps=355000, episode_reward=-441.63 +/- 139.24
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -442        |
| time/                   |             |
|    total_timesteps      | 355000      |
| train/                  |             |
|    approx_kl            | 0.004283642 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.17       |
|    explained_variance   | 0.997       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0417      |
|    n_updates            | 860         |
|    policy_gradient_loss | -0.00214    |
|    std                  | 0.97        |
|    value_loss           | 0.252       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 87     |
|    time_elapsed    | 755    |
|    total_timesteps | 356352 |
-------------------------------
Eval num_timesteps=360000, episode_reward=-438.46 +/- 114.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -438         |
| time/                   |              |
|    total_timesteps      | 360000       |
| train/                  |              |
|    approx_kl            | 0.0030878535 |
|    clip_fraction        | 0.0138       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.998        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0658       |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00111     |
|    std                  | 0.969        |
|    value_loss           | 0.288        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 88     |
|    time_elapsed    | 763    |
|    total_timesteps | 360448 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 474          |
|    iterations           | 89           |
|    time_elapsed         | 768          |
|    total_timesteps      | 364544       |
| train/                  |              |
|    approx_kl            | 0.0036482923 |
|    clip_fraction        | 0.00532      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.16        |
|    explained_variance   | 0.98         |
|    learning_rate        | 0.0001       |
|    loss                 | 0.236        |
|    n_updates            | 880          |
|    policy_gradient_loss | -5.07e-05    |
|    std                  | 0.97         |
|    value_loss           | 7.1          |
------------------------------------------
Eval num_timesteps=365000, episode_reward=-316.78 +/- 167.27
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -317        |
| time/                   |             |
|    total_timesteps      | 365000      |
| train/                  |             |
|    approx_kl            | 0.003875618 |
|    clip_fraction        | 0.013       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0108      |
|    n_updates            | 890         |
|    policy_gradient_loss | -0.00104    |
|    std                  | 0.963       |
|    value_loss           | 0.137       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 90     |
|    time_elapsed    | 778    |
|    total_timesteps | 368640 |
-------------------------------
Eval num_timesteps=370000, episode_reward=-447.44 +/- 127.16
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -447         |
| time/                   |              |
|    total_timesteps      | 370000       |
| train/                  |              |
|    approx_kl            | 0.0041534766 |
|    clip_fraction        | 0.016        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.13        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0126       |
|    n_updates            | 900          |
|    policy_gradient_loss | -0.00107     |
|    std                  | 0.955        |
|    value_loss           | 0.325        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 91     |
|    time_elapsed    | 787    |
|    total_timesteps | 372736 |
-------------------------------
Eval num_timesteps=375000, episode_reward=-379.55 +/- 121.42
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -380         |
| time/                   |              |
|    total_timesteps      | 375000       |
| train/                  |              |
|    approx_kl            | 0.0035684388 |
|    clip_fraction        | 0.00986      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.13         |
|    n_updates            | 910          |
|    policy_gradient_loss | 6.68e-05     |
|    std                  | 0.957        |
|    value_loss           | 2.83         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 92     |
|    time_elapsed    | 797    |
|    total_timesteps | 376832 |
-------------------------------
Eval num_timesteps=380000, episode_reward=-330.14 +/- 140.01
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -330        |
| time/                   |             |
|    total_timesteps      | 380000      |
| train/                  |             |
|    approx_kl            | 0.004393125 |
|    clip_fraction        | 0.0147      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.11       |
|    explained_variance   | 0.99        |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0745      |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00154    |
|    std                  | 0.953       |
|    value_loss           | 0.889       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 93     |
|    time_elapsed    | 806    |
|    total_timesteps | 380928 |
-------------------------------
Eval num_timesteps=385000, episode_reward=-374.51 +/- 169.25
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -375        |
| time/                   |             |
|    total_timesteps      | 385000      |
| train/                  |             |
|    approx_kl            | 0.003091874 |
|    clip_fraction        | 0.0121      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.129       |
|    n_updates            | 930         |
|    policy_gradient_loss | -0.0023     |
|    std                  | 0.953       |
|    value_loss           | 6.04        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 94     |
|    time_elapsed    | 815    |
|    total_timesteps | 385024 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 95           |
|    time_elapsed         | 821          |
|    total_timesteps      | 389120       |
| train/                  |              |
|    approx_kl            | 0.0047629187 |
|    clip_fraction        | 0.0239       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.988        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0129       |
|    n_updates            | 940          |
|    policy_gradient_loss | -0.00191     |
|    std                  | 0.954        |
|    value_loss           | 0.769        |
------------------------------------------
Eval num_timesteps=390000, episode_reward=-476.11 +/- 142.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -476         |
| time/                   |              |
|    total_timesteps      | 390000       |
| train/                  |              |
|    approx_kl            | 0.0055025793 |
|    clip_fraction        | 0.0351       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.11        |
|    explained_variance   | 0.921        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.041        |
|    n_updates            | 950          |
|    policy_gradient_loss | -0.00229     |
|    std                  | 0.952        |
|    value_loss           | 0.32         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 96     |
|    time_elapsed    | 830    |
|    total_timesteps | 393216 |
-------------------------------
Eval num_timesteps=395000, episode_reward=-513.82 +/- 83.75
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -514         |
| time/                   |              |
|    total_timesteps      | 395000       |
| train/                  |              |
|    approx_kl            | 0.0045319474 |
|    clip_fraction        | 0.0131       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.965        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.69         |
|    n_updates            | 960          |
|    policy_gradient_loss | -0.00188     |
|    std                  | 0.951        |
|    value_loss           | 1.08         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 97     |
|    time_elapsed    | 839    |
|    total_timesteps | 397312 |
-------------------------------
Eval num_timesteps=400000, episode_reward=-223.26 +/- 96.22
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -223        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.005324548 |
|    clip_fraction        | 0.0277      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.1        |
|    explained_variance   | 0.987       |
|    learning_rate        | 0.0001      |
|    loss                 | 2.46        |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00232    |
|    std                  | 0.953       |
|    value_loss           | 3.54        |
-----------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 98     |
|    time_elapsed    | 847    |
|    total_timesteps | 401408 |
-------------------------------
Eval num_timesteps=405000, episode_reward=-270.97 +/- 121.94
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -271         |
| time/                   |              |
|    total_timesteps      | 405000       |
| train/                  |              |
|    approx_kl            | 0.0035848734 |
|    clip_fraction        | 0.014        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.1         |
|    explained_variance   | 0.877        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0312       |
|    n_updates            | 980          |
|    policy_gradient_loss | -0.00124     |
|    std                  | 0.951        |
|    value_loss           | 0.655        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 99     |
|    time_elapsed    | 858    |
|    total_timesteps | 405504 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 473         |
|    iterations           | 100         |
|    time_elapsed         | 865         |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.003503595 |
|    clip_fraction        | 0.0263      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.993       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0425      |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00269    |
|    std                  | 0.946       |
|    value_loss           | 0.173       |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=-371.59 +/- 46.76
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -372         |
| time/                   |              |
|    total_timesteps      | 410000       |
| train/                  |              |
|    approx_kl            | 0.0053058313 |
|    clip_fraction        | 0.0305       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0594       |
|    n_updates            | 1000         |
|    policy_gradient_loss | -0.00288     |
|    std                  | 0.947        |
|    value_loss           | 0.318        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 101    |
|    time_elapsed    | 874    |
|    total_timesteps | 413696 |
-------------------------------
Eval num_timesteps=415000, episode_reward=-456.08 +/- 100.41
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -456         |
| time/                   |              |
|    total_timesteps      | 415000       |
| train/                  |              |
|    approx_kl            | 0.0032215999 |
|    clip_fraction        | 0.0158       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.986        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.161        |
|    n_updates            | 1010         |
|    policy_gradient_loss | -0.002       |
|    std                  | 0.946        |
|    value_loss           | 8.13         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 472    |
|    iterations      | 102    |
|    time_elapsed    | 884    |
|    total_timesteps | 417792 |
-------------------------------
Eval num_timesteps=420000, episode_reward=-299.48 +/- 164.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -299         |
| time/                   |              |
|    total_timesteps      | 420000       |
| train/                  |              |
|    approx_kl            | 0.0024823442 |
|    clip_fraction        | 0.00417      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.984        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.202        |
|    n_updates            | 1020         |
|    policy_gradient_loss | -0.000878    |
|    std                  | 0.944        |
|    value_loss           | 8.95         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 103    |
|    time_elapsed    | 894    |
|    total_timesteps | 421888 |
-------------------------------
Eval num_timesteps=425000, episode_reward=-316.76 +/- 45.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -317         |
| time/                   |              |
|    total_timesteps      | 425000       |
| train/                  |              |
|    approx_kl            | 0.0043215808 |
|    clip_fraction        | 0.0368       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.978        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0863       |
|    n_updates            | 1030         |
|    policy_gradient_loss | -0.00288     |
|    std                  | 0.944        |
|    value_loss           | 11.3         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 104    |
|    time_elapsed    | 903    |
|    total_timesteps | 425984 |
-------------------------------
Eval num_timesteps=430000, episode_reward=-388.99 +/- 134.23
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -389        |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.005440311 |
|    clip_fraction        | 0.0374      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.986       |
|    learning_rate        | 0.0001      |
|    loss                 | 4.37        |
|    n_updates            | 1040        |
|    policy_gradient_loss | -0.0037     |
|    std                  | 0.942       |
|    value_loss           | 3.58        |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 471    |
|    iterations      | 105    |
|    time_elapsed    | 911    |
|    total_timesteps | 430080 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 473          |
|    iterations           | 106          |
|    time_elapsed         | 917          |
|    total_timesteps      | 434176       |
| train/                  |              |
|    approx_kl            | 0.0075334907 |
|    clip_fraction        | 0.0637       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.162        |
|    n_updates            | 1050         |
|    policy_gradient_loss | -0.00574     |
|    std                  | 0.94         |
|    value_loss           | 0.362        |
------------------------------------------
Eval num_timesteps=435000, episode_reward=-366.55 +/- 77.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -367         |
| time/                   |              |
|    total_timesteps      | 435000       |
| train/                  |              |
|    approx_kl            | 0.0050545274 |
|    clip_fraction        | 0.0291       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0441       |
|    n_updates            | 1060         |
|    policy_gradient_loss | -0.00212     |
|    std                  | 0.945        |
|    value_loss           | 0.23         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 107    |
|    time_elapsed    | 926    |
|    total_timesteps | 438272 |
-------------------------------
Eval num_timesteps=440000, episode_reward=-286.88 +/- 205.78
Episode length: 820.60 +/- 360.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 821          |
|    mean_reward          | -287         |
| time/                   |              |
|    total_timesteps      | 440000       |
| train/                  |              |
|    approx_kl            | 0.0041285716 |
|    clip_fraction        | 0.0206       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.468        |
|    n_updates            | 1070         |
|    policy_gradient_loss | -0.00188     |
|    std                  | 0.943        |
|    value_loss           | 0.702        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 108    |
|    time_elapsed    | 934    |
|    total_timesteps | 442368 |
-------------------------------
Eval num_timesteps=445000, episode_reward=-363.20 +/- 236.23
Episode length: 816.40 +/- 369.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 816         |
|    mean_reward          | -363        |
| time/                   |             |
|    total_timesteps      | 445000      |
| train/                  |             |
|    approx_kl            | 0.005798877 |
|    clip_fraction        | 0.0278      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0235      |
|    n_updates            | 1080        |
|    policy_gradient_loss | -0.00197    |
|    std                  | 0.941       |
|    value_loss           | 0.363       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 109    |
|    time_elapsed    | 942    |
|    total_timesteps | 446464 |
-------------------------------
Eval num_timesteps=450000, episode_reward=-478.52 +/- 75.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -479         |
| time/                   |              |
|    total_timesteps      | 450000       |
| train/                  |              |
|    approx_kl            | 0.0036789938 |
|    clip_fraction        | 0.0212       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.983        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.12         |
|    n_updates            | 1090         |
|    policy_gradient_loss | -0.00272     |
|    std                  | 0.941        |
|    value_loss           | 4.5          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 473    |
|    iterations      | 110    |
|    time_elapsed    | 951    |
|    total_timesteps | 450560 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 475          |
|    iterations           | 111          |
|    time_elapsed         | 957          |
|    total_timesteps      | 454656       |
| train/                  |              |
|    approx_kl            | 0.0052321623 |
|    clip_fraction        | 0.0423       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0368       |
|    n_updates            | 1100         |
|    policy_gradient_loss | -0.00455     |
|    std                  | 0.943        |
|    value_loss           | 0.121        |
------------------------------------------
Eval num_timesteps=455000, episode_reward=-328.35 +/- 144.23
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -328         |
| time/                   |              |
|    total_timesteps      | 455000       |
| train/                  |              |
|    approx_kl            | 0.0036886884 |
|    clip_fraction        | 0.0152       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.979        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.039        |
|    n_updates            | 1110         |
|    policy_gradient_loss | -0.00158     |
|    std                  | 0.946        |
|    value_loss           | 0.186        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 112    |
|    time_elapsed    | 965    |
|    total_timesteps | 458752 |
-------------------------------
Eval num_timesteps=460000, episode_reward=-314.70 +/- 178.35
Episode length: 814.60 +/- 372.80
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 815          |
|    mean_reward          | -315         |
| time/                   |              |
|    total_timesteps      | 460000       |
| train/                  |              |
|    approx_kl            | 0.0037703917 |
|    clip_fraction        | 0.00854      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.09        |
|    explained_variance   | 0.962        |
|    learning_rate        | 0.0001       |
|    loss                 | 2.96         |
|    n_updates            | 1120         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 0.946        |
|    value_loss           | 3.2          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 113    |
|    time_elapsed    | 973    |
|    total_timesteps | 462848 |
-------------------------------
Eval num_timesteps=465000, episode_reward=-516.89 +/- 117.66
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -517         |
| time/                   |              |
|    total_timesteps      | 465000       |
| train/                  |              |
|    approx_kl            | 0.0051001525 |
|    clip_fraction        | 0.0241       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.08        |
|    explained_variance   | 0.994        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0114       |
|    n_updates            | 1130         |
|    policy_gradient_loss | -0.00142     |
|    std                  | 0.942        |
|    value_loss           | 1.48         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 114    |
|    time_elapsed    | 982    |
|    total_timesteps | 466944 |
-------------------------------
Eval num_timesteps=470000, episode_reward=-454.70 +/- 193.08
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -455         |
| time/                   |              |
|    total_timesteps      | 470000       |
| train/                  |              |
|    approx_kl            | 0.0036002612 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.982        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.118        |
|    n_updates            | 1140         |
|    policy_gradient_loss | -0.00118     |
|    std                  | 0.94         |
|    value_loss           | 7.7          |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 115    |
|    time_elapsed    | 991    |
|    total_timesteps | 471040 |
-------------------------------
Eval num_timesteps=475000, episode_reward=-294.77 +/- 180.85
Episode length: 807.80 +/- 386.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 808         |
|    mean_reward          | -295        |
| time/                   |             |
|    total_timesteps      | 475000      |
| train/                  |             |
|    approx_kl            | 0.005321813 |
|    clip_fraction        | 0.0371      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.998       |
|    learning_rate        | 0.0001      |
|    loss                 | 0.0125      |
|    n_updates            | 1150        |
|    policy_gradient_loss | -0.00195    |
|    std                  | 0.94        |
|    value_loss           | 0.168       |
-----------------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 116    |
|    time_elapsed    | 999    |
|    total_timesteps | 475136 |
-------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 476          |
|    iterations           | 117          |
|    time_elapsed         | 1006         |
|    total_timesteps      | 479232       |
| train/                  |              |
|    approx_kl            | 0.0028917133 |
|    clip_fraction        | 0.0061       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.955        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.209        |
|    n_updates            | 1160         |
|    policy_gradient_loss | -0.00119     |
|    std                  | 0.939        |
|    value_loss           | 9.67         |
------------------------------------------
Eval num_timesteps=480000, episode_reward=-361.12 +/- 172.06
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -361         |
| time/                   |              |
|    total_timesteps      | 480000       |
| train/                  |              |
|    approx_kl            | 0.0045068404 |
|    clip_fraction        | 0.0318       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0185       |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00199     |
|    std                  | 0.938        |
|    value_loss           | 0.066        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 476    |
|    iterations      | 118    |
|    time_elapsed    | 1014   |
|    total_timesteps | 483328 |
-------------------------------
Eval num_timesteps=485000, episode_reward=-324.99 +/- 55.57
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -325         |
| time/                   |              |
|    total_timesteps      | 485000       |
| train/                  |              |
|    approx_kl            | 0.0036926987 |
|    clip_fraction        | 0.0163       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.999        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0659       |
|    n_updates            | 1180         |
|    policy_gradient_loss | -0.00104     |
|    std                  | 0.939        |
|    value_loss           | 0.206        |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 476    |
|    iterations      | 119    |
|    time_elapsed    | 1023   |
|    total_timesteps | 487424 |
-------------------------------
Eval num_timesteps=490000, episode_reward=-211.28 +/- 92.19
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -211         |
| time/                   |              |
|    total_timesteps      | 490000       |
| train/                  |              |
|    approx_kl            | 0.0038224594 |
|    clip_fraction        | 0.0111       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.991        |
|    learning_rate        | 0.0001       |
|    loss                 | 7.08         |
|    n_updates            | 1190         |
|    policy_gradient_loss | -0.000627    |
|    std                  | 0.939        |
|    value_loss           | 2.66         |
------------------------------------------
New best mean reward!
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 120    |
|    time_elapsed    | 1033   |
|    total_timesteps | 491520 |
-------------------------------
Eval num_timesteps=495000, episode_reward=-419.17 +/- 160.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -419         |
| time/                   |              |
|    total_timesteps      | 495000       |
| train/                  |              |
|    approx_kl            | 0.0053604227 |
|    clip_fraction        | 0.0317       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.06        |
|    explained_variance   | 0.995        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.18         |
|    n_updates            | 1200         |
|    policy_gradient_loss | -0.00193     |
|    std                  | 0.938        |
|    value_loss           | 2.16         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 121    |
|    time_elapsed    | 1042   |
|    total_timesteps | 495616 |
-------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 476         |
|    iterations           | 122         |
|    time_elapsed         | 1049        |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.004952128 |
|    clip_fraction        | 0.0226      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.06       |
|    explained_variance   | 0.992       |
|    learning_rate        | 0.0001      |
|    loss                 | 7.1         |
|    n_updates            | 1210        |
|    policy_gradient_loss | -0.0022     |
|    std                  | 0.936       |
|    value_loss           | 2.16        |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=-398.97 +/- 85.58
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -399         |
| time/                   |              |
|    total_timesteps      | 500000       |
| train/                  |              |
|    approx_kl            | 0.0038693792 |
|    clip_fraction        | 0.0185       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.07        |
|    explained_variance   | 0.997        |
|    learning_rate        | 0.0001       |
|    loss                 | 0.0167       |
|    n_updates            | 1220         |
|    policy_gradient_loss | -0.00152     |
|    std                  | 0.944        |
|    value_loss           | 0.12         |
------------------------------------------
-------------------------------
| time/              |        |
|    fps             | 475    |
|    iterations      | 123    |
|    time_elapsed    | 1058   |
|    total_timesteps | 503808 |
-------------------------------
Model saved successfully!
