Using cpu device
Training started...
-----------------------------
| time/              |      |
|    fps             | 185  |
|    iterations      | 1    |
|    time_elapsed    | 22   |
|    total_timesteps | 4096 |
-----------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 162         |
|    iterations           | 2           |
|    time_elapsed         | 50          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.001730195 |
|    clip_fraction        | 0.00125     |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | -0.0409     |
|    learning_rate        | 5e-05       |
|    loss                 | 50.1        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.00172    |
|    std                  | 0.999       |
|    value_loss           | 91.7        |
-----------------------------------------
C:\Users\jimal\anaconda3\envs\Y2B\lib\site-packages\stable_baselines3\common\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=10000, episode_reward=-5526.35 +/- 142.41
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -5.53e+03     |
| time/                   |               |
|    total_timesteps      | 10000         |
| train/                  |               |
|    approx_kl            | 0.00023200568 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.26         |
|    explained_variance   | 0.0193        |
|    learning_rate        | 5e-05         |
|    loss                 | 55.1          |
|    n_updates            | 20            |
|    policy_gradient_loss | -0.000357     |
|    std                  | 1             |
|    value_loss           | 131           |
-------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 126   |
|    iterations      | 3     |
|    time_elapsed    | 96    |
|    total_timesteps | 12288 |
------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 132           |
|    iterations           | 4             |
|    time_elapsed         | 123           |
|    total_timesteps      | 16384         |
| train/                  |               |
|    approx_kl            | 0.00034274952 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.27         |
|    explained_variance   | 0.529         |
|    learning_rate        | 5e-05         |
|    loss                 | 7.78          |
|    n_updates            | 30            |
|    policy_gradient_loss | -0.000489     |
|    std                  | 1.01          |
|    value_loss           | 25.8          |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=-5556.74 +/- 110.95
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -5.56e+03     |
| time/                   |               |
|    total_timesteps      | 20000         |
| train/                  |               |
|    approx_kl            | 0.00018711141 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.28         |
|    explained_variance   | 0.311         |
|    learning_rate        | 5e-05         |
|    loss                 | 35.5          |
|    n_updates            | 40            |
|    policy_gradient_loss | -0.000395     |
|    std                  | 1.01          |
|    value_loss           | 86.2          |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 117   |
|    iterations      | 5     |
|    time_elapsed    | 174   |
|    total_timesteps | 20480 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 123          |
|    iterations           | 6            |
|    time_elapsed         | 198          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0003992195 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.0756       |
|    learning_rate        | 5e-05        |
|    loss                 | 50.6         |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.000627    |
|    std                  | 1.01         |
|    value_loss           | 98           |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 129          |
|    iterations           | 7            |
|    time_elapsed         | 221          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0011209894 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.46         |
|    learning_rate        | 5e-05        |
|    loss                 | 31.2         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00153     |
|    std                  | 1.01         |
|    value_loss           | 64.7         |
------------------------------------------
Eval num_timesteps=30000, episode_reward=-5429.55 +/- 158.84
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -5.43e+03     |
| time/                   |               |
|    total_timesteps      | 30000         |
| train/                  |               |
|    approx_kl            | 0.00026918604 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.29         |
|    explained_variance   | 0.115         |
|    learning_rate        | 5e-05         |
|    loss                 | 25.7          |
|    n_updates            | 70            |
|    policy_gradient_loss | -0.000417     |
|    std                  | 1.01          |
|    value_loss           | 51.7          |
-------------------------------------------
New best mean reward!
------------------------------
| time/              |       |
|    fps             | 122   |
|    iterations      | 8     |
|    time_elapsed    | 268   |
|    total_timesteps | 32768 |
------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 127          |
|    iterations           | 9            |
|    time_elapsed         | 288          |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0009103536 |
|    clip_fraction        | 0.00022      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.29        |
|    explained_variance   | 0.0673       |
|    learning_rate        | 5e-05        |
|    loss                 | 67.4         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.0011      |
|    std                  | 1.01         |
|    value_loss           | 135          |
------------------------------------------
Eval num_timesteps=40000, episode_reward=-5474.20 +/- 76.15
Episode length: 1001.00 +/- 0.00
-------------------------------------------
| eval/                   |               |
|    mean_ep_length       | 1e+03         |
|    mean_reward          | -5.47e+03     |
| time/                   |               |
|    total_timesteps      | 40000         |
| train/                  |               |
|    approx_kl            | 0.00083831046 |
|    clip_fraction        | 4.88e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.3          |
|    explained_variance   | 0.117         |
|    learning_rate        | 5e-05         |
|    loss                 | 42.9          |
|    n_updates            | 90            |
|    policy_gradient_loss | -0.00116      |
|    std                  | 1.02          |
|    value_loss           | 74            |
-------------------------------------------
------------------------------
| time/              |       |
|    fps             | 124   |
|    iterations      | 10    |
|    time_elapsed    | 329   |
|    total_timesteps | 40960 |
------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 126           |
|    iterations           | 11            |
|    time_elapsed         | 356           |
|    total_timesteps      | 45056         |
| train/                  |               |
|    approx_kl            | 0.00030529386 |
|    clip_fraction        | 0             |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.3          |
|    explained_variance   | 0.0274        |
|    learning_rate        | 5e-05         |
|    loss                 | 61.1          |
|    n_updates            | 100           |
|    policy_gradient_loss | -0.00027      |
|    std                  | 1.02          |
|    value_loss           | 99            |
-------------------------------------------
-------------------------------------------
| time/                   |               |
|    fps                  | 129           |
|    iterations           | 12            |
|    time_elapsed         | 379           |
|    total_timesteps      | 49152         |
| train/                  |               |
|    approx_kl            | 0.00082960003 |
|    clip_fraction        | 9.77e-05      |
|    clip_range           | 0.2           |
|    entropy_loss         | -4.3          |
|    explained_variance   | 0.0381        |
|    learning_rate        | 5e-05         |
|    loss                 | 46.9          |
|    n_updates            | 110           |
|    policy_gradient_loss | -0.000687     |
|    std                  | 1.01          |
|    value_loss           | 97.9          |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=-5530.38 +/- 205.62
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -5.53e+03    |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0008791687 |
|    clip_fraction        | 4.88e-05     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.0384       |
|    learning_rate        | 5e-05        |
|    loss                 | 44.4         |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.000784    |
|    std                  | 1.01         |
|    value_loss           | 86.6         |
------------------------------------------
------------------------------
| time/              |       |
|    fps             | 124   |
|    iterations      | 13    |
|    time_elapsed    | 428   |
|    total_timesteps | 53248 |
------------------------------
