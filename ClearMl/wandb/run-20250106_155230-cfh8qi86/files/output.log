Using cpu device
C:\Users\jimal\anaconda3\envs\Y2B\lib\site-packages\stable_baselines3\common\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.
  warnings.warn(
Eval num_timesteps=1000, episode_reward=-441.21 +/- 127.31
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-446.11 +/- 195.12
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -446     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 276  |
|    iterations      | 1    |
|    time_elapsed    | 7    |
|    total_timesteps | 2048 |
-----------------------------
Eval num_timesteps=3000, episode_reward=-387.95 +/- 94.31
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -388         |
| time/                   |              |
|    total_timesteps      | 3000         |
| train/                  |              |
|    approx_kl            | 0.0040626684 |
|    clip_fraction        | 0.0122       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.0432       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.474        |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00188     |
|    std                  | 0.994        |
|    value_loss           | 11.3         |
------------------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=-442.75 +/- 109.59
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -443     |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 243  |
|    iterations      | 2    |
|    time_elapsed    | 16   |
|    total_timesteps | 4096 |
-----------------------------
Eval num_timesteps=5000, episode_reward=-645.13 +/- 51.98
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -645         |
| time/                   |              |
|    total_timesteps      | 5000         |
| train/                  |              |
|    approx_kl            | 0.0034721144 |
|    clip_fraction        | 0.00674      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.441        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.56         |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.000267    |
|    std                  | 0.996        |
|    value_loss           | 11.6         |
------------------------------------------
Eval num_timesteps=6000, episode_reward=-586.64 +/- 130.52
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -587     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
-----------------------------
| time/              |      |
|    fps             | 236  |
|    iterations      | 3    |
|    time_elapsed    | 25   |
|    total_timesteps | 6144 |
-----------------------------
Eval num_timesteps=7000, episode_reward=-479.37 +/- 280.68
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -479         |
| time/                   |              |
|    total_timesteps      | 7000         |
| train/                  |              |
|    approx_kl            | 0.0052643204 |
|    clip_fraction        | 0.0194       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.835        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.238        |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.00313     |
|    std                  | 0.992        |
|    value_loss           | 4.52         |
------------------------------------------
Eval num_timesteps=8000, episode_reward=-383.86 +/- 72.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -384     |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
New best mean reward!
-----------------------------
| time/              |      |
|    fps             | 234  |
|    iterations      | 4    |
|    time_elapsed    | 34   |
|    total_timesteps | 8192 |
-----------------------------
Eval num_timesteps=9000, episode_reward=-627.57 +/- 143.79
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -628         |
| time/                   |              |
|    total_timesteps      | 9000         |
| train/                  |              |
|    approx_kl            | 0.0068913056 |
|    clip_fraction        | 0.0283       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.23        |
|    explained_variance   | 0.0463       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.506        |
|    n_updates            | 40           |
|    policy_gradient_loss | -0.00282     |
|    std                  | 0.993        |
|    value_loss           | 8.19         |
------------------------------------------
Eval num_timesteps=10000, episode_reward=-668.74 +/- 141.07
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -669     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 231   |
|    iterations      | 5     |
|    time_elapsed    | 44    |
|    total_timesteps | 10240 |
------------------------------
Eval num_timesteps=11000, episode_reward=-548.75 +/- 108.90
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -549         |
| time/                   |              |
|    total_timesteps      | 11000        |
| train/                  |              |
|    approx_kl            | 0.0004139642 |
|    clip_fraction        | 0            |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.945        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.02         |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.000128    |
|    std                  | 0.994        |
|    value_loss           | 14.7         |
------------------------------------------
Eval num_timesteps=12000, episode_reward=-810.18 +/- 285.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -810     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 232   |
|    iterations      | 6     |
|    time_elapsed    | 52    |
|    total_timesteps | 12288 |
------------------------------
Eval num_timesteps=13000, episode_reward=-592.23 +/- 158.68
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -592         |
| time/                   |              |
|    total_timesteps      | 13000        |
| train/                  |              |
|    approx_kl            | 0.0037579755 |
|    clip_fraction        | 0.0323       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.948        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.66         |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.0045      |
|    std                  | 0.996        |
|    value_loss           | 5.98         |
------------------------------------------
Eval num_timesteps=14000, episode_reward=-701.93 +/- 198.82
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -702     |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 233   |
|    iterations      | 7     |
|    time_elapsed    | 61    |
|    total_timesteps | 14336 |
------------------------------
Eval num_timesteps=15000, episode_reward=-508.40 +/- 264.69
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -508        |
| time/                   |             |
|    total_timesteps      | 15000       |
| train/                  |             |
|    approx_kl            | 0.004402525 |
|    clip_fraction        | 0.0269      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.503       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00287    |
|    std                  | 0.996       |
|    value_loss           | 2.34        |
-----------------------------------------
Eval num_timesteps=16000, episode_reward=-659.82 +/- 121.76
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -660     |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 232   |
|    iterations      | 8     |
|    time_elapsed    | 70    |
|    total_timesteps | 16384 |
------------------------------
Eval num_timesteps=17000, episode_reward=-739.42 +/- 126.60
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -739        |
| time/                   |             |
|    total_timesteps      | 17000       |
| train/                  |             |
|    approx_kl            | 0.005617949 |
|    clip_fraction        | 0.0133      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.34        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00242    |
|    std                  | 0.995       |
|    value_loss           | 30.4        |
-----------------------------------------
Eval num_timesteps=18000, episode_reward=-662.31 +/- 118.42
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -662     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 234   |
|    iterations      | 9     |
|    time_elapsed    | 78    |
|    total_timesteps | 18432 |
------------------------------
Eval num_timesteps=19000, episode_reward=-658.44 +/- 211.09
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -658         |
| time/                   |              |
|    total_timesteps      | 19000        |
| train/                  |              |
|    approx_kl            | 0.0012573432 |
|    clip_fraction        | 0.000293     |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.24        |
|    explained_variance   | 0.916        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.67         |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.000462    |
|    std                  | 0.996        |
|    value_loss           | 9.02         |
------------------------------------------
Eval num_timesteps=20000, episode_reward=-525.24 +/- 133.95
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 235   |
|    iterations      | 10    |
|    time_elapsed    | 87    |
|    total_timesteps | 20480 |
------------------------------
Eval num_timesteps=21000, episode_reward=-697.49 +/- 198.05
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -697        |
| time/                   |             |
|    total_timesteps      | 21000       |
| train/                  |             |
|    approx_kl            | 0.004238719 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.24       |
|    explained_variance   | 0.867       |
|    learning_rate        | 0.0003      |
|    loss                 | 30          |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00271    |
|    std                  | 0.997       |
|    value_loss           | 16.4        |
-----------------------------------------
Eval num_timesteps=22000, episode_reward=-549.05 +/- 125.78
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 236   |
|    iterations      | 11    |
|    time_elapsed    | 95    |
|    total_timesteps | 22528 |
------------------------------
Eval num_timesteps=23000, episode_reward=-515.94 +/- 208.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -516        |
| time/                   |             |
|    total_timesteps      | 23000       |
| train/                  |             |
|    approx_kl            | 0.003955904 |
|    clip_fraction        | 0.0155      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22        |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0027     |
|    std                  | 1           |
|    value_loss           | 11.2        |
-----------------------------------------
Eval num_timesteps=24000, episode_reward=-581.12 +/- 192.27
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -581     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 236   |
|    iterations      | 12    |
|    time_elapsed    | 103   |
|    total_timesteps | 24576 |
------------------------------
Eval num_timesteps=25000, episode_reward=-687.21 +/- 225.13
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -687         |
| time/                   |              |
|    total_timesteps      | 25000        |
| train/                  |              |
|    approx_kl            | 0.0056231855 |
|    clip_fraction        | 0.026        |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.28        |
|    explained_variance   | 0.705        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.775        |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.00356     |
|    std                  | 1.02         |
|    value_loss           | 3.1          |
------------------------------------------
Eval num_timesteps=26000, episode_reward=-498.14 +/- 181.53
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -498     |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 236   |
|    iterations      | 13    |
|    time_elapsed    | 112   |
|    total_timesteps | 26624 |
------------------------------
Eval num_timesteps=27000, episode_reward=-527.05 +/- 207.50
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -527         |
| time/                   |              |
|    total_timesteps      | 27000        |
| train/                  |              |
|    approx_kl            | 0.0047500124 |
|    clip_fraction        | 0.0332       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.31        |
|    explained_variance   | 0.797        |
|    learning_rate        | 0.0003       |
|    loss                 | 4.29         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.00463     |
|    std                  | 1.02         |
|    value_loss           | 9.64         |
------------------------------------------
Eval num_timesteps=28000, episode_reward=-612.75 +/- 73.21
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -613     |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 236   |
|    iterations      | 14    |
|    time_elapsed    | 121   |
|    total_timesteps | 28672 |
------------------------------
Eval num_timesteps=29000, episode_reward=-368.30 +/- 148.07
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -368         |
| time/                   |              |
|    total_timesteps      | 29000        |
| train/                  |              |
|    approx_kl            | 0.0031740149 |
|    clip_fraction        | 0.00791      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.32        |
|    explained_variance   | 0.74         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.316        |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00168     |
|    std                  | 1.02         |
|    value_loss           | 5.33         |
------------------------------------------
New best mean reward!
Eval num_timesteps=30000, episode_reward=-623.99 +/- 83.25
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -624     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 235   |
|    iterations      | 15    |
|    time_elapsed    | 130   |
|    total_timesteps | 30720 |
------------------------------
Eval num_timesteps=31000, episode_reward=-536.25 +/- 174.60
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -536         |
| time/                   |              |
|    total_timesteps      | 31000        |
| train/                  |              |
|    approx_kl            | 0.0028011063 |
|    clip_fraction        | 0.00874      |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.31        |
|    explained_variance   | 0.935        |
|    learning_rate        | 0.0003       |
|    loss                 | 5.5          |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00136     |
|    std                  | 1.02         |
|    value_loss           | 6.35         |
------------------------------------------
Eval num_timesteps=32000, episode_reward=-635.19 +/- 152.36
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -635     |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 235   |
|    iterations      | 16    |
|    time_elapsed    | 139   |
|    total_timesteps | 32768 |
------------------------------
Eval num_timesteps=33000, episode_reward=-741.52 +/- 200.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -742         |
| time/                   |              |
|    total_timesteps      | 33000        |
| train/                  |              |
|    approx_kl            | 0.0046749394 |
|    clip_fraction        | 0.0265       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.3         |
|    explained_variance   | 0.923        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.298        |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00207     |
|    std                  | 1.01         |
|    value_loss           | 3.85         |
------------------------------------------
Eval num_timesteps=34000, episode_reward=-633.80 +/- 279.17
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -634     |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 235   |
|    iterations      | 17    |
|    time_elapsed    | 147   |
|    total_timesteps | 34816 |
------------------------------
Eval num_timesteps=35000, episode_reward=-684.07 +/- 134.52
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -684         |
| time/                   |              |
|    total_timesteps      | 35000        |
| train/                  |              |
|    approx_kl            | 0.0072120135 |
|    clip_fraction        | 0.0433       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.31        |
|    explained_variance   | -0.295       |
|    learning_rate        | 0.0003       |
|    loss                 | 1.26         |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.0027      |
|    std                  | 1.02         |
|    value_loss           | 8.41         |
------------------------------------------
Eval num_timesteps=36000, episode_reward=-788.48 +/- 59.23
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -788     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 234   |
|    iterations      | 18    |
|    time_elapsed    | 157   |
|    total_timesteps | 36864 |
------------------------------
Eval num_timesteps=37000, episode_reward=-592.14 +/- 180.50
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -592        |
| time/                   |             |
|    total_timesteps      | 37000       |
| train/                  |             |
|    approx_kl            | 0.007919849 |
|    clip_fraction        | 0.0435      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.3        |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.566       |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00177    |
|    std                  | 1.01        |
|    value_loss           | 2.02        |
-----------------------------------------
Eval num_timesteps=38000, episode_reward=-681.42 +/- 279.98
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -681     |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 232   |
|    iterations      | 19    |
|    time_elapsed    | 167   |
|    total_timesteps | 38912 |
------------------------------
Eval num_timesteps=39000, episode_reward=-444.09 +/- 140.17
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -444       |
| time/                   |            |
|    total_timesteps      | 39000      |
| train/                  |            |
|    approx_kl            | 0.00521539 |
|    clip_fraction        | 0.0155     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.29      |
|    explained_variance   | 0.935      |
|    learning_rate        | 0.0003     |
|    loss                 | 3.99       |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00311   |
|    std                  | 1.01       |
|    value_loss           | 20.3       |
----------------------------------------
Eval num_timesteps=40000, episode_reward=-398.88 +/- 81.86
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 229   |
|    iterations      | 20    |
|    time_elapsed    | 178   |
|    total_timesteps | 40960 |
------------------------------
Eval num_timesteps=41000, episode_reward=-585.87 +/- 126.96
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -586        |
| time/                   |             |
|    total_timesteps      | 41000       |
| train/                  |             |
|    approx_kl            | 0.008300431 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.28       |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.64        |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00694    |
|    std                  | 1           |
|    value_loss           | 8.84        |
-----------------------------------------
Eval num_timesteps=42000, episode_reward=-504.95 +/- 186.49
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -505     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-601.77 +/- 203.09
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -602     |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 222   |
|    iterations      | 21    |
|    time_elapsed    | 193   |
|    total_timesteps | 43008 |
------------------------------
Eval num_timesteps=44000, episode_reward=-570.55 +/- 169.18
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -571         |
| time/                   |              |
|    total_timesteps      | 44000        |
| train/                  |              |
|    approx_kl            | 0.0068849055 |
|    clip_fraction        | 0.0656       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | -0.622       |
|    learning_rate        | 0.0003       |
|    loss                 | 0.663        |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.00596     |
|    std                  | 1            |
|    value_loss           | 2.2          |
------------------------------------------
Eval num_timesteps=45000, episode_reward=-603.87 +/- 101.69
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -604     |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 222   |
|    iterations      | 22    |
|    time_elapsed    | 202   |
|    total_timesteps | 45056 |
------------------------------
Eval num_timesteps=46000, episode_reward=-682.44 +/- 144.17
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -682         |
| time/                   |              |
|    total_timesteps      | 46000        |
| train/                  |              |
|    approx_kl            | 0.0040357895 |
|    clip_fraction        | 0.0244       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.27        |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.0003       |
|    loss                 | 1.7          |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.00223     |
|    std                  | 1            |
|    value_loss           | 3.62         |
------------------------------------------
Eval num_timesteps=47000, episode_reward=-736.61 +/- 100.13
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -737     |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 222   |
|    iterations      | 23    |
|    time_elapsed    | 211   |
|    total_timesteps | 47104 |
------------------------------
Eval num_timesteps=48000, episode_reward=-552.22 +/- 243.49
Episode length: 1001.00 +/- 0.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 1e+03     |
|    mean_reward          | -552      |
| time/                   |           |
|    total_timesteps      | 48000     |
| train/                  |           |
|    approx_kl            | 0.0035026 |
|    clip_fraction        | 0.00903   |
|    clip_range           | 0.2       |
|    entropy_loss         | -4.27     |
|    explained_variance   | 0.785     |
|    learning_rate        | 0.0003    |
|    loss                 | 5.26      |
|    n_updates            | 230       |
|    policy_gradient_loss | 0.000208  |
|    std                  | 1         |
|    value_loss           | 10.7      |
---------------------------------------
Eval num_timesteps=49000, episode_reward=-575.66 +/- 293.81
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -576     |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 222   |
|    iterations      | 24    |
|    time_elapsed    | 221   |
|    total_timesteps | 49152 |
------------------------------
Eval num_timesteps=50000, episode_reward=-737.56 +/- 185.63
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -738         |
| time/                   |              |
|    total_timesteps      | 50000        |
| train/                  |              |
|    approx_kl            | 0.0065131374 |
|    clip_fraction        | 0.0693       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.932        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.187        |
|    n_updates            | 240          |
|    policy_gradient_loss | -0.00451     |
|    std                  | 1            |
|    value_loss           | 1.36         |
------------------------------------------
Eval num_timesteps=51000, episode_reward=-586.06 +/- 160.94
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 222   |
|    iterations      | 25    |
|    time_elapsed    | 230   |
|    total_timesteps | 51200 |
------------------------------
Eval num_timesteps=52000, episode_reward=-550.73 +/- 119.65
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -551       |
| time/                   |            |
|    total_timesteps      | 52000      |
| train/                  |            |
|    approx_kl            | 0.00599677 |
|    clip_fraction        | 0.034      |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.952      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.235      |
|    n_updates            | 250        |
|    policy_gradient_loss | -0.00385   |
|    std                  | 1          |
|    value_loss           | 2.15       |
----------------------------------------
Eval num_timesteps=53000, episode_reward=-546.58 +/- 165.26
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 221   |
|    iterations      | 26    |
|    time_elapsed    | 240   |
|    total_timesteps | 53248 |
------------------------------
Eval num_timesteps=54000, episode_reward=-614.84 +/- 151.13
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -615        |
| time/                   |             |
|    total_timesteps      | 54000       |
| train/                  |             |
|    approx_kl            | 0.005451722 |
|    clip_fraction        | 0.019       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 4.03        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00175    |
|    std                  | 1           |
|    value_loss           | 5.18        |
-----------------------------------------
Eval num_timesteps=55000, episode_reward=-537.51 +/- 161.48
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 221   |
|    iterations      | 27    |
|    time_elapsed    | 249   |
|    total_timesteps | 55296 |
------------------------------
Eval num_timesteps=56000, episode_reward=-640.80 +/- 75.57
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -641         |
| time/                   |              |
|    total_timesteps      | 56000        |
| train/                  |              |
|    approx_kl            | 0.0075012716 |
|    clip_fraction        | 0.0502       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.906        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.419        |
|    n_updates            | 270          |
|    policy_gradient_loss | -0.00259     |
|    std                  | 1            |
|    value_loss           | 4.15         |
------------------------------------------
Eval num_timesteps=57000, episode_reward=-629.38 +/- 148.94
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -629     |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 221   |
|    iterations      | 28    |
|    time_elapsed    | 259   |
|    total_timesteps | 57344 |
------------------------------
Eval num_timesteps=58000, episode_reward=-572.35 +/- 149.72
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -572        |
| time/                   |             |
|    total_timesteps      | 58000       |
| train/                  |             |
|    approx_kl            | 0.005931679 |
|    clip_fraction        | 0.034       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.842       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.397       |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.00339    |
|    std                  | 1           |
|    value_loss           | 3.82        |
-----------------------------------------
Eval num_timesteps=59000, episode_reward=-640.76 +/- 81.04
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -641     |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 220   |
|    iterations      | 29    |
|    time_elapsed    | 269   |
|    total_timesteps | 59392 |
------------------------------
Eval num_timesteps=60000, episode_reward=-698.92 +/- 248.92
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -699        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.004933919 |
|    clip_fraction        | 0.0232      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.962       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.711       |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00245    |
|    std                  | 1           |
|    value_loss           | 4.74        |
-----------------------------------------
Eval num_timesteps=61000, episode_reward=-573.83 +/- 184.24
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 220   |
|    iterations      | 30    |
|    time_elapsed    | 278   |
|    total_timesteps | 61440 |
------------------------------
Eval num_timesteps=62000, episode_reward=-672.98 +/- 125.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -673         |
| time/                   |              |
|    total_timesteps      | 62000        |
| train/                  |              |
|    approx_kl            | 0.0055112885 |
|    clip_fraction        | 0.0295       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.25        |
|    explained_variance   | 0.966        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.424        |
|    n_updates            | 300          |
|    policy_gradient_loss | -0.00374     |
|    std                  | 0.997        |
|    value_loss           | 6.19         |
------------------------------------------
Eval num_timesteps=63000, episode_reward=-525.68 +/- 217.75
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 220   |
|    iterations      | 31    |
|    time_elapsed    | 287   |
|    total_timesteps | 63488 |
------------------------------
Eval num_timesteps=64000, episode_reward=-729.35 +/- 80.41
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -729        |
| time/                   |             |
|    total_timesteps      | 64000       |
| train/                  |             |
|    approx_kl            | 0.006120169 |
|    clip_fraction        | 0.0295      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.951       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.553       |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.00335    |
|    std                  | 1           |
|    value_loss           | 5.1         |
-----------------------------------------
Eval num_timesteps=65000, episode_reward=-451.17 +/- 190.31
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -451     |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 219   |
|    iterations      | 32    |
|    time_elapsed    | 298   |
|    total_timesteps | 65536 |
------------------------------
Eval num_timesteps=66000, episode_reward=-437.62 +/- 171.21
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -438         |
| time/                   |              |
|    total_timesteps      | 66000        |
| train/                  |              |
|    approx_kl            | 0.0058481395 |
|    clip_fraction        | 0.0474       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.912        |
|    learning_rate        | 0.0003       |
|    loss                 | 2.07         |
|    n_updates            | 320          |
|    policy_gradient_loss | -0.00249     |
|    std                  | 1            |
|    value_loss           | 8.82         |
------------------------------------------
Eval num_timesteps=67000, episode_reward=-526.56 +/- 221.37
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 218   |
|    iterations      | 33    |
|    time_elapsed    | 308   |
|    total_timesteps | 67584 |
------------------------------
Eval num_timesteps=68000, episode_reward=-583.97 +/- 138.76
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -584       |
| time/                   |            |
|    total_timesteps      | 68000      |
| train/                  |            |
|    approx_kl            | 0.00513669 |
|    clip_fraction        | 0.0426     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.82       |
|    learning_rate        | 0.0003     |
|    loss                 | 2.82       |
|    n_updates            | 330        |
|    policy_gradient_loss | -0.00256   |
|    std                  | 1          |
|    value_loss           | 3.49       |
----------------------------------------
Eval num_timesteps=69000, episode_reward=-493.09 +/- 182.60
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -493     |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 217   |
|    iterations      | 34    |
|    time_elapsed    | 319   |
|    total_timesteps | 69632 |
------------------------------
Eval num_timesteps=70000, episode_reward=-563.09 +/- 256.97
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -563         |
| time/                   |              |
|    total_timesteps      | 70000        |
| train/                  |              |
|    approx_kl            | 0.0063013695 |
|    clip_fraction        | 0.0262       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.964        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.165        |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.000273    |
|    std                  | 0.998        |
|    value_loss           | 2.63         |
------------------------------------------
Eval num_timesteps=71000, episode_reward=-588.05 +/- 170.52
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -588     |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 217   |
|    iterations      | 35    |
|    time_elapsed    | 329   |
|    total_timesteps | 71680 |
------------------------------
Eval num_timesteps=72000, episode_reward=-578.79 +/- 232.19
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -579        |
| time/                   |             |
|    total_timesteps      | 72000       |
| train/                  |             |
|    approx_kl            | 0.005233085 |
|    clip_fraction        | 0.0367      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.26       |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0525      |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00239    |
|    std                  | 1           |
|    value_loss           | 1.03        |
-----------------------------------------
Eval num_timesteps=73000, episode_reward=-511.91 +/- 195.45
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 217   |
|    iterations      | 36    |
|    time_elapsed    | 338   |
|    total_timesteps | 73728 |
------------------------------
Eval num_timesteps=74000, episode_reward=-373.54 +/- 121.02
Episode length: 1001.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 1e+03      |
|    mean_reward          | -374       |
| time/                   |            |
|    total_timesteps      | 74000      |
| train/                  |            |
|    approx_kl            | 0.00761709 |
|    clip_fraction        | 0.0499     |
|    clip_range           | 0.2        |
|    entropy_loss         | -4.26      |
|    explained_variance   | 0.795      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.2        |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.00605   |
|    std                  | 1          |
|    value_loss           | 2.67       |
----------------------------------------
Eval num_timesteps=75000, episode_reward=-581.00 +/- 217.14
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -581     |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 217   |
|    iterations      | 37    |
|    time_elapsed    | 348   |
|    total_timesteps | 75776 |
------------------------------
Eval num_timesteps=76000, episode_reward=-410.77 +/- 93.80
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -411         |
| time/                   |              |
|    total_timesteps      | 76000        |
| train/                  |              |
|    approx_kl            | 0.0067523904 |
|    clip_fraction        | 0.0389       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.26        |
|    explained_variance   | 0.974        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.086        |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.00341     |
|    std                  | 1.01         |
|    value_loss           | 5.22         |
------------------------------------------
Eval num_timesteps=77000, episode_reward=-581.77 +/- 212.01
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -582     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 217   |
|    iterations      | 38    |
|    time_elapsed    | 358   |
|    total_timesteps | 77824 |
------------------------------
Eval num_timesteps=78000, episode_reward=-569.41 +/- 145.80
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -569        |
| time/                   |             |
|    total_timesteps      | 78000       |
| train/                  |             |
|    approx_kl            | 0.006724539 |
|    clip_fraction        | 0.0484      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.27       |
|    explained_variance   | 0.957       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.399       |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.00423    |
|    std                  | 1           |
|    value_loss           | 6.29        |
-----------------------------------------
Eval num_timesteps=79000, episode_reward=-399.04 +/- 144.33
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 216   |
|    iterations      | 39    |
|    time_elapsed    | 368   |
|    total_timesteps | 79872 |
------------------------------
Eval num_timesteps=80000, episode_reward=-653.88 +/- 223.98
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -654        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.007818433 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.25       |
|    explained_variance   | 0.926       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.175       |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00772    |
|    std                  | 0.993       |
|    value_loss           | 1.63        |
-----------------------------------------
Eval num_timesteps=81000, episode_reward=-629.11 +/- 151.32
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -629     |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 215   |
|    iterations      | 40    |
|    time_elapsed    | 379   |
|    total_timesteps | 81920 |
------------------------------
Eval num_timesteps=82000, episode_reward=-471.37 +/- 191.35
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -471        |
| time/                   |             |
|    total_timesteps      | 82000       |
| train/                  |             |
|    approx_kl            | 0.003837531 |
|    clip_fraction        | 0.0428      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.23       |
|    explained_variance   | 0.915       |
|    learning_rate        | 0.0003      |
|    loss                 | 7.07        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00264    |
|    std                  | 0.99        |
|    value_loss           | 4.37        |
-----------------------------------------
Eval num_timesteps=83000, episode_reward=-452.08 +/- 235.92
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -452     |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 214   |
|    iterations      | 41    |
|    time_elapsed    | 390   |
|    total_timesteps | 83968 |
------------------------------
Eval num_timesteps=84000, episode_reward=-364.89 +/- 100.44
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -365         |
| time/                   |              |
|    total_timesteps      | 84000        |
| train/                  |              |
|    approx_kl            | 0.0062982277 |
|    clip_fraction        | 0.0479       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | -0.0159      |
|    learning_rate        | 0.0003       |
|    loss                 | 1.23         |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00472     |
|    std                  | 0.981        |
|    value_loss           | 2.5          |
------------------------------------------
New best mean reward!
Eval num_timesteps=85000, episode_reward=-491.75 +/- 136.79
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -492     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-567.58 +/- 180.03
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 212   |
|    iterations      | 42    |
|    time_elapsed    | 404   |
|    total_timesteps | 86016 |
------------------------------
Eval num_timesteps=87000, episode_reward=-551.57 +/- 178.63
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -552        |
| time/                   |             |
|    total_timesteps      | 87000       |
| train/                  |             |
|    approx_kl            | 0.006208663 |
|    clip_fraction        | 0.0451      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.2        |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.169       |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00187    |
|    std                  | 0.983       |
|    value_loss           | 1.92        |
-----------------------------------------
Eval num_timesteps=88000, episode_reward=-448.19 +/- 210.89
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -448     |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 212   |
|    iterations      | 43    |
|    time_elapsed    | 413   |
|    total_timesteps | 88064 |
------------------------------
Eval num_timesteps=89000, episode_reward=-405.27 +/- 126.85
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -405         |
| time/                   |              |
|    total_timesteps      | 89000        |
| train/                  |              |
|    approx_kl            | 0.0053821504 |
|    clip_fraction        | 0.0297       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.21        |
|    explained_variance   | 0.772        |
|    learning_rate        | 0.0003       |
|    loss                 | 3.07         |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00189     |
|    std                  | 0.989        |
|    value_loss           | 3.12         |
------------------------------------------
Eval num_timesteps=90000, episode_reward=-427.25 +/- 79.85
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 213   |
|    iterations      | 44    |
|    time_elapsed    | 422   |
|    total_timesteps | 90112 |
------------------------------
Eval num_timesteps=91000, episode_reward=-623.53 +/- 163.93
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -624        |
| time/                   |             |
|    total_timesteps      | 91000       |
| train/                  |             |
|    approx_kl            | 0.006892269 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.203       |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00269    |
|    std                  | 0.991       |
|    value_loss           | 2.75        |
-----------------------------------------
Eval num_timesteps=92000, episode_reward=-429.38 +/- 125.15
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 213   |
|    iterations      | 45    |
|    time_elapsed    | 431   |
|    total_timesteps | 92160 |
------------------------------
Eval num_timesteps=93000, episode_reward=-530.22 +/- 187.43
Episode length: 1001.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 1e+03        |
|    mean_reward          | -530         |
| time/                   |              |
|    total_timesteps      | 93000        |
| train/                  |              |
|    approx_kl            | 0.0074145426 |
|    clip_fraction        | 0.0613       |
|    clip_range           | 0.2          |
|    entropy_loss         | -4.22        |
|    explained_variance   | 0.917        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.51         |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.00637     |
|    std                  | 0.987        |
|    value_loss           | 0.986        |
------------------------------------------
Eval num_timesteps=94000, episode_reward=-601.32 +/- 228.78
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -601     |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 213   |
|    iterations      | 46    |
|    time_elapsed    | 441   |
|    total_timesteps | 94208 |
------------------------------
Eval num_timesteps=95000, episode_reward=-502.61 +/- 145.35
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -503        |
| time/                   |             |
|    total_timesteps      | 95000       |
| train/                  |             |
|    approx_kl            | 0.005377463 |
|    clip_fraction        | 0.0423      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.955       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.28        |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00306    |
|    std                  | 0.987       |
|    value_loss           | 3.98        |
-----------------------------------------
Eval num_timesteps=96000, episode_reward=-488.44 +/- 129.08
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 213   |
|    iterations      | 47    |
|    time_elapsed    | 451   |
|    total_timesteps | 96256 |
------------------------------
Eval num_timesteps=97000, episode_reward=-505.38 +/- 193.41
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -505        |
| time/                   |             |
|    total_timesteps      | 97000       |
| train/                  |             |
|    approx_kl            | 0.004386791 |
|    clip_fraction        | 0.0229      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.21       |
|    explained_variance   | 0.912       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 470         |
|    policy_gradient_loss | -0.00243    |
|    std                  | 0.986       |
|    value_loss           | 0.859       |
-----------------------------------------
Eval num_timesteps=98000, episode_reward=-670.04 +/- 175.45
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -670     |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
------------------------------
| time/              |       |
|    fps             | 213   |
|    iterations      | 48    |
|    time_elapsed    | 460   |
|    total_timesteps | 98304 |
------------------------------
Eval num_timesteps=99000, episode_reward=-433.31 +/- 93.14
Episode length: 1001.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 1e+03       |
|    mean_reward          | -433        |
| time/                   |             |
|    total_timesteps      | 99000       |
| train/                  |             |
|    approx_kl            | 0.005451899 |
|    clip_fraction        | 0.0374      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.22       |
|    explained_variance   | 0.95        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00242    |
|    std                  | 0.992       |
|    value_loss           | 3.94        |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=-370.21 +/- 121.30
Episode length: 1001.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -370     |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
-------------------------------
| time/              |        |
|    fps             | 213    |
|    iterations      | 49     |
|    time_elapsed    | 470    |
|    total_timesteps | 100352 |
-------------------------------
